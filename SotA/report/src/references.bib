
@misc{chernozhukov_applied_2024,
	title = {Applied {Causal} {Inference} {Powered} by {ML} and {AI}},
	url = {http://arxiv.org/abs/2403.02467},
	doi = {10.48550/arXiv.2403.02467},
	abstract = {An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.},
	urldate = {2025-12-14},
	publisher = {arXiv},
	author = {Chernozhukov, Victor and Hansen, Christian and Kallus, Nathan and Spindler, Martin and Syrgkanis, Vasilis},
	month = mar,
	year = {2024},
	note = {arXiv:2403.02467 [econ]},
	keywords = {Computer Science - Machine Learning, Economics - Econometrics, Statistics - Machine Learning, Statistics - Methodology, Core, Theory, CML},
	file = {Preprint PDF:/Users/nb/Zotero/storage/T97SY2L7/Chernozhukov et al. - 2024 - Applied Causal Inference Powered by ML and AI.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/M3YSQHUX/2403.html:text/html},
}

@article{chernozhukov_doubledebiased_2018,
	title = {Double/debiased machine learning for treatment and structural parameters},
	volume = {21},
	issn = {1368-4221},
	url = {https://doi.org/10.1111/ectj.12097},
	doi = {10.1111/ectj.12097},
	abstract = {We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
	number = {1},
	urldate = {2025-12-14},
	journal = {The Econometrics Journal},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	month = feb,
	year = {2018},
	keywords = {Core, Theory, DML},
	pages = {C1--C68},
	file = {Full Text PDF:/Users/nb/Zotero/storage/MPAPLPLG/Chernozhukov et al. - 2018 - Doubledebiased machine learning for treatment and structural parameters.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/THJJMYC6/ectj.html:text/html},
}

@article{racek_capturing_2025,
	title = {Capturing the spatio-temporal diffusion effects of armed conflict: {A} nonparametric smoothing approach},
	issn = {0964-1998},
	shorttitle = {Capturing the spatio-temporal diffusion effects of armed conflict},
	url = {https://doi.org/10.1093/jrsssa/qnaf120},
	doi = {10.1093/jrsssa/qnaf120},
	abstract = {Facilitated by advancements in conflict event databases, studies have moved towards predicting armed conflict and understanding its determinants subnationally. However, existing statistical models neither analyse nor capture the diffusion of armed conflict, and hence do not adequately account for its dependence across both time and space. To address this, we introduce a regression approach that simultaneously captures both spatial and temporal dimension of the diffusion of armed conflict through nonparametric smoothing, while all its effects and parameters remain fully interpretable. Using fine-grained conflict data on Africa, we observe that diffusion exhibits long-lasting and far-reaching dependencies that decay exponentially in both space and time, thus highlighting the importance of controlling for these effects. We illustrate the flexibility of our method for studying conflict diffusion by investigating the role of population in the transmission of conflict. We find that conflict typically breaks out in densely populated areas, and from there diffuses, specifically to lower population areas.},
	urldate = {2025-12-14},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Racek, Daniel and Thurner, Paul W and Kauermann, Göran},
	month = jul,
	year = {2025},
	keywords = {Core, DML, Applied, Spatio Temporal Causal Inference},
	pages = {qnaf120},
	file = {Full Text PDF:/Users/nb/Zotero/storage/BN93PAAX/Racek et al. - 2025 - Capturing the spatio-temporal diffusion effects of armed conflict A nonparametric smoothing approac.pdf:application/pdf;PDF:/Users/nb/Zotero/storage/XSFJIRAV/Racek et al. - 2024 - Capturing the Spatio-Temporal Diffusion Effects of Armed Conflict A Non-parametric Smoothing Approa.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/KTJHYK8E/qnaf120.html:text/html},
}

@article{feuerriegel_causal_2024,
	title = {Causal machine learning for predicting treatment outcomes},
	volume = {30},
	copyright = {2024 Springer Nature America, Inc.},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-024-02902-1},
	doi = {10.1038/s41591-024-02902-1},
	abstract = {Causal machine learning (ML) offers flexible, data-driven methods for predicting treatment outcomes including efficacy and toxicity, thereby supporting the assessment and safety of drugs. A key benefit of causal ML is that it allows for estimating individualized treatment effects, so that clinical decision-making can be personalized to individual patient profiles. Causal ML can be used in combination with both clinical trial data and real-world data, such as clinical registries and electronic health records, but caution is needed to avoid biased or incorrect predictions. In this Perspective, we discuss the benefits of causal ML (relative to traditional statistical or ML approaches) and outline the key components and steps. Finally, we provide recommendations for the reliable use of causal ML and effective translation into the clinic.},
	language = {en},
	number = {4},
	urldate = {2025-12-14},
	journal = {Nature Medicine},
	publisher = {Nature Publishing Group},
	author = {Feuerriegel, Stefan and Frauen, Dennis and Melnychuk, Valentyn and Schweisthal, Jonas and Hess, Konstantin and Curth, Alicia and Bauer, Stefan and Kilbertus, Niki and Kohane, Isaac S. and van der Schaar, Mihaela},
	month = apr,
	year = {2024},
	keywords = {Clinical trial design, CML, Core, Medical research, Predictive markers, Therapeutics},
	pages = {958--968},
	file = {Full Text PDF:/Users/nb/Zotero/storage/IRJI6I55/Feuerriegel et al. - 2024 - Causal machine learning for predicting treatment outcomes.pdf:application/pdf},
}

@inproceedings{lewis_doubledebiased_2021,
	title = {Double/{Debiased} {Machine} {Learning} for {Dynamic} {Treatment} {Effects}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/bf65417dcecc7f2b0006e1f5793b7143-Abstract.html},
	abstract = {We consider the estimation of treatment effects in settings when multiple treatments are assigned over time and treatments can have a causal effect on future outcomes. We propose an extension of the double/debiased machine learning framework to estimate the dynamic effects of treatments and apply it to a concrete linear Markovian high-dimensional state space model and to general structural nested mean models. Our method allows the use of arbitrary machine learning methods to control for the high dimensional state, subject to a mean square error guarantee, while still allowing parametric estimation and construction of confidence intervals for the dynamic treatment effect parameters of interest.  Our method is based on a sequential regression peeling process, which we show can be equivalently interpreted as a Neyman orthogonal moment estimator. This allows us to show root-n asymptotic normality of the estimated causal effects.},
	urldate = {2025-12-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Greg and Syrgkanis, Vasilis},
	year = {2021},
	keywords = {Core, Theory, DML},
	pages = {22695--22707},
	file = {Full Text PDF:/Users/nb/Zotero/storage/DGA6D2VR/Lewis and Syrgkanis - 2021 - DoubleDebiased Machine Learning for Dynamic Treatment Effects.pdf:application/pdf},
}

@article{semenova_inference_2023,
	title = {Inference on heterogeneous treatment effects in high-dimensional dynamic panels under weak dependence},
	volume = {14},
	copyright = {Copyright © 2023 The Authors.},
	issn = {1759-7331},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/QE1670},
	doi = {10.3982/QE1670},
	abstract = {This paper provides estimation and inference methods for conditional average treatment effects (CATE) characterized by a high-dimensional parameter in both homogeneous cross-sectional and unit-heterogeneous dynamic panel data settings. In our leading example, we model CATE by interacting the base treatment variable with explanatory variables. The first step of our procedure is orthogonalization, where we partial out the controls and unit effects from the outcome and the base treatment and take the cross-fitted residuals. This step uses a novel generic cross-fitting method that we design for weakly dependent time series and panel data. This method “leaves out the neighbors” when fitting nuisance components, and we theoretically power it by using Strassen's coupling. As a result, we can rely on any modern machine learning method in the first step, provided it learns the residuals well enough. Second, we construct an orthogonal (or residual) learner of CATE—the lasso CATE—that regresses the outcome residual on the vector of interactions of the residualized treatment with explanatory variables. If the complexity of CATE function is simpler than that of the first-stage regression, the orthogonal learner converges faster than the single-stage regression-based learner. Third, we perform simultaneous inference on parameters of the CATE function using debiasing. We also can use ordinary least squares in the last two steps when CATE is low-dimensional. In heterogeneous panel data settings, we model the unobserved unit heterogeneity as a weakly sparse deviation from Mundlak's (1978) model of correlated unit effects as a linear function of time-invariant covariates and make use of L1-penalization to estimate these models. We demonstrate our methods by estimating price elasticities of groceries based on scanner data. We note that our results are new even for the cross-sectional (i.i.d.) case.},
	language = {en},
	number = {2},
	urldate = {2025-12-14},
	journal = {Quantitative Economics},
	author = {Semenova, Vira and Goldman, Matt and Chernozhukov, Victor and Taddy, Matt},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/QE1670},
	keywords = {C14, C23, C33, CATE, cross-fitting, dynamic panel data, mixing, neighbors-left-out, Orthogonal learning, residual learning, time series, Core, Theory},
	pages = {471--510},
	file = {Full Text PDF:/Users/nb/Zotero/storage/GCV9PL6C/Semenova et al. - 2023 - Inference on heterogeneous treatment effects in high-dimensional dynamic panels under weak dependenc.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/STERUD82/QE1670.html:text/html},
}

@misc{noauthor_praxis_2025,
	title = {Praxis der {Forschung}: {Applying} {Causal} {Machine} {Learning} in {Conflict} {Research} ({Project} {Description})},
	shorttitle = {Project {Description}},
	language = {en},
	urldate = {2025-12-14},
	month = nov,
	year = {2025},
	file = {PDF:/Users/nb/Zotero/storage/KTRMNDEG/Praxis der Forschung Applying Causal Machine Learning in Conflict Research (Project Description).pdf:application/pdf;PXL_20251217_125710873:/Users/nb/Zotero/storage/8APB942V/PXL_20251217_125710873.jpg:image/jpeg},
}

@article{hegre_202324_2025,
	title = {The 2023/24 {VIEWS} {Prediction} challenge: {Predicting} the number of fatalities in armed conflict, with uncertainty},
	volume = {62},
	issn = {0022-3433},
	shorttitle = {The 2023/24 {VIEWS} {Prediction} challenge},
	url = {https://doi.org/10.1177/00223433241300862},
	doi = {10.1177/00223433241300862},
	abstract = {Governmental and nongovernmental organizations have increasingly relied on early-warning systems of conflict to support their decisionmaking. Predictions of war intensity as probability distributions prove closer to what policymakers need than point estimates, as they encompass useful representations of both the most likely outcome and the lower-probability risk that conflicts escalate catastrophically. Point-estimate predictions, by contrast, fail to represent the inherent uncertainty in the distribution of conflict fatalities. Yet, current early warning systems are preponderantly focused on providing point estimates, while efforts to forecast conflict fatalities as a probability distribution remain sparse. Building on the predecessor VIEWS competition, we organize a prediction challenge to encourage endeavours in this direction. We invite researchers across multiple disciplinary fields, from conflict studies to computer science, to forecast the number of fatalities in state-based armed conflicts, in the form of the UCDP ‘best’ estimates aggregated to two units of analysis (country-months and PRIO-GRID-months), with estimates of uncertainty. This article introduces the goal and motivation behind the prediction challenge, presents a set of evaluation metrics to assess the performance of the forecasting models, describes the benchmark models which the contributions are evaluated against, and summarizes the salient features of the submitted contributions.},
	language = {EN},
	number = {6},
	urldate = {2025-12-14},
	journal = {Journal of Peace Research},
	publisher = {SAGE Publications Ltd},
	author = {Hegre, Håvard and Vesco, Paola and Colaresi, Michael and Vestby, Jonas and Timlick, Alexa and Kazmi, Noorain Syed and Lindqvist-McGowan, Angelica and Becker, Friederike and Binetti, Marco and Bodentien, Tobias and Bohne, Tobias and Brandt, Patrick T. and Chadefaux, Thomas and Drauz, Simon and Dworschak, Christoph and D’Orazio, Vito and Frank, Hannah and Fritz, Cornelius and Gleditsch, Kristian Skrede and Häffner, Sonja and Hofer, Martin and Klebe, Finn L and Macis, Luca and Malaga, Alexandra and Mehrl, Marius and Metternich, Nils W and Mittermaier, Daniel and Muchlinski, David and Mueller, Hannes and Oswald, Christian and Pisano, Paola and Randahl, David and Rauh, Christopher and Rüter, Lotta and Schincariol, Thomas and Seimon, Benjamin and Siletti, Elena and Tagliapietra, Marco and Thornhill, Chandler and Vegelius, Johan and Walterskirchen, Julian},
	month = nov,
	year = {2025},
	keywords = {Applied, Data},
	pages = {2070--2087},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/QTC7NUJP/Hegre et al. - 2025 - The 202324 VIEWS Prediction challenge Predicting the number of fatalities in armed conflict, with.pdf:application/pdf},
}

@article{racek_conflict_2024,
	title = {Conflict forecasting using remote sensing data: {An} application to the {Syrian} civil war},
	volume = {40},
	issn = {0169-2070},
	shorttitle = {Conflict forecasting using remote sensing data},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207023000377},
	doi = {10.1016/j.ijforecast.2023.04.001},
	abstract = {Conflict research is increasingly influenced by modern computational and statistical techniques. Combined with recent advances in the collection and public availability of new data sources, this allows for more accurate forecasting models in ever more fine-grained spatial areas. This paper demonstrates the utilization of remote sensing data as a potential solution to the lack of official data sources for conflict forecasting in crisis-ridden countries. We evaluate and quantify remote sensing data’s differentiated impact on forecasting accuracy across fine-grained spatial grid cells using the Syrian civil war as a use case. It can be shown that conflict, particularly its onset, can be forecasted more accurately by employing publicly available remote sensing datasets. These results are consistent across a range of established statistical and machine learning models, which raises the hope to get closer to reliable early-warning systems for conflict prediction.},
	number = {1},
	urldate = {2025-12-14},
	journal = {International Journal of Forecasting},
	author = {Racek, Daniel and Thurner, Paul W. and Davidson, Brittany I. and Zhu, Xiao Xiang and Kauermann, Göran},
	month = jan,
	year = {2024},
	keywords = {Conflict prediction, Forecasting, Machine learning, Remote sensing, Satellite imagery, Statistical modeling, Syria, Applied},
	pages = {373--391},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/2E4MVI6S/Racek et al. - 2024 - Conflict forecasting using remote sensing data An application to the Syrian civil war.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/SREUUN9G/S0169207023000377.html:text/html},
}

@article{fritz_role_2022,
	title = {The role of governmental weapons procurements in forecasting monthly fatalities in intrastate conflicts: {A} semiparametric hierarchical hurdle model},
	volume = {48},
	issn = {0305-0629},
	shorttitle = {The role of governmental weapons procurements in forecasting monthly fatalities in intrastate conflicts},
	url = {https://doi.org/10.1080/03050629.2022.1993210},
	doi = {10.1080/03050629.2022.1993210},
	abstract = {Accurate and interpretable forecasting models predicting spatially and temporally fine-grained changes in the numbers of intrastate conflict casualties are of crucial importance for policymakers and international non-governmental organizations (NGOs). Using a count data approach, we propose a hierarchical hurdle regression model to address the corresponding prediction challenge at the monthly PRIO-grid level. More precisely, we model the intensity of local armed conflict at a specific point in time as a three-stage process. Stages one and two of our approach estimate whether we will observe any casualties at the country- and grid-cell-level, respectively, while stage three applies a regression model for truncated data to predict the number of such fatalities conditional upon the previous two stages. Within this modeling framework, we focus on the role of governmental arms imports as a processual factor allowing governments to intensify or deter from fighting. We further argue that a grid cell’s geographic remoteness is bound to moderate the effects of these military buildups. Out-of-sample predictions corroborate the effectiveness of our parsimonious and theory-driven model, which enables full transparency combined with accuracy in the forecasting process. Los modelos de previsión precisos e interpretables que predicen los cambios a nivel espacial y temporal en la cantidad de víctimas de los conflictos intraestatales son de vital importancia para los responsables políticos y las organizaciones no gubernamentales (ONG) internacionales. Utilizando un enfoque de datos de recuento, proponemos un modelo de regresión Hurdle jerárquico para abordar el correspondiente reto de predicción a nivel mensual de PRIO-GRID. Más concretamente, modelamos la intensidad del conflicto armado local en un momento determinado como un proceso de tres etapas. Las etapas uno y dos de nuestro enfoque estiman si observaremos alguna víctima a nivel de país y de celda de la red, respectivamente, mientras que la etapa tres aplica un modelo de regresión para datos truncados con el propósito de predecir la cantidad potencial de dichas víctimas mortales en función de las dos etapas anteriores. Dentro de este marco de modelización, nos centramos en el rol de las importaciones de armas por parte de los gobiernos como un factor de proceso que permite a los gobiernos intensificar o impedir los enfrentamientos. Además, sostenemos que la lejanía geográfica de una célula de la red está destinada a moderar los efectos de estas concentraciones militares. Las predicciones fuera de la muestra corroboran la eficacia de nuestro modelo parsimonioso y basado en la teoría, que permite una transparencia total combinada con precisión en el proceso de previsión. Les modèles de prévision précis et interprétables, qui permettent de prédire spatialement et temporellement les détails des changements dans les nombres de victimes de conflits intra-étatiques, sont d’une importance cruciale pour les décideurs politiques et les organizations non gouvernementales (ONG) internationales. Nous adoptons une approche par données de comptage et nous proposons un modèle de régression hiérarchique à obstacle (hurdle) pour relever le défi de la prédiction correspondante au niveau de la grille mensuelle du PRIO (Peace research institute Oslo, Institut de recherche sur la paix d’Oslo). Plus précisément, nous modélisons l’intensité des conflits armés locaux à un moment spécifique sous la forme d’un processus en trois étapes. Les étapes un et deux de notre approche consistent à estimer si nous observerons des pertes respectivement au niveau du pays et de la cellule de grille, tandis que l’étape trois consiste à appliquer un modèle de régression pour les données tronquées afin de prédire le nombre de ces pertes en fonction des deux étapes précédentes. Dans ce cadre de modélisation, nous nous concentrons sur le rôle des importations d’armes gouvernementales en tant que facteur processuel permettant aux gouvernements d’intensifier ou de dissuader les combats. Nous soutenons également que l’isolement géographique d’une cellule de la grille est susceptible de modérer les effets de ces renforcements militaires. Des prédictions hors échantillon corroborent l’efficacité de notre modèle parcimonieux fondé sur la théorie qui permet une totale transparence associée à une précision du processus de prévision.},
	number = {4},
	urldate = {2025-12-14},
	journal = {International Interactions},
	publisher = {Routledge},
	author = {Fritz, Cornelius and Mehrl, Marius and Thurner, Paul W. and Kauermann, Göran},
	month = jul,
	year = {2022},
	note = {\_eprint: https://doi.org/10.1080/03050629.2022.1993210},
	keywords = {Applied, Confilct, Conflict forecasting, conflict intensity, Forecasting, hurdle regression, semiparametric regression},
	pages = {778--799},
	file = {Full Text PDF:/Users/nb/Zotero/storage/YEB4DJ2W/Fritz et al. - 2022 - The role of governmental weapons procurements in forecasting monthly fatalities in intrastate confli.pdf:application/pdf},
}

@article{tomko_causal_2024,
	title = {Causal {Inference} for {Spatial} {Data} {Analytics} ({Dagstuhl} {Seminar} 24202)},
	volume = {14},
	issn = {2192-5283},
	url = {https://drops.dagstuhl.de/entities/document/10.4230/DagRep.14.5.25},
	doi = {10.4230/DagRep.14.5.25},
	number = {5},
	urldate = {2025-12-15},
	journal = {Dagstuhl Reports},
	publisher = {Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
	author = {Tomko, Martin and Xin, Yanan and Wahl, Jonas},
	editor = {Tomko, Martin and Xin, Yanan and Wahl, Jonas},
	year = {2024},
	note = {Place: Dagstuhl, Germany},
	keywords = {Dagstuhl Seminar, Spatial Analysis, Spatial Causal Analysis, Spatial Causal Discovery, Spatial Causal Inference, Spatial Data, Spatio Temporal Causal Inference},
	pages = {25--57},
	file = {Full Text PDF:/Users/nb/Zotero/storage/3EZ8JDJV/Tomko et al. - 2024 - Causal Inference for Spatial Data Analytics (Dagstuhl Seminar 24202).pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/4WT8WDV3/DagRep.14.5.html:text/html},
}

@incollection{yang_stoat_2025,
	address = {New York, NY, USA},
	title = {{STOAT}: {Spatial}-{Temporal} {Probabilistic} {Causal} {Inference} {Network}},
	isbn = {979-8-4007-2086-4},
	shorttitle = {{STOAT}},
	url = {https://doi.org/10.1145/3748636.3762761},
	abstract = {Spatial-temporal causal time series (STC-TS) involve region-specific temporal observations driven by causally relevant covariates and interconnected across geographic or network-based spaces. Existing methods often model spatial and temporal dynamics independently and overlook causality-driven probabilistic forecasting, limiting their predictive power. To address this, we propose STOAT (Spatial-Temporal Probabilistic Causal Inference Network), a novel framework for probabilistic forecasting in STC-TS. The proposed method extends a causal inference approach by incorporating a spatial relation matrix that encodes interregional dependencies, enabling spatially informed causal effect estimation. The resulting latent series are processed by deep probabilistic models to estimate the parameters of the distributions, enabling calibrated uncertainty modeling. Experiments on COVID-19 data across six countries demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting models in key metrics, particularly in regions with strong spatial dependencies. By bridging causal inference and geospatial probabilistic forecasting, STOAT offers a generalizable framework for complex spatial-temporal tasks, such as epidemic management.},
	urldate = {2025-12-15},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Yang and Yin, Du and Xue, Hao and Salim, Flora},
	month = dec,
	year = {2025},
	keywords = {Spatio Temporal Causal Inference},
	pages = {499--510},
	file = {Full Text PDF:/Users/nb/Zotero/storage/7RHLCCNC/Yang et al. - 2025 - STOAT Spatial-Temporal Probabilistic Causal Inference Network.pdf:application/pdf},
}

@article{athey_estimating_2019,
	title = {Estimating {Treatment} {Effects} with {Causal} {Forests}: {An} {Application}},
	volume = {5},
	issn = {2767-3324},
	shorttitle = {Estimating {Treatment} {Effects} with {Causal} {Forests}},
	url = {https://muse.jhu.edu/pub/56/article/793356},
	abstract = {We apply causal forests to a dataset derived from the National Study of Learning Mindsets, and discusses resulting practical and conceptual challenges. This note will appear in an upcoming issue of Observational Studies, Empirical Investigation of Methods for Heterogeneity, that compiles several analyses of the same dataset.},
	number = {2},
	urldate = {2025-12-15},
	journal = {Observational Studies},
	publisher = {University of Pennsylvania Press},
	author = {Athey, Susan and Wager, Stefan},
	year = {2019},
	keywords = {Causal Forests, Theory},
	pages = {37--51},
	file = {Estimating Treatment Effects with Causal Forests\: An Application:/Users/nb/Zotero/storage/SSFNNFRT/Athey and Wager - 2019 - Estimating Treatment Effects with Causal Forests An Application.pdf:application/pdf},
}

@article{croicu_forecasting_2025,
	title = {Forecasting battles : {New} machine learning methods for predicting armed conflict},
	shorttitle = {Forecasting battles},
	url = {https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-545176},
	abstract = {Over the past decade, the field of conflict forecasting has undergone a remarkable metamorphosis, transforming from a series of isolated efforts with low predictive power into large, globe-spanning ...},
	language = {eng},
	urldate = {2025-12-15},
	publisher = {Uppsala University},
	author = {Croicu, Mihai},
	year = {2025},
	keywords = {Applied, Confilct},
	file = {Full Text PDF:/Users/nb/Zotero/storage/94GQ29ZK/Croicu - 2025 - Forecasting battles  New machine learning methods for predicting armed conflict.pdf:application/pdf},
}

@article{hegre_introducing_2020,
	title = {Introducing the {UCDP} {Candidate} {Events} {Dataset}},
	volume = {7},
	issn = {2053-1680},
	url = {https://doi.org/10.1177/2053168020935257},
	doi = {10.1177/2053168020935257},
	abstract = {This article presents a new, monthly updated dataset on organized violence—the Uppsala Conflict Data Program Candidate Events Dataset. It contains recent observations of candidate events, a majority of which are eventually included in the Uppsala Conflict Data Program Georeferenced Event Dataset as part of its annual update after a careful vetting process. We describe the definitions, sources and procedures employed to code the candidate events, and a set of issues that emerge when coding data on organized violence in near-real time. Together, the Uppsala Conflict Data Program Candidate and Georeferenced Event Datasets minimize an inherent trade-off between update speed and quality control. Having monthly updated conflict data is advantageous for users needing near-real time monitoring of violent situations and aiming to anticipate future developments. To demonstrate this, we show that including them in a conflict forecasting system yields distinct improvements in terms of predictive performance: Average precision increases by 20–40\% relative to using the Uppsala Conflict Data Program Georeferenced Event Dataset only. We also show that to ensure quality and consistency, revisiting the initial coding making use of sources that become available later is absolutely necessary.},
	language = {EN},
	number = {3},
	urldate = {2025-12-15},
	journal = {Research \& Politics},
	publisher = {SAGE Publications Ltd},
	author = {Hegre, Håvard and Croicu, Mihai and Eck, Kristine and Högbladh, Stina},
	month = jul,
	year = {2020},
	keywords = {Confilct, Data},
	pages = {2053168020935257},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/IFB7VADB/Hegre et al. - 2020 - Introducing the UCDP Candidate Events Dataset.pdf:application/pdf},
}

@article{mueller_monitoring_2021,
	title = {Monitoring war destruction from space using machine learning},
	volume = {118},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2025400118},
	doi = {10.1073/pnas.2025400118},
	abstract = {Existing data on building destruction in conflict zones rely on eyewitness reports
or manual detection, which makes it generally scarce, incomplete...},
	language = {EN},
	number = {23},
	urldate = {2025-12-15},
	journal = {Proceedings of the National Academy of Sciences},
	publisher = {Proceedings of the National Academy of Sciences},
	author = {Mueller, Hannes and Groeger, Andre and Hersh, Jonathan and Matranga, Andrea and Serrat, Joan},
	month = jun,
	year = {2021},
	note = {Company: National Academy of Sciences
Distributor: National Academy of Sciences
Institution: National Academy of Sciences
Label: National Academy of Sciences},
	keywords = {Applied, Confilct},
	pages = {e2025400118},
	file = {Full Text PDF:/Users/nb/Zotero/storage/EX8FEN9P/Mueller et al. - 2021 - Monitoring war destruction from space using machine learning.pdf:application/pdf},
}

@inproceedings{zhang_near-optimal_2019,
	title = {Near-{Optimal} {Reinforcement} {Learning} in {Dynamic} {Treatment} {Regimes}},
	volume = {32},
	url = {https://papers.neurips.cc/paper_files/paper/2019/hash/8252831b9fce7a49421e622c14ce0f65-Abstract.html},
	abstract = {A dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine the treatment assignment to patients based on evolving treatments and covariates' history. These regimes are particularly effective for managing chronic disorders and is arguably one of the key aspects towards more personalized decision-making. In this paper, we investigate the online reinforcement learning (RL) problem for selecting optimal DTRs provided that observational data is available. We develop the first adaptive algorithm that achieves near-optimal regret in DTRs in online settings, without any access to historical data. We further derive informative bounds on the system dynamics of the underlying DTR from confounded, observational data. Finally, we combine these results and develop a novel RL algorithm that efficiently learns the optimal DTR while leveraging the abundant, yet imperfect confounded observations.},
	urldate = {2025-12-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Junzhe and Bareinboim, Elias},
	year = {2019},
	keywords = {Theory, DTR},
	file = {Full Text PDF:/Users/nb/Zotero/storage/48GF7X67/Zhang and Bareinboim - 2019 - Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes.pdf:application/pdf},
}

@inproceedings{bach_hyperparameter_2024,
	title = {Hyperparameter {Tuning} for {Causal} {Inference} with {Double} {Machine} {Learning}: {A} {Simulation} {Study}},
	issn = {2640-3498},
	shorttitle = {Hyperparameter {Tuning} for {Causal} {Inference} with {Double} {Machine} {Learning}},
	url = {https://proceedings.mlr.press/v236/bach24a.html},
	abstract = {Proper hyperparameter tuning is essential for achieving optimal performance of modern machine learning (ML) methods in predictive tasks. While there is an extensive literature on tuning ML learners for prediction, there is only littleguidance available on tuning ML learners for causal machine learning. In this paper, we investigate the role of hyperparameter tuning and other practical decisions for causal estimation as based on the Double Machine Learning approach by Chernozhukov et al. (2018). Double Machine Learning relies on estimating so-called nuisance parameters by treating them as supervised learning problems and using them as plug-in estimates to solve for the (causal) parameter. We conduct an extensive simulation study using data from the 2019 Atlantic Causal Inference Conference Data Challenge. We provide empirical insights on the selection and calibration of the ML methods for the performance of causal estimation. First, we assess the importance of data splitting schemes for tuning ML learners within Double Machine Learning. Second, we investigate the choice of ML methods and hyperparameters, including recent AutoML frameworks, and consider the relationship between their predictive power and the estimation performance for a causal parameter of interest. Third, we investigate to what extent the choice of a particular causal model, as characterized by incorporated parametric assumptions, can be based on predictive performance metrics.},
	language = {en},
	urldate = {2025-12-15},
	booktitle = {Proceedings of the {Third} {Conference} on {Causal} {Learning} and {Reasoning}},
	publisher = {PMLR},
	author = {Bach, Philipp and Schacht, Oliver and Chernozhukov, Victor and Klaassen, Sven and Spindler, Martin},
	month = mar,
	year = {2024},
	keywords = {Applied, Training},
	pages = {1065--1117},
	file = {Full Text PDF:/Users/nb/Zotero/storage/TLAP5T9J/Bach et al. - 2024 - Hyperparameter Tuning for Causal Inference with Double Machine Learning A Simulation Study.pdf:application/pdf},
}

@misc{wager_estimation_2017,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	url = {http://arxiv.org/abs/1510.04342},
	doi = {10.48550/arXiv.1510.04342},
	abstract = {Many scientific and engineering challenges -- ranging from personalized medicine to customized marketing recommendations -- require an understanding of treatment effect heterogeneity. In this paper, we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect, and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Wager, Stefan and Athey, Susan},
	month = jul,
	year = {2017},
	note = {arXiv:1510.04342 [stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Causal Forests, Mathematics - Statistics Theory},
	file = {Preprint PDF:/Users/nb/Zotero/storage/NWNYZR8B/Wager and Athey - 2017 - Estimation and Inference of Heterogeneous Treatment Effects using Random Forests.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/KQ9MD7N6/1510.html:text/html},
}

@misc{wang_causal_2024,
	title = {Causal {Inference} with {Complex} {Treatments}: {A} {Survey}},
	shorttitle = {Causal {Inference} with {Complex} {Treatments}},
	url = {http://arxiv.org/abs/2407.14022},
	doi = {10.48550/arXiv.2407.14022},
	abstract = {Causal inference plays an important role in explanatory analysis and decision making across various fields like statistics, marketing, health care, and education. Its main task is to estimate treatment effects and make intervention policies. Traditionally, most of the previous works typically focus on the binary treatment setting that there is only one treatment for a unit to adopt or not. However, in practice, the treatment can be much more complex, encompassing multi-valued, continuous, or bundle options. In this paper, we refer to these as complex treatments and systematically and comprehensively review the causal inference methods for addressing them. First, we formally revisit the problem definition, the basic assumptions, and their possible variations under specific conditions. Second, we sequentially review the related methods for multi-valued, continuous, and bundled treatment settings. In each situation, we tentatively divide the methods into two categories: those conforming to the unconfoundedness assumption and those violating it. Subsequently, we discuss the available datasets and open-source codes. Finally, we provide a brief summary of these works and suggest potential directions for future research.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Wang, Yingrong and Li, Haoxuan and Zhu, Minqin and Wu, Anpeng and Xiong, Ruoxuan and Wu, Fei and Kuang, Kun},
	month = jul,
	year = {2024},
	note = {arXiv:2407.14022 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Methodology, causal inference, Survey},
	file = {Preprint PDF:/Users/nb/Zotero/storage/4BPPV2UU/Wang et al. - 2024 - Causal Inference with Complex Treatments A Survey.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/6AQJGM9R/2407.html:text/html},
}

@article{chernozhukov_generic_2025,
	title = {Generic {Machine} {Learning} {Inference} on {Heterogeneous} {Treatment} {Effects} in {Randomized} {Experiments}, {With} an {Application} to {Immunization} in {India}},
	volume = {93},
	copyright = {© 2025 The Authors. Econometrica published by John Wiley \& Sons Ltd on behalf of The Econometric Society},
	issn = {1468-0262},
	shorttitle = {Fisher–{Schultz} {Lecture}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA19303},
	doi = {10.3982/ECTA19303},
	abstract = {We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects using machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. The approach is valid in high-dimensional settings, where the effects are proxied (but not necessarily consistently estimated) by predictive and causal machine learning methods. We post-process these proxies into estimates of the key features. Our approach is generic; it can be used in conjunction with penalized methods, neural networks, random forests, boosted trees, and ensemble methods, both predictive and causal. Estimation and inference are based on repeated data splitting to avoid overfitting and achieve validity. We use quantile aggregation of the results across many potential splits, in particular taking medians of p-values and medians and other quantiles of confidence intervals. We show that quantile aggregation lowers estimation risks over a single split procedure, and establish its principal inferential properties. Finally, our analysis reveals ways to build provably better machine learning proxies through causal learning: we can use the objective functions that we develop to construct the best linear predictors of the effects, to obtain better machine learning proxies in the initial step. We illustrate the use of both inferential tools and causal learners with a randomized field experiment that evaluates a combination of nudges to stimulate demand for immunization in India.},
	language = {en},
	number = {4},
	urldate = {2025-12-15},
	journal = {Econometrica},
	author = {Chernozhukov, Victor and Demirer, Mert and Duflo, Esther and Fernández-Val, Iván},
	year = {2025},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA19303},
	keywords = {DML, Agnostic inference, causal machine learning, confidence intervals, heterogeneous effects, immunization incentives, multiple sample splitting, nudges, quantification of uncertainty, Multiple interventions},
	pages = {1121--1164},
	file = {Full Text PDF:/Users/nb/Zotero/storage/WYM4CF3R/Chernozhukov et al. - 2025 - Fisher–Schultz Lecture Generic Machine Learning Inference on Heterogeneous Treatment Effects in Ran.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/WYR3SWG9/ECTA19303.html:text/html},
}

@article{papadogeorgou_causal_2022,
	title = {Causal {Inference} with {Spatio}-{Temporal} {Data}: {Estimating} the {Effects} of {Airstrikes} on {Insurgent} {Violence} in {Iraq}},
	volume = {84},
	issn = {1369-7412},
	shorttitle = {Causal {Inference} with {Spatio}-{Temporal} {Data}},
	url = {https://doi.org/10.1111/rssb.12548},
	doi = {10.1111/rssb.12548},
	abstract = {Many causal processes have spatial and temporal dimensions. Yet the classic causal inference framework is not directly applicable when the treatment and outcome variables are generated by spatio-temporal point processes. We extend the potential outcomes framework to these settings by formulating the treatment point process as a stochastic intervention. Our causal estimands include the expected number of outcome events in a specified area under a particular stochastic treatment assignment strategy. Our methodology allows for arbitrary patterns of spatial spillover and temporal carryover effects. Using martingale theory, we show that the proposed estimator is consistent and asymptotically normal as the number of time periods increases. We propose a sensitivity analysis for the possible existence of unmeasured confounders, and extend it to the Hájek estimator. Simulation studies are conducted to examine the estimators' finite sample performance. Finally, we illustrate the proposed methods by estimating the effects of American airstrikes on insurgent violence in Iraq from February 2007 to July 2008. Our analysis suggests that increasing the average number of daily airstrikes for up to 1 month may result in more insurgent attacks. We also find some evidence that airstrikes can displace attacks from Baghdad to new locations up to 400 km away.},
	number = {5},
	urldate = {2025-12-15},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Papadogeorgou, Georgia and Imai, Kosuke and Lyall, Jason and Li, Fan},
	month = nov,
	year = {2022},
	keywords = {Applied, Confilct, Spatio Temporal Causal Inference},
	pages = {1969--1999},
	file = {Full Text PDF:/Users/nb/Zotero/storage/I8UQ68MJ/Papadogeorgou et al. - 2022 - Causal Inference with Spatio-Temporal Data Estimating the Effects of Airstrikes on Insurgent Violen.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/WWF6LG2W/rssb.html:text/html},
}

@article{christiansen_toward_2022,
	title = {Toward {Causal} {Inference} for {Spatio}-{Temporal} {Data}: {Conflict} and {Forest} {Loss} in {Colombia}},
	volume = {117},
	issn = {0162-1459},
	shorttitle = {Toward {Causal} {Inference} for {Spatio}-{Temporal} {Data}},
	url = {https://doi.org/10.1080/01621459.2021.2013241},
	doi = {10.1080/01621459.2021.2013241},
	abstract = {How does armed conflict influence tropical forest loss? For Colombia, both enhancing and reducing effect estimates have been reported. However, a lack of causal methodology has prevented establishing clear causal links between these two variables. In this work, we propose a class of causal models for spatio-temporal stochastic processes which allows us to formally define and quantify the causal effect of a vector of covariates X on a real-valued response Y. We introduce a procedure for estimating causal effects and a nonparametric hypothesis test for these effects being zero. Our application is based on geospatial information on conflict events and remote-sensing-based data on forest loss between 2000 and 2018 in Colombia. Across the entire country, we estimate the effect to be slightly negative (conflict reduces forest loss) but insignificant (P = 0.578), while at the provincial level, we find both positive effects (e.g., La Guajira, P = 0.047) and negative effects (e.g., Magdalena, P = 0.004). The proposed methods do not make strong distributional assumptions, and allow for arbitrarily many latent confounders, given that these confounders do not vary across time. Our theoretical findings are supported by simulations, and code is available online.},
	number = {538},
	urldate = {2025-12-15},
	journal = {Journal of the American Statistical Association},
	publisher = {Taylor \& Francis},
	author = {Christiansen, Rune and Baumann, Matthias and Kuemmerle, Tobias and Mahecha, Miguel D. and Peters, Jonas},
	month = apr,
	year = {2022},
	note = {\_eprint: https://doi.org/10.1080/01621459.2021.2013241},
	keywords = {Applied, Causal model, Confilct, Covariate adjustment, Latent confounding, Permutation test, Spatio Temporal Causal Inference, Treatment effect},
	pages = {591--601},
	file = {Full Text PDF:/Users/nb/Zotero/storage/KMI3IRGK/Christiansen et al. - 2022 - Toward Causal Inference for Spatio-Temporal Data Conflict and Forest Loss in Colombia.pdf:application/pdf},
}

@misc{zhou_estimating_2025,
	title = {Estimating {Heterogeneous} {Treatment} {Effects} for {Spatio}-{Temporal} {Causal} {Inference}},
	url = {http://arxiv.org/abs/2412.15128},
	doi = {10.48550/arXiv.2412.15128},
	abstract = {Scholars from diverse fields increasingly rely on high-frequency spatio-temporal data. Yet, causal inference with these data remains challenging due to spatial spillover and temporal carryover effects. We develop methods to estimate heterogeneous treatment effects by allowing for arbitrary spatial and temporal causal dependencies. We focus on common settings where the treatment and outcomes are time-varying spatial point patterns and where moderators are either spatial or spatio-temporal variables. We define causal estimands based on stochastic interventions where researchers specify counterfactual distributions of treatment events. We propose the Hajek-type estimator of the conditional average treatment effect (CATE) as a function of spatio-temporal moderator variables, and establish its asymptotic normality as the number of time periods increases. We then introduce a statistical test of no heterogeneous treatment effects. Through simulations, we evaluate the finite-sample performance of the proposed CATE estimator and its inferential properties. Our motivating application examines the heterogeneous effects of US airstrikes on insurgent violence in Iraq. Drawing on declassified spatio-temporal data, we examine how prior aid distributions moderate airstrike effects. Contrary to expectations from counterinsurgency theories, we find that prior aid distribution, along with greater amounts of aid per capita, is associated with increased insurgent attacks following airstrikes.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Zhou, Lingxiao and Imai, Kosuke and Lyall, Jason and Papadogeorgou, Georgia},
	month = oct,
	year = {2025},
	note = {arXiv:2412.15128 [stat]},
	keywords = {Statistics - Methodology, Applied, Confilct, Spatio Temporal Causal Inference, heterogeneous effects},
	file = {Preprint PDF:/Users/nb/Zotero/storage/TR65PSCP/Zhou et al. - 2025 - Estimating Heterogeneous Treatment Effects for Spatio-Temporal Causal Inference.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/DY68DLC4/2412.html:text/html},
}

@inproceedings{ali_estimating_2024,
	address = {Cham},
	title = {Estimating {Direct} and {Indirect} {Causal} {Effects} of {Spatiotemporal} {Interventions} in {Presence} of {Spatial} {Interference}},
	isbn = {978-3-031-70352-2},
	doi = {10.1007/978-3-031-70352-2_13},
	abstract = {Spatial interference (SI) occurs when the treatment at one location affects the outcomes at other locations. Accounting for spatial interference in spatiotemporal settings poses further challenges as interference violates the stable unit treatment value assumption, making it infeasible for standard causal inference methods to quantify the effects of time-varying treatment at spatially varying outcomes. In this paper, we first formalize the concept of spatial interference in the case of time-varying treatment assignments by extending the potential outcome framework under the assumption of no unmeasured confounding. We then propose our deep learning based potential outcome model for spatiotemporal causal inference. We utilize latent factor modeling to reduce the bias due to time-varying confounding while leveraging the power of U-Net architecture to capture global and local spatial interference in data over time. Our causal estimators are an extension of average treatment effect (ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial interference on treated and untreated data. Being the first of its kind deep learning based spatiotemporal causal inference technique, our approach shows advantages over several baseline methods based on the experiment results on two synthetic datasets, with and without spatial interference. Our results on real-world climate dataset also align with domain knowledge, further demonstrating the effectiveness of our proposed method.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}. {Research} {Track}},
	publisher = {Springer Nature Switzerland},
	author = {Ali, Sahara and Faruque, Omar and Wang, Jianwu},
	editor = {Bifet, Albert and Davis, Jesse and Krilavičius, Tomas and Kull, Meelis and Ntoutsi, Eirini and Žliobaitė, Indrė},
	year = {2024},
	keywords = {Spatio Temporal Causal Inference, Deep Learning, Spatial Interference, Spatiotemporal Causal Inference},
	pages = {213--230},
	file = {Full Text PDF:/Users/nb/Zotero/storage/8HXKBJ9M/Ali et al. - 2024 - Estimating Direct and Indirect Causal Effects of Spatiotemporal Interventions in Presence of Spatial.pdf:application/pdf},
}

@article{obukhov_identifying_2023,
	title = {Identifying {Conditioning} {Factors} and {Predictors} of {Conflict} {Likelihood} for {Machine} {Learning} {Models}: {A} {Literature} {Review}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2220-9964},
	shorttitle = {Identifying {Conditioning} {Factors} and {Predictors} of {Conflict} {Likelihood} for {Machine} {Learning} {Models}},
	url = {https://www.mdpi.com/2220-9964/12/8/322},
	doi = {10.3390/ijgi12080322},
	abstract = {In this research, we focused on armed conflicts and related violence. The study reviewed the use of machine learning to predict the likelihood of conflict escalation and the role of conditioning factors. The results showed that machine learning and predictive models could help identify conflict-prone locations and geospatial factors contributing to conflict escalation. The study found 46 relevant papers and emphasized the importance of considering unique predictors and conditioning factors for each conflict. It was found that the conflict susceptibility of a region is influenced principally by its socioeconomic conditions and its political/governance factors. We concluded that machine learning has the potential to be a valuable tool in conflict analysis and, therefore, it can be an asset in conflict mitigation and prevention, but the accuracy of the models depends on data quality and the careful selection of conditioning factors. Future research should aim to refine the methodology for more accurate prediction of the models.},
	language = {en},
	number = {8},
	urldate = {2025-12-15},
	journal = {ISPRS International Journal of Geo-Information},
	publisher = {Multidisciplinary Digital Publishing Institute},
	author = {Obukhov, Timur and Brovelli, Maria A.},
	month = aug,
	year = {2023},
	keywords = {Applied, conditioning factors, Confilct, conflict susceptibility, conflicts, machine learning, predictors, war},
	pages = {322},
	file = {Full Text PDF:/Users/nb/Zotero/storage/RQNA9I9X/Obukhov and Brovelli - 2023 - Identifying Conditioning Factors and Predictors of Conflict Likelihood for Machine Learning Models.pdf:application/pdf},
}

@misc{kaddour_causal_2022,
	title = {Causal {Machine} {Learning}: {A} {Survey} and {Open} {Problems}},
	shorttitle = {Causal {Machine} {Learning}},
	url = {http://arxiv.org/abs/2206.15475},
	doi = {10.48550/arXiv.2206.15475},
	abstract = {Causal Machine Learning (CausalML) is an umbrella term for machine learning methods that formalize the data-generation process as a structural causal model (SCM). This perspective enables us to reason about the effects of changes to this process (interventions) and what would have happened in hindsight (counterfactuals). We categorize work in CausalML into five groups according to the problems they address: (1) causal supervised learning, (2) causal generative modeling, (3) causal explanations, (4) causal fairness, and (5) causal reinforcement learning. We systematically compare the methods in each category and point out open problems. Further, we review data-modality-specific applications in computer vision, natural language processing, and graph representation learning. Finally, we provide an overview of causal benchmarks and a critical discussion of the state of this nascent field, including recommendations for future work.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Kaddour, Jean and Lynch, Aengus and Liu, Qi and Kusner, Matt J. and Silva, Ricardo},
	month = jul,
	year = {2022},
	note = {arXiv:2206.15475 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Methodology, CML, Survey},
	file = {Preprint PDF:/Users/nb/Zotero/storage/DRMYS6QU/Kaddour et al. - 2022 - Causal Machine Learning A Survey and Open Problems.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/E9I2CXZY/2206.html:text/html},
}

@misc{fuhr_estimating_2024,
	title = {Estimating {Causal} {Effects} with {Double} {Machine} {Learning} -- {A} {Method} {Evaluation}},
	url = {http://arxiv.org/abs/2403.14385},
	doi = {10.48550/arXiv.2403.14385},
	abstract = {The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - "double/debiased machine learning" (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification. When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods. From our overall results, we provide actionable recommendations for specific choices researchers must make when applying DML in practice.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Fuhr, Jonathan and Berens, Philipp and Papies, Dominik},
	month = apr,
	year = {2024},
	note = {arXiv:2403.14385 [stat]},
	keywords = {Computer Science - Machine Learning, Economics - Econometrics, Statistics - Machine Learning, Statistics - Methodology, Theory, DML, causal machine learning},
	file = {Preprint PDF:/Users/nb/Zotero/storage/A347IGQ5/Fuhr et al. - 2024 - Estimating Causal Effects with Double Machine Learning -- A Method Evaluation.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/YJAB6EM3/2403.html:text/html},
}

@article{chen_recent_2025,
	title = {Recent {Advances} in {Causal} {Machine} {Learning} and {Dynamic} {Policy} {Learning}},
	volume = {17},
	copyright = {© 2025 Wiley Periodicals LLC.},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.70050},
	doi = {10.1002/wics.70050},
	abstract = {Causal machine learning has emerged as a vital field at the intersection of machine learning and econometrics, addressing challenges in estimating treatment effect heterogeneity and optimizing policies. The first half of this review examines recent advances in causal machine learning within a static framework, covering methods such as meta-learners, double machine learning, and causal forests, which improve conditional average treatment effect estimation and support personalized decision-making. The second half focuses on dynamic policy learning, which integrates dynamic treatment regimes and reinforcement learning techniques to address sequential decision problems. We discuss value-based, policy-based, and model-based approaches in both online and offline environments, and conclude with the challenges of offline settings, fundamentally a causal inference problem, together with strategies to address them. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Exploratory Data Analysis Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Modeling Methods Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Classification and Regression Trees},
	language = {en},
	number = {4},
	urldate = {2025-12-15},
	journal = {WIREs Computational Statistics},
	author = {Chen, Jau-er and Jing, Annette},
	year = {2025},
	note = {\_eprint: https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.70050},
	keywords = {CATE, Survey, causal machine learning, causal forests, double machine learning, dynamic treatment regimes, meta learner, offline reinforcement learning, policy learning},
	pages = {e70050},
	file = {Full Text PDF:/Users/nb/Zotero/storage/22LMQQ7Q/Chen and Jing - 2025 - Recent Advances in Causal Machine Learning and Dynamic Policy Learning.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/D75QS4QD/wics.html:text/html},
}

@article{koch_primer_2025,
	title = {A {Primer} on {Deep} {Learning} for {Causal} {Inference}},
	volume = {54},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/00491241241234866},
	doi = {10.1177/00491241241234866},
	abstract = {This primer systematizes the emerging literature on causal inference using deep neural networks under the potential outcomes framework. It provides an intuitive introduction to building and optimizing custom deep learning models and shows how to adapt them to estimate/predict heterogeneous treatment effects. It also discusses ongoing work to extend causal inference to settings where confounding is nonlinear, time-varying, or encoded in text, networks, and images. To maximize accessibility, we also introduce prerequisite concepts from causal inference and deep learning. The primer differs from other treatments of deep learning and causal inference in its sharp focus on observational causal estimation, its extended exposition of key algorithms, and its detailed tutorials for implementing, training, and selecting among deep estimators in TensorFlow 2 and PyTorch.},
	language = {EN},
	number = {2},
	urldate = {2025-12-15},
	journal = {Sociological Methods \& Research},
	publisher = {SAGE Publications Inc},
	author = {Koch, Bernard J. and Sainburg, Tim and Geraldo Bastías, Pablo and Jiang, Song and Sun, Yizhou and Foster, Jacob G.},
	month = may,
	year = {2025},
	keywords = {causal inference, Theory},
	pages = {397--447},
	file = {Submitted Version:/Users/nb/Zotero/storage/H9NI9ED8/Koch et al. - 2025 - A Primer on Deep Learning for Causal Inference.pdf:application/pdf},
}

@article{jung_estimating_2021,
	title = {Estimating {Identifiable} {Causal} {Effects} through {Double} {Machine} {Learning}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17438},
	doi = {10.1609/aaai.v35i13.17438},
	abstract = {Identifying causal effects from observational data is a pervasive challenge found throughout the empirical sciences. Very general methods have been developed to decide the identifiability of a causal quantity from a combination of observational data and causal knowledge about the underlying system. In practice, however, there are still challenges to estimating identifiable causal functionals from finite samples. Recently, a method known as double/debiased machine learning (DML) (Chernozhukov et al. 2018) has been proposed to learn parameters leveraging modern machine learning techniques, which is both robust to model misspecification and bias-reducing. Still, DML has only been used for causal estimation in settings when the back-door condition (also known as conditional ignorability) holds. In this paper, we develop a new, general class of estimators for any identifiable causal functionals that exhibit DML properties, which we name DML-ID. In particular, we introduce a complete identification algorithm that returns an influence function (IF) for any identifiable causal functional. We then construct the DML estimator based on the derived IF. We show that DML-ID estimators hold the key properties of debiasedness and doubly robustness. Simulation results corroborate with the theory.},
	language = {en},
	number = {13},
	urldate = {2025-12-15},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Jung, Yonghan and Tian, Jin and Bareinboim, Elias},
	month = may,
	year = {2021},
	keywords = {DML, Causal Learning},
	pages = {12113--12122},
	file = {Full Text PDF:/Users/nb/Zotero/storage/NQ4NYAPW/Jung et al. - 2021 - Estimating Identifiable Causal Effects through Double Machine Learning.pdf:application/pdf},
}

@article{mulomba_applying_2025,
	title = {Applying {Causal} {Machine} {Learning} to {Spatiotemporal} {Data} {Analysis}: {An} {Investigation} of {Opportunities} and {Challenges}},
	volume = {13},
	issn = {2169-3536},
	shorttitle = {Applying {Causal} {Machine} {Learning} to {Spatiotemporal} {Data} {Analysis}},
	url = {https://ieeexplore.ieee.org/document/11119537},
	doi = {10.1109/ACCESS.2025.3596680},
	abstract = {Traditional spatiotemporal data analysis often relies on predictive models that overlook causal relationships, making it difficult to identify true drivers and formulate effective interventions. To bridge this gap, we review causal machine learning (CML) techniques for spatiotemporal data, aiming to provide robust insights into their unique advantages. Our literature review reveals that fewer than 1\% of studies in major databases explicitly integrate CML with spatiotemporal analysis. After rigorous screening, we analyze 51 relevant papers, categorizing their contributions into four key areas (totaling 62 methodological approaches due to multi-category papers): 1) causal effect discovery and estimation (32 approaches), 2) prediction accuracy enhancement (19), 3) pattern recognition limitations (10), and 4) interpretability (1). This distribution highlights a critical research gap, particularly in interpretability and comprehensive frameworks. We further examine unique challenges in spatiotemporal data, such as spatial autocorrelation and temporal dependencies, that complicate causal inference but also present opportunities for innovation. Promising approaches include the synergy of spatiotemporal Granger causality and structural equation modeling with spatial lags, which capture complex interdependencies while preserving interpretability. Future directions include developing interpretable causal models, advancing real-time causal inference in dynamic environments, and addressing computational challenges (scalability, efficiency, and complexity-interpretability trade-offs). We also discuss ethical considerations, such as bias mitigation in causal discovery and societal implications of spatiotemporal causal inference. By synthesizing challenges and opportunities, this work advances the application of CML in spatiotemporal analysis, with implications for climate science, economics, epidemiology, and urban planning.},
	urldate = {2025-12-15},
	journal = {IEEE Access},
	author = {Mulomba, Christian M. and Kiketa, Vogel M. and Kutangila, David M. and Mampuya, Pescie H. K. and Mukenze, Junior N. and Kasunzi, Landry M. and Kyamakya, Kyandoghere and Tashev, Tasho and Kasereka, Selain K.},
	year = {2025},
	keywords = {Machine learning, CML, Spatio Temporal Causal Inference, Survey, Causal machine learning, Cause effect analysis, Computational modeling, Data analysis, Estimation, ethics, Ethics, Mathematical models, Reviews, Spatial resolution, spatiotemporal data analysis, Spatiotemporal phenomena, synergy methods},
	pages = {141832--141857},
	file = {Full Text PDF:/Users/nb/Zotero/storage/58DVC4VG/Mulomba et al. - 2025 - Applying Causal Machine Learning to Spatiotemporal Data Analysis An Investigation of Opportunities.pdf:application/pdf},
}

@misc{klaassen_doublemldeep_2024,
	title = {{DoubleMLDeep}: {Estimation} of {Causal} {Effects} with {Multimodal} {Data}},
	shorttitle = {{DoubleMLDeep}},
	url = {http://arxiv.org/abs/2402.01785},
	doi = {10.48550/arXiv.2402.01785},
	abstract = {This paper explores the use of unstructured, multimodal data, namely text and images, in causal inference and treatment effect estimation. We propose a neural network architecture that is adapted to the double machine learning (DML) framework, specifically the partially linear model. An additional contribution of our paper is a new method to generate a semi-synthetic dataset which can be used to evaluate the performance of causal effect estimation in the presence of text and images as confounders. The proposed methods and architectures are evaluated on the semi-synthetic dataset and compared to standard approaches, highlighting the potential benefit of using text and images directly in causal studies. Our findings have implications for researchers and practitioners in economics, marketing, finance, medicine and data science in general who are interested in estimating causal quantities using non-traditional data.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Klaassen, Sven and Teichert-Kluge, Jan and Bach, Philipp and Chernozhukov, Victor and Spindler, Martin and Vijaykumar, Suhas},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01785 [cs]},
	keywords = {Computer Science - Machine Learning, Economics - Econometrics, Statistics - Machine Learning, Statistics - Methodology, causal inference, DML, Applied, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/nb/Zotero/storage/UM9WISXB/Klaassen et al. - 2024 - DoubleMLDeep Estimation of Causal Effects with Multimodal Data.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/TIBASG35/2402.html:text/html},
}

@article{bach_doubleml_2022,
	title = {{DoubleML} - {An} {Object}-{Oriented} {Implementation} of {Double} {Machine} {Learning} in {Python}},
	volume = {23},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v23/21-0862.html},
	abstract = {DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org.},
	number = {53},
	urldate = {2025-12-15},
	journal = {Journal of Machine Learning Research},
	author = {Bach, Philipp and Chernozhukov, Victor and Kurz, Malte S. and Spindler, Martin},
	year = {2022},
	keywords = {Theory, DML, Python},
	pages = {1--6},
	file = {Full Text PDF:/Users/nb/Zotero/storage/LK8Q7FZA/Bach et al. - 2022 - DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python.pdf:application/pdf;Source Code:/Users/nb/Zotero/storage/CB74EZH6/21-0862.html:text/html},
}

@article{bach_doubleml_2024,
	title = {{DoubleML}: {An} {Object}-{Oriented} {Implementation} of {Double} {Machine} {Learning} in {R}},
	volume = {108},
	copyright = {Copyright (c) 2024 Philipp Bach, Malte S. Kurz, Victor Chernozhukov, Martin Spindler, Sven Klaassen},
	issn = {1548-7660},
	shorttitle = {{DoubleML}},
	url = {https://doi.org/10.18637/jss.v108.i03},
	doi = {10.18637/jss.v108.i03},
	abstract = {The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consists of three key ingredients: Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R package DoubleML. In reproducible code examples with simulated and real data sets, we demonstrate how DoubleML users can perform valid inference based on machine learning methods.},
	language = {en},
	urldate = {2025-12-15},
	journal = {Journal of Statistical Software},
	author = {Bach, Philipp and Kurz, Malte S. and Chernozhukov, Victor and Spindler, Martin and Klaassen, Sven},
	month = feb,
	year = {2024},
	keywords = {Machine Learning, Theory, DML, Causal Inference, Causal Machine Learning, mlr3, Object Orientation, R},
	pages = {1--56},
	file = {Full Text PDF:/Users/nb/Zotero/storage/59EFJCPT/Bach et al. - 2024 - DoubleML An Object-Oriented Implementation of Double Machine Learning in R.pdf:application/pdf},
}

@article{chang_doubledebiased_2020,
	title = {Double/debiased machine learning for difference-in-differences models},
	volume = {23},
	issn = {1368-4221},
	url = {https://doi.org/10.1093/ectj/utaa001},
	doi = {10.1093/ectj/utaa001},
	abstract = {This paper provides an orthogonal extension of the semiparametric difference-in-differences estimator proposed in earlier literature. The proposed estimator enjoys the so-called Neyman orthogonality (Chernozhukov et al., 2018), and thus it allows researchers to flexibly use a rich set of machine learning methods in the first-step estimation. It is particularly useful when researchers confront a high-dimensional data set in which the number of potential control variables is larger than the sample size and the conventional nonparametric estimation methods, such as kernel and sieve estimators, do not apply. I apply this orthogonal difference-in-differences estimator to evaluate the effect of tariff reduction on corruption. The empirical results show that tariff reduction decreases corruption in large magnitude.},
	number = {2},
	urldate = {2025-12-15},
	journal = {The Econometrics Journal},
	author = {Chang, Neng-Chieh},
	month = may,
	year = {2020},
	keywords = {Theory, CML, DiD},
	pages = {177--191},
	file = {Full Text PDF:/Users/nb/Zotero/storage/ZN4VRDIF/Chang - 2020 - Doubledebiased machine learning for difference-in-differences models.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/6DA362M7/utaa001.html:text/html},
}

@article{clarke_double_2025,
	title = {Double machine learning for static panel models with fixed effects},
	issn = {1368-4221},
	url = {https://doi.org/10.1093/ectj/utaf011},
	doi = {10.1093/ectj/utaf011},
	abstract = {Recent advances in causal inference have seen the development of methods that make use of the predictive power of machine learning algorithms. In this paper, we develop novel double machine learning procedures for panel data in which these algorithms are used to approximate high-dimensional and non-linear nuisance functions of the covariates. Our new procedures are extensions of the well-known correlated random effects, within-group, and first-difference estimators from linear to non-linear panel models, specifically, the partially linear regression model with fixed effects and unspecified non-linear confounding. Our simulation study assesses the performance of these procedures using different machine learning algorithms. We use our procedures to re-estimate the impact of the introduction of the National Minimum Wage on voting behaviour in the United Kingdom. From our results, we recommend the use of first-differencing because it imposes the fewest constraints on the distribution of the fixed effects, and an ensemble learning strategy to ensure optimum estimator accuracy.},
	urldate = {2025-12-15},
	journal = {The Econometrics Journal},
	author = {Clarke, Paul S and Polselli, Annalivia},
	month = apr,
	year = {2025},
	keywords = {Theory, DML, Methodology extension},
	pages = {utaf011},
	file = {Full Text PDF:/Users/nb/Zotero/storage/XQPHJ7HP/Clarke and Polselli - 2025 - Double machine learning for static panel models with fixed effects.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/IMYXPC8I/utaf011.html:text/html},
}

@article{semenova_debiased_2021,
	title = {Debiased machine learning of conditional average treatment effects and other causal functions},
	volume = {24},
	issn = {1368-4221},
	url = {https://doi.org/10.1093/ectj/utaa027},
	doi = {10.1093/ectj/utaa027},
	abstract = {This paper provides estimation and inference methods for the best linear predictor (approximation) of a structural function, such as conditional average structural and treatment effects, and structural derivatives, based on modern machine learning tools. We represent this structural function as a conditional expectation of an unbiased signal that depends on a nuisance parameter, which we estimate by modern machine learning techniques. We first adjust the signal to make it insensitive (Neyman-orthogonal) with respect to the first-stage regularisation bias. We then project the signal onto a set of basis functions, which grow with sample size, to get the best linear predictor of the structural function. We derive a complete set of results for estimation and simultaneous inference on all parameters of the best linear predictor, conducting inference by Gaussian bootstrap. When the structural function is smooth and the basis is sufficiently rich, our estimation and inference results automatically target this function. When basis functions are group indicators, the best linear predictor reduces to the group average treatment/structural effect, and our inference automatically targets these parameters. We demonstrate our method by estimating uniform confidence bands for the average price elasticity of gasoline demand conditional on income.},
	number = {2},
	urldate = {2025-12-15},
	journal = {The Econometrics Journal},
	author = {Semenova, Vira and Chernozhukov, Victor},
	month = may,
	year = {2021},
	keywords = {Theory, DML, Methodology extension},
	pages = {264--289},
	file = {Full Text PDF:/Users/nb/Zotero/storage/E57XDRIK/Semenova and Chernozhukov - 2021 - Debiased machine learning of conditional average treatment effects and other causal functions.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/D7FAH2WI/utaa027.html:text/html},
}

@misc{mukaigawara_spatiotemporal_2025,
	title = {Spatiotemporal causal inference with arbitrary spillover and carryover effects: {Airstrikes} and insurgent violence in the {Iraq} {War}},
	shorttitle = {Spatiotemporal causal inference with arbitrary spillover and carryover effects},
	url = {http://arxiv.org/abs/2504.03464},
	doi = {10.48550/arXiv.2504.03464},
	abstract = {Social scientists now routinely draw on high-frequency, high-granularity ''microlevel'' data to estimate the causal effects of subnational interventions. To date, most researchers aggregate these data into panels, often tied to large-scale administrative units. This approach has two limitations. First, data (over)aggregation obscures valuable spatial and temporal information, heightening the risk of mistaken inferences. Second, existing panel approaches either ignore spatial spillover and temporal carryover effects completely or impose overly restrictive assumptions. We introduce a general methodological framework and an accompanying open-source R package, geocausal, that enable spatiotemporal causal inference with arbitrary spillover and carryover effects. Using this framework, we demonstrate how to define and estimate causal quantities of interest, explore heterogeneous treatment effects, conduct causal mediation analysis, and perform data visualization. We apply our methodology to the Iraq War (2003-11), where we reexamine long-standing questions about the effects of airstrikes on insurgent violence.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Mukaigawara, Mitsuru and Imai, Kosuke and Lyall, Jason and Papadogeorgou, Georgia},
	month = nov,
	year = {2025},
	note = {arXiv:2504.03464 [stat]},
	keywords = {Statistics - Methodology, Applied, Confilct, Spatio Temporal Causal Inference, R, Statistics - Applications},
	file = {Preprint PDF:/Users/nb/Zotero/storage/SFPTDCG2/Mukaigawara et al. - 2025 - Spatiotemporal causal inference with arbitrary spillover and carryover effects Airstrikes and insur.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/7TWH5VF8/2504.html:text/html},
}

@article{ge_modelling_2022,
	title = {Modelling armed conflict risk under climate change with machine learning and time-series data},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-30356-x},
	doi = {10.1038/s41467-022-30356-x},
	abstract = {Understanding the risk of armed conflict is essential for promoting peace. Although the relationship between climate variability and armed conflict has been studied by the research community for decades with quantitative and qualitative methods at different spatial and temporal scales, causal linkages at a global scale remain poorly understood. Here we adopt a quantitative modelling framework based on machine learning to infer potential causal linkages from high-frequency time-series data and simulate the risk of armed conflict worldwide from 2000–2015. Our results reveal that the risk of armed conflict is primarily influenced by stable background contexts with complex patterns, followed by climate deviations related covariates. The inferred patterns show that positive temperature deviations or precipitation extremes are associated with increased risk of armed conflict worldwide. Our findings indicate that a better understanding of climate-conflict linkages at the global scale enhances the spatiotemporal modelling capacity for the risk of armed conflict.},
	language = {en},
	number = {1},
	urldate = {2025-12-15},
	journal = {Nature Communications},
	publisher = {Nature Publishing Group},
	author = {Ge, Quansheng and Hao, Mengmeng and Ding, Fangyu and Jiang, Dong and Scheffran, Jürgen and Helman, David and Ide, Tobias},
	month = may,
	year = {2022},
	keywords = {Applied, Climate-change impacts, Confilct, ML, Society, time series},
	pages = {2839},
	file = {Full Text PDF:/Users/nb/Zotero/storage/EG2AQ99L/Ge et al. - 2022 - Modelling armed conflict risk under climate change with machine learning and time-series data.pdf:application/pdf},
}

@article{reich_review_2021,
	title = {A {Review} of {Spatial} {Causal} {Inference} {Methods} for {Environmental} and {Epidemiological} {Applications}},
	volume = {89},
	issn = {0306-7734},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC10187770/},
	doi = {10.1111/insr.12452},
	abstract = {The scientific rigor and computational methods of causal inference have had great impacts on many disciplines but have only recently begun to take hold in spatial applications. Spatial causal inference poses analytic challenges due to complex correlation structures and interference between the treatment at one location and the outcomes at others. In this paper, we review the current literature on spatial causal inference and identify areas of future work. We first discuss methods that exploit spatial structure to account for unmeasured confounding variables. We then discuss causal analysis in the presence of spatial interference including several common assumptions used to reduce the complexity of the interference patterns under consideration. These methods are extended to the spatiotemporal case where we compare and contrast the potential outcomes framework with Granger causality and to geostatistical analyses involving spatial random fields of treatments and responses. The methods are introduced in the context of observational environmental and epidemiological studies and are compared using both a simulation study and analysis of the effect of ambient air pollution on COVID-19 mortality rate. Code to implement many of the methods using the popular Bayesian software OpenBUGS is provided.},
	number = {3},
	urldate = {2025-12-15},
	journal = {International statistical review = Revue internationale de statistique},
	author = {Reich, Brian J. and Yang, Shu and Guan, Yawen and Giffin, Andrew B. and Miller, Matthew J. and Rappold, Ana},
	month = dec,
	year = {2021},
	keywords = {Applied, Spatio Temporal Causal Inference},
	pages = {605--634},
	file = {Submitted Version:/Users/nb/Zotero/storage/2Y4FAH8U/Reich et al. - 2021 - A Review of Spatial Causal Inference Methods for Environmental and Epidemiological Applications.pdf:application/pdf},
}

@misc{butts_difference--differences_2023,
	title = {Difference-in-{Differences} {Estimation} with {Spatial} {Spillovers}},
	url = {http://arxiv.org/abs/2105.03737},
	doi = {10.48550/arXiv.2105.03737},
	abstract = {Empirical work often uses treatment assigned following geographic boundaries. When the effects of treatment cross over borders, classical difference-in-differences estimation produces biased estimates for the average treatment effect. In this paper, I introduce a potential outcomes framework to model spillover effects and decompose the estimate's bias in two parts: (1) the control group no longer identifies the counterfactual trend because their outcomes are affected by treatment and (2) changes in treated units' outcomes reflect the effect of their own treatment status and the effect from the treatment status of 'close' units. I propose conditions for non-parametric identification that can remove both sources of bias and semi-parametrically estimate the spillover effects themselves including in settings with staggered treatment timing. To highlight the importance of spillover effects, I revisit analyses of three place-based interventions.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Butts, Kyle},
	month = jun,
	year = {2023},
	note = {arXiv:2105.03737 [econ]},
	keywords = {Economics - Econometrics, Spatio Temporal Causal Inference, DiD},
	file = {Preprint PDF:/Users/nb/Zotero/storage/9LZJBGYF/Butts - 2023 - Difference-in-Differences Estimation with Spatial Spillovers.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/ZSG53GS6/2105.html:text/html},
}

@article{credit_structured_2024,
	title = {A structured comparison of causal machine learning methods to assess heterogeneous treatment effects in spatial data},
	volume = {26},
	issn = {1435-5949},
	url = {https://doi.org/10.1007/s10109-023-00413-0},
	doi = {10.1007/s10109-023-00413-0},
	abstract = {The development of the “causal” forest by Wager and Athey (J Am Stat Assoc 113(523): 1228–1242, 2018) represents a significant advance in the area of explanatory/causal machine learning. However, this approach has not yet been widely applied to geographically referenced data, which present some unique issues: the random split of the test and training sets in the typical causal forest design fractures the spatial fabric of geographic data. To help solve this issue, we use a simulated dataset with known properties for average treatment effects and conditional average treatment effects to compare the performance of CF models across different definitions of the test/train split. We also develop a new “spatial” T-learner that can be implemented using predictive methods like random forest to provide estimates of heterogeneous treatment effects across all units. Our results show that all of the machine learning models outperform traditional ordinary least squares regression at identifying the true average treatment effect, but are not significantly different from one another. We then apply the preferred causal forest model in the context of analysing the treatment effect of the construction of the Valley Metro light rail (tram) system on on-road CO2 emissions per capita at the block group level in Maricopa County, Arizona, and find that the neighbourhoods most likely to benefit from treatment are those with higher pre-treatment proportions of transit and pedestrian commuting and lower proportions of auto commuting.},
	language = {en},
	number = {4},
	urldate = {2025-12-15},
	journal = {Journal of Geographical Systems},
	author = {Credit, Kevin and Lehnert, Matthew},
	month = oct,
	year = {2024},
	keywords = {Machine learning, CML, Spatio Temporal Causal Inference, Survey, C21, C52, C54, C63, Causal forest, Causal inference, CO2 emissions, Heterogeneous treatment effects, Spatial, Transit},
	pages = {483--510},
	file = {Full Text PDF:/Users/nb/Zotero/storage/SWQ8EP9I/Credit and Lehnert - 2024 - A structured comparison of causal machine learning methods to assess heterogeneous treatment effects.pdf:application/pdf},
}

@article{kunzel_metalearners_2019,
	title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
	volume = {116},
	copyright = {Copyright © 2019 the Author(s). Published by PNAS.},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1804597116},
	doi = {10.1073/pnas.1804597116},
	abstract = {There is growing interest in estimating and analyzing heterogeneous treatment effects
in experimental and observational studies. We describe a numb...},
	language = {EN},
	number = {10},
	urldate = {2025-12-15},
	journal = {Proceedings of the National Academy of Sciences},
	publisher = {Proceedings of the National Academy of Sciences},
	author = {Künzel, Sören R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
	month = mar,
	year = {2019},
	note = {Company: National Academy of Sciences
Distributor: National Academy of Sciences
Institution: National Academy of Sciences
Label: National Academy of Sciences},
	keywords = {meta learner, Theory},
	pages = {4156--4165},
	file = {Full Text PDF:/Users/nb/Zotero/storage/I7JN666D/Künzel et al. - 2019 - Metalearners for estimating heterogeneous treatment effects using machine learning.pdf:application/pdf},
}

@misc{jerzak_effect_2024,
	title = {Effect {Heterogeneity} with {Earth} {Observation} in {Randomized} {Controlled} {Trials}: {Exploring} the {Role} of {Data}, {Model}, and {Evaluation} {Metric} {Choice}},
	shorttitle = {Effect {Heterogeneity} with {Earth} {Observation} in {Randomized} {Controlled} {Trials}},
	url = {http://arxiv.org/abs/2407.11674},
	doi = {10.48550/arXiv.2407.11674},
	abstract = {Many social and environmental phenomena are associated with macroscopic changes in the built environment, captured by satellite imagery on a global scale and with daily temporal resolution. While widely used for prediction, these images and especially image sequences remain underutilized for causal inference, especially in the context of randomized controlled trials (RCTs), where causal identification is established by design. In this paper, we develop and compare a set of general tools for analyzing Conditional Average Treatment Effects (CATEs) from temporal satellite data that can be applied to any RCT where geographical identifiers are available. Through a simulation study, we analyze different modeling strategies for estimating CATE in sequences of satellite images. We find that image sequence representation models with more parameters generally yield a greater ability to detect heterogeneity. To explore the role of model and data choice in practice, we apply the approaches to two influential RCTs -- Banerjee et al. (2015), a poverty study in Cusco, Peru, and Bolsen et al. (2014), a water conservation experiment in Georgia, USA. We benchmark our image sequence models against image-only, tabular-only, and combined image-tabular data sources, summarizing practical implications for investigators in a multivariate analysis. Land cover classifications over satellite images facilitate interpretation of what image features drive heterogeneity. We also show robustness to data and model choice of satellite-based generalization of the RCT results to larger geographical areas outside the original. Overall, this paper shows how satellite sequence data can be incorporated into the analysis of RCTs, and provides evidence about the implications of data, model, and evaluation metric choice for causal analysis.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Jerzak, Connor T. and Vashistha, Ritwik and Daoud, Adel},
	month = jul,
	year = {2024},
	note = {arXiv:2407.11674 [stat]},
	keywords = {Statistics - Methodology, Remote sensing, Applied, Spatio Temporal Causal Inference, R},
	file = {Preprint PDF:/Users/nb/Zotero/storage/8GEDADHZ/Jerzak et al. - 2024 - Effect Heterogeneity with Earth Observation in Randomized Controlled Trials Exploring the Role of D.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/32ZN2R2C/2407.html:text/html},
}

@misc{pettersson_debiasing_2025,
	title = {Debiasing {Machine} {Learning} {Predictions} for {Causal} {Inference} {Without} {Additional} {Ground} {Truth} {Data}: "{One} {Map}, {Many} {Trials}" in {Satellite}-{Driven} {Poverty} {Analysis}},
	shorttitle = {Debiasing {Machine} {Learning} {Predictions} for {Causal} {Inference} {Without} {Additional} {Ground} {Truth} {Data}},
	url = {http://arxiv.org/abs/2508.01341},
	doi = {10.48550/arXiv.2508.01341},
	abstract = {Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a "one map, many trials" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Pettersson, Markus B. and Jerzak, Connor T. and Daoud, Adel},
	month = nov,
	year = {2025},
	note = {arXiv:2508.01341 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, causal inference, Applied},
	file = {Preprint PDF:/Users/nb/Zotero/storage/F43G4TL4/Pettersson et al. - 2025 - Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data O.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/PL75KRST/2508.html:text/html},
}

@article{miguel_economic_2004,
	title = {Economic {Shocks} and {Civil} {Conflict}: {An} {Instrumental} {Variables} {Approach}},
	volume = {112},
	issn = {0022-3808, 1537-534X},
	shorttitle = {Economic {Shocks} and {Civil} {Conflict}},
	url = {https://www.journals.uchicago.edu/doi/10.1086/421174},
	doi = {10.1086/421174},
	abstract = {Determining the impact of poverty on the likelihood of civil conflict in less developed countries is difficult because of omitted variable bias and endogeneity. We use exogenous weather variation – as measured in satellite vegetation readings – as an instrumental variable for economic growth in 40 Sub-Saharan African countries during 1983-1999, and estimate that economic growth is strongly negatively related to the incidence of civil conflict: a negative growth shock of 5 percentage points increases the likelihood of major civil conflicts by roughly one-half. This relationship is not significantly different in countries that have higher per capita income, that are more democratic or more ethnically diverse. We use a new and comprehensive dataset of civil conflict in the analysis.},
	language = {en},
	number = {4},
	urldate = {2025-12-15},
	journal = {Journal of Political Economy},
	author = {Miguel, Edward and Satyanath, Shanker and Sergenti, Ernest},
	month = aug,
	year = {2004},
	keywords = {Confilct},
	pages = {725--753},
	file = {PDF:/Users/nb/Zotero/storage/ZMMHYNY7/Miguel et al. - 2004 - Economic Shocks and Civil Conflict An Instrumental Variables Approach.pdf:application/pdf},
}

@article{nunn_us_2014,
	title = {{US} {Food} {Aid} and {Civil} {Conflict}},
	volume = {104},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.104.6.1630},
	doi = {10.1257/aer.104.6.1630},
	abstract = {We study the effect of U.S. food aid on conflict in recipient countries. Our analysis exploits
time variation in food aid shipments due to changes in U.S. wheat production and cross-sectional
variation in a country's tendency to receive any U.S. food aid. According to our estimates, an
increase in U.S. food aid increases the incidence and duration of civil conflicts, but has no robust
effect on inter-state conflicts or the onset of civil conflicts. We also provide suggestive evidence
that the effects are most pronounced in countries with a recent history of civil conflict.},
	language = {en},
	number = {6},
	urldate = {2025-12-15},
	journal = {American Economic Review},
	author = {Nunn, Nathan and Qian, Nancy},
	month = jun,
	year = {2014},
	keywords = {Confilct, Agricultural Policy; Food Policy, Agriculture: Aggregate Supply and Demand Analysis; Prices, Alliances, Foreign Aid, Formal and Informal Sectors, Conflict, Conflict Resolution, Conflict; Conflict Resolution; Alliances, Food Policy, Foreign Aid, Formal and Informal Sectors; Shadow Economy; Institutional Arrangements, Institutional Arrangements, International Linkages to Development, International Linkages to Development; Role of International Organizations, Prices, Agricultural Policy, Role of International Organizations, Agriculture: Aggregate Supply and Demand Analysis, Shadow Economy},
	pages = {1630--1666},
	file = {Submitted Version:/Users/nb/Zotero/storage/W9DXLRS8/Nunn and Qian - 2014 - US Food Aid and Civil Conflict.pdf:application/pdf},
}

@article{blackwell_how_2018,
	title = {How to {Make} {Causal} {Inferences} with {Time}-{Series} {Cross}-{Sectional} {Data} under {Selection} on {Observables}},
	volume = {112},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0003-0554, 1537-5943},
	url = {https://www.cambridge.org/core/product/identifier/S0003055418000357/type/journal_article},
	doi = {10.1017/S0003055418000357},
	abstract = {Repeated measurements of the same countries, people, or groups over time are vital to many fields of political science. These measurements, sometimes called time-series cross-sectional (TSCS) data, allow researchers to estimate a broad set of causal quantities, including contemporaneous effects and direct effects of lagged treatments. Unfortunately, popular methods for TSCS data can only produce valid inferences for lagged effects under some strong assumptions. In this paper, we use potential outcomes to define causal quantities of interest in these settings and clarify how standard models like the autoregressive distributed lag model can produce biased estimates of these quantities due to post-treatment conditioning. We then describe two estimation strategies that avoid these post-treatment biases—inverse probability weighting and structural nested mean models—and show via simulations that they can outperform standard approaches in small sample settings. We illustrate these methods in a study of how welfare spending affects terrorism.},
	language = {en},
	number = {4},
	urldate = {2025-12-15},
	journal = {American Political Science Review},
	author = {Blackwell, Matthew and Glynn, Adam N.},
	month = nov,
	year = {2018},
	keywords = {time series, Spatio Temporal Causal Inference},
	pages = {1067--1082},
	file = {PDF:/Users/nb/Zotero/storage/XD2LPGHE/Blackwell and Glynn - 2018 - How to Make Causal Inferences with Time-Series Cross-Sectional Data under Selection on Observables.pdf:application/pdf},
}

@misc{pollmann_causal_2023,
	title = {Causal {Inference} for {Spatial} {Treatments}},
	url = {http://arxiv.org/abs/2011.00373},
	doi = {10.48550/arXiv.2011.00373},
	abstract = {Many events and policies (treatments) occur at specific spatial locations, with researchers interested in their effects on nearby units of interest. I approach the spatial treatment setting from an experimental perspective: What ideal experiment would we design to estimate the causal effects of spatial treatments? This perspective motivates a comparison between individuals near realized treatment locations and individuals near counterfactual (unrealized) candidate locations, which differs from current empirical practice. I derive design-based standard errors that are straightforward to compute irrespective of spatial correlations in outcomes. Furthermore, I propose machine learning methods to find counterfactual candidate locations using observational data under unconfounded assignment of the treatment to locations. I apply the proposed methods to study the causal effects of grocery stores on foot traffic to nearby businesses during COVID-19 shelter-in-place policies, finding a substantial positive effect at a very short distance, with no effect at larger distances.},
	language = {en},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Pollmann, Michael},
	month = jan,
	year = {2023},
	note = {arXiv:2011.00373 [econ]},
	keywords = {Economics - Econometrics, Statistics - Methodology, causal inference, Spatio Temporal Causal Inference},
	file = {PDF:/Users/nb/Zotero/storage/5XQIP745/Pollmann - 2023 - Causal Inference for Spatial Treatments.pdf:application/pdf},
}

@article{athey_generalized_2019,
	title = {Generalized random forests},
	volume = {47},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-2/Generalized-random-forests/10.1214/18-AOS1709.full},
	doi = {10.1214/18-AOS1709},
	abstract = {We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001) 5–32]) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: nonparametric quantile regression, conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.},
	number = {2},
	urldate = {2025-12-15},
	journal = {The Annals of Statistics},
	publisher = {Institute of Mathematical Statistics},
	author = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
	month = apr,
	year = {2019},
	keywords = {62G05, Asymptotic theory, Causal inference, instrumental variable, Random forests, Theory},
	pages = {1148--1178},
	file = {Full Text PDF:/Users/nb/Zotero/storage/RKW9PES7/Athey et al. - 2019 - Generalized random forests.pdf:application/pdf},
}

@misc{nie_quasi-oracle_2020,
	title = {Quasi-{Oracle} {Estimation} of {Heterogeneous} {Treatment} {Effects}},
	url = {http://arxiv.org/abs/1712.04912},
	doi = {10.48550/arXiv.1712.04912},
	abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: In both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Nie, Xinkun and Wager, Stefan},
	month = aug,
	year = {2020},
	note = {arXiv:1712.04912 [stat]},
	keywords = {Economics - Econometrics, Statistics - Machine Learning, Theory, Mathematics - Statistics Theory, meta learner},
	file = {Preprint PDF:/Users/nb/Zotero/storage/FVJ2WTJD/Nie and Wager - 2020 - Quasi-Oracle Estimation of Heterogeneous Treatment Effects.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/JHVNA2P2/1712.html:text/html},
}

@misc{kennedy_towards_2023,
	title = {Towards optimal doubly robust estimation of heterogeneous causal effects},
	url = {http://arxiv.org/abs/2004.14497},
	doi = {10.48550/arXiv.2004.14497},
	abstract = {Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Kennedy, Edward H.},
	month = aug,
	year = {2023},
	note = {arXiv:2004.14497 [math]},
	keywords = {Theory, Mathematics - Statistics Theory, Methodology extension},
	file = {Preprint PDF:/Users/nb/Zotero/storage/NTIUEDJL/Kennedy - 2023 - Towards optimal doubly robust estimation of heterogeneous causal effects.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/GNG7QU9E/2004.html:text/html},
}

@misc{athey_policy_2020,
	title = {Policy {Learning} with {Observational} {Data}},
	url = {http://arxiv.org/abs/1702.02896},
	doi = {10.48550/arXiv.1702.02896},
	abstract = {In many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisfies application-specific constraints, such as budget, fairness, simplicity, or other functional form constraints. For example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. We propose a new approach to this problem motivated by the theory of semiparametrically efficient estimation. Our method can be used to optimize either binary treatments or infinitesimal nudges to continuous treatments, and can leverage observational data where causal effects are identified using a variety of strategies, including selection on observables and instrumental variables. Given a doubly robust estimator of the causal effect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.},
	urldate = {2025-12-15},
	publisher = {arXiv},
	author = {Athey, Susan and Wager, Stefan},
	month = sep,
	year = {2020},
	note = {arXiv:1702.02896 [math]},
	keywords = {Computer Science - Machine Learning, Economics - Econometrics, Statistics - Machine Learning, Theory, Mathematics - Statistics Theory, policy learning},
	file = {Preprint PDF:/Users/nb/Zotero/storage/5KV7BF9U/Athey and Wager - 2020 - Policy Learning with Observational Data.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/3ICQK8FZ/1702.html:text/html},
}

@article{knaus_machine_2021,
	title = {Machine learning estimation of heterogeneous causal effects: {Empirical} {Monte} {Carlo} evidence},
	volume = {24},
	issn = {1368-4221},
	shorttitle = {Machine learning estimation of heterogeneous causal effects},
	url = {https://doi.org/10.1093/ectj/utaa014},
	doi = {10.1093/ectj/utaa014},
	abstract = {We investigate the finite-sample performance of causal machine learning estimators for heterogeneous causal effects at different aggregation levels. We employ an empirical Monte Carlo study that relies on arguably realistic data generation processes (DGPs) based on actual data in an observational setting. We consider 24 DGPs, eleven causal machine learning estimators, and three aggregation levels of the estimated effects. Four of the considered estimators perform consistently well across all DGPs and aggregation levels. These estimators have multiple steps to account for the selection into the treatment and the outcome process.},
	number = {1},
	urldate = {2025-12-16},
	journal = {The Econometrics Journal},
	author = {Knaus, Michael C and Lechner, Michael and Strittmatter, Anthony},
	month = jan,
	year = {2021},
	pages = {134--161},
	file = {Full Text PDF:/Users/nb/Zotero/storage/LND7NPZM/Knaus et al. - 2021 - Machine learning estimation of heterogeneous causal effects Empirical Monte Carlo evidence.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/IBQBJZH6/utaa014.html:text/html},
}

@inproceedings{dorazio_modeling_2019,
	title = {Modeling and {Forecasting} {Armed} {Conflict}: {AutoML} with {Human}-{Guided} {Machine} {Learning}},
	shorttitle = {Modeling and {Forecasting} {Armed} {Conflict}},
	url = {https://ieeexplore.ieee.org/document/9005963},
	doi = {10.1109/BigData47090.2019.9005963},
	abstract = {Machine learning has made slow inroads into quantitative social science due to both a mismatch of machine learning's strengths to the causal and inferential tasks domain researchers pursue [1] and also a lack of algorithmic training among many domain experts [2]. However, conflict research- the empirical examination of political unrest, violence and civil war-has seen a growing emphasis on prediction and forecasting models. We describe automated machine learning (AutoML) to identify models, and human-guided machine learning (HGML), and show how these can incorporate domain knowledge and research requirements into model selection and assessment, and provide high quality machine learning pipelines to domain experts comparable to state-of-the-literature solutions. We examine three peer-reviewed papers with predictive models of conflict [3, 4, 5] and run their data through our HGML system using multiple AutoML engines and find this system produces slightly elevated performance on each paper's model, without any ML expertise required of the user. Our research has three takeaways for computational social science. First, predictive models of conflict would benefit from even minimal applications of AutoML; Secondly, human-guided machine learning offers the attractive option of constraining AutoML systems to address the kinds of questions conflict researchers assess with predictive models; Finally, current existing AutoML implementations produce divergent solutions and so can be productively harnessed in parallel.},
	urldate = {2025-12-16},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {D’Orazio, Vito and Honaker, James and Prasady, Raman and Shoemate, Michael},
	month = dec,
	year = {2019},
	keywords = {Forecasting, Machine learning, Automated machine learning (AutoML), conflict forecasting, Data models, Human-guided machine learning, Logistics, Predictive models, Task analysis, Testing},
	pages = {4714--4723},
	file = {Full Text PDF:/Users/nb/Zotero/storage/T67R3E9B/D’Orazio et al. - 2019 - Modeling and Forecasting Armed Conflict AutoML with Human-Guided Machine Learning.pdf:application/pdf},
}

@article{murphy_promise_2024,
	title = {The promise of machine learning in violent conflict forecasting},
	volume = {6},
	issn = {2632-3249},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/promise-of-machine-learning-in-violent-conflict-forecasting/40D559ADA18FF7308915B08956B4E8F3},
	doi = {10.1017/dap.2024.27},
	abstract = {In 2022, the world experienced the deadliest year of armed conflict since the 1994 Rwandan genocide. Much of the intensity and frequency of recent conflicts has drawn more attention to failures in forecasting—that is, a failure to anticipate conflicts. Such capabilities have the potential to greatly reduce the time, motivation, and opportunities peacemakers have to intervene through mediation or peacekeeping operations. In recent years, the growth in the volume of open-source data coupled with the wide-scale advancements in machine learning suggests that it may be possible for computational methods to help the international community forecast intrastate conflict more accurately, and in doing so reduce the rise of conflict. In this commentary, we argue for the promise of conflict forecasting under several technical and policy conditions. From a technical perspective, the success of this work depends on improvements in the quality of conflict-related data and an increased focus on model interpretability. In terms of policy implementation, we suggest that this technology should be used primarily to aid policy analysis heuristically and help identify unexpected conflicts.},
	language = {en},
	urldate = {2025-12-16},
	journal = {Data \& Policy},
	author = {Murphy, Max and Sharpe, Ezra and Huang, Kayla},
	month = jan,
	year = {2024},
	keywords = {machine learning, artificial intelligence, forecasting, peacemaking, violent conflict},
	pages = {e35},
	file = {Full Text PDF:/Users/nb/Zotero/storage/FFTJSY5Q/Murphy et al. - 2024 - The promise of machine learning in violent conflict forecasting.pdf:application/pdf},
}

@inproceedings{kuzmanovic_causal_2024,
	address = {New York, NY, USA},
	series = {{KDD} '24},
	title = {Causal {Machine} {Learning} for {Cost}-{Effective} {Allocation} of {Development} {Aid}},
	isbn = {979-8-4007-0490-1},
	url = {https://dl.acm.org/doi/10.1145/3637528.3671551},
	doi = {10.1145/3637528.3671551},
	abstract = {The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by "leaving no one behind", and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. For this, we first show that our framework successfully computes heterogeneous treatment-response curves using semi-synthetic data. Then, we demonstrate our framework using real-world HIV data. Our framework points to large opportunities for a more effective aid allocation, suggesting that the total number of new HIV infections could be reduced by up to 3.3\% ({\textasciitilde}50,000 cases) compared to the current allocation practice.},
	urldate = {2025-12-15},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Kuzmanovic, Milan and Frauen, Dennis and Hatt, Tobias and Feuerriegel, Stefan},
	month = aug,
	year = {2024},
	pages = {5283--5294},
	file = {Full Text PDF:/Users/nb/Zotero/storage/JUQANM36/Kuzmanovic et al. - 2024 - Causal Machine Learning for Cost-Effective Allocation of Development Aid.pdf:application/pdf},
}

@article{mhanna_using_2023,
	title = {Using machine learning and remote sensing to track land use/land cover changes due to armed conflict},
	volume = {898},
	issn = {0048-9697},
	url = {https://www.sciencedirect.com/science/article/pii/S0048969723042237},
	doi = {10.1016/j.scitotenv.2023.165600},
	abstract = {Armed conflicts have detrimental impacts on the environment, including land systems. The prevailing understanding of the relation between Land Use/Land Cover (LULC) and armed conflict fails to fully recognize the complexity of their dynamics – a shortcoming that could undermine food security and sustainable land/water resources management in conflict settings. The Syrian portion of the transboundary Orontes River Basin (ORB) has been a site of violent conflict since 2013. Correspondingly, the Lebanese and Turkish portions of the ORB have seen large influxes of refugees. A major challenge in any geoscientific investigation in this region, specifically the Syrian portion, is the unavailability of directly-measured “ground truth” data. To circumvent this problem, we develop a novel methodology that combines remote sensing products, machine learning techniques and quasi-experimental statistical analysis to better understand LULC changes in the ORB between 2004 and 2022. Through analysis of the resulting annual LULC maps, we can draw several quantitative conclusions. Cropland areas decreased by 21–24 \% in Syria's conflict hotspot zones after 2013, whereas a 3.4-fold increase was detected in Lebanon. The development of refugee settlements was also tracked in Lebanon and on the Syrian/Turkish borders, revealing different LULC patterns that depend on settlement dynamics. The results highlight the importance of understanding the heterogenous spatio-temporal LULC changes in conflict-affected and refugee-hosting countries. The developed methodology is a flexible, cloud-based approach that can be applied to wide variety of LULC investigations related to conflict, policy and climate.},
	urldate = {2025-12-16},
	journal = {Science of The Total Environment},
	author = {Mhanna, Saeed and Halloran, Landon J. S. and Zwahlen, François and Asaad, Ahmed Haj and Brunner, Philip},
	month = nov,
	year = {2023},
	keywords = {Syria, Croplands, Difference-in-differences, Google earth engine, Land use/land cover change, Orontes River basin},
	pages = {165600},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/HMXRD2KY/Mhanna et al. - 2023 - Using machine learning and remote sensing to track land useland cover changes due to armed conflict.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/8JXY4UPG/S0048969723042237.html:text/html},
}

@misc{maase_next-generation_2025,
	title = {Next-{Generation} {Conflict} {Forecasting}: {Unleashing} {Predictive} {Patterns} through {Spatiotemporal} {Learning}},
	shorttitle = {Next-{Generation} {Conflict} {Forecasting}},
	url = {http://arxiv.org/abs/2506.14817},
	doi = {10.48550/arXiv.2506.14817},
	abstract = {Forecasting violent conflict at high spatial and temporal resolution remains a central challenge for both researchers and policymakers. This study presents a novel neural network architecture for forecasting three distinct types of violence -- state-based, non-state, and one-sided -- at the subnational (priogrid-month) level, up to 36 months in advance. The model jointly performs classification and regression tasks, producing both probabilistic estimates and expected magnitudes of future events. It achieves state-of-the-art performance across all tasks and generates approximate predictive posterior distributions to quantify forecast uncertainty. The architecture is built on a Monte Carlo Dropout Long Short-Term Memory (LSTM) U-Net, integrating convolutional layers to capture spatial dependencies with recurrent structures to model temporal dynamics. Unlike many existing approaches, it requires no manual feature engineering and relies solely on historical conflict data. This design enables the model to autonomously learn complex spatiotemporal patterns underlying violent conflict. Beyond achieving state-of-the-art predictive performance, the model is also highly extensible: it can readily integrate additional data sources and jointly forecast auxiliary variables. These capabilities make it a promising tool for early warning systems, humanitarian response planning, and evidence-based peacebuilding initiatives.},
	urldate = {2025-12-16},
	publisher = {arXiv},
	author = {Maase, Simon P. von der},
	month = jun,
	year = {2025},
	note = {arXiv:2506.14817 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Applications, Computer Science - Computers and Society, Statistics - Other Statistics},
	file = {Preprint PDF:/Users/nb/Zotero/storage/M3DHYIC6/Maase - 2025 - Next-Generation Conflict Forecasting Unleashing Predictive Patterns through Spatiotemporal Learning.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/CLZG3C97/2506.html:text/html},
}

@misc{sun_revealing_2022,
	title = {Revealing the {Excitation} {Causality} between {Climate} and {Political} {Violence} via a {Neural} {Forward}-{Intensity} {Poisson} {Process}},
	url = {http://arxiv.org/abs/2203.04511},
	doi = {10.48550/arXiv.2203.04511},
	abstract = {The causal mechanism between climate and political violence is fraught with complex mechanisms. Current quantitative causal models rely on one or more assumptions: (1) the climate drivers persistently generate conflict, (2) the causal mechanisms have a linear relationship with the conflict generation parameter, and/or (3) there is sufficient data to inform the prior distribution. Yet, we know conflict drivers often excite a social transformation process which leads to violence (e.g., drought forces agricultural producers to join urban militia), but further climate effects do not necessarily contribute to further violence. Therefore, not only is this bifurcation relationship highly non-linear, there is also often a lack of data to support prior assumptions for high resolution modeling. Here, we aim to overcome the aforementioned causal modeling challenges by proposing a neural forward-intensity Poisson process (NFIPP) model. The NFIPP is designed to capture the potential non-linear causal mechanism in climate induced political violence, whilst being robust to sparse and timing-uncertain data. Our results span 20 recent years and reveal an excitation-based causal link between extreme climate events and political violence across diverse countries. Our climate-induced conflict model results are cross-validated against qualitative climate vulnerability indices. Furthermore, we label historical events that either improve or reduce our predictability gain, demonstrating the importance of domain expertise in informing interpretation.},
	urldate = {2025-12-16},
	publisher = {arXiv},
	author = {Sun, Schyler C. and Jin, Bailu and Wei, Zhuangkun and Guo, Weisi},
	month = mar,
	year = {2022},
	note = {arXiv:2203.04511 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Applications},
	file = {Preprint PDF:/Users/nb/Zotero/storage/8PPCMLJJ/Sun et al. - 2022 - Revealing the Excitation Causality between Climate and Political Violence via a Neural Forward-Inten.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/F3UNJX9H/2203.html:text/html},
}

@article{gavrilova_difference--difference_2025,
	title = {Difference-in-{Difference} {Causal} {Forests} {With} an {Application} to {Payroll} {Tax} {Incidence} in {Norway}},
	volume = {40},
	copyright = {© 2025 The Author(s). Journal of Applied Econometrics published by John Wiley \& Sons Ltd.},
	issn = {1099-1255},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.70001},
	doi = {10.1002/jae.70001},
	abstract = {This paper introduces the difference-in-difference causal forest (DiDCF) method, which extends the causal-forest technique for estimating heterogeneous treatment effects to settings with dynamic treatment effects. Regular causal forests require independence between treatment assignment and the outcome variable (after conditioning out observables). In contrast, DiDCFs provide consistent estimates with a parallel trend assumption. DiDCFs can be used to create event-study plots. The method is applied to estimate payroll tax incidence on wages. We find that heterogeneity in incidence is explained by firm- and workforce-level variables. Firms with a large and heterogeneous workforce are most effective in passing on the incidence of the payroll tax to workers.},
	language = {en},
	number = {7},
	urldate = {2025-12-16},
	journal = {Journal of Applied Econometrics},
	author = {Gavrilova, Evelina and Langørgen, Audun and Zoutman, Floris T.},
	year = {2025},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.70001},
	keywords = {administrative data, causal forest, parallel trends, payroll tax incidence, treatment effect heterogeneity},
	pages = {727--740},
	file = {Full Text PDF:/Users/nb/Zotero/storage/ETNEH4SM/Gavrilova et al. - 2025 - Difference-in-Difference Causal Forests With an Application to Payroll Tax Incidence in Norway.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/PZU4CVBC/jae.html:text/html},
}

@article{nielsen_foreign_2011,
	title = {Foreign {Aid} {Shocks} as a {Cause} of {Violent} {Armed} {Conflict}},
	volume = {55},
	copyright = {©2011, Midwest Political Science Association},
	issn = {1540-5907},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2010.00492.x},
	doi = {10.1111/j.1540-5907.2010.00492.x},
	abstract = {In this study we resolve part of the confusion over how foreign aid affects armed conflict. We argue that aid shocks—severe decreases in aid revenues—inadvertently shift the domestic balance of power and potentially induce violence. During aid shocks, potential rebels gain bargaining strength vis-à-vis the government. To appease the rebels, the government must promise future resource transfers, but the government has no incentive to continue its promised transfers if the aid shock proves to be temporary. With the government unable to credibly commit to future resource transfers, violence breaks out. Using AidData's comprehensive dataset of bilateral and multilateral aid from 1981 to 2005, we evaluate the effects of foreign aid on violent armed conflict. In addition to rare-event logit analysis, we employ matching methods to account for the possibility that aid donors anticipate conflict. The results show that negative aid shocks significantly increase the probability of armed conflict onset.},
	language = {en},
	number = {2},
	urldate = {2025-12-16},
	journal = {American Journal of Political Science},
	author = {Nielsen, Richard A. and Findley, Michael G. and Davis, Zachary S. and Candland, Tara and Nielson, Daniel L.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5907.2010.00492.x},
	pages = {219--232},
	file = {Full Text PDF:/Users/nb/Zotero/storage/DPJDK28P/Nielsen et al. - 2011 - Foreign Aid Shocks as a Cause of Violent Armed Conflict.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/7B6LJSUY/j.1540-5907.2010.00492.html:text/html},
}

@article{athey_recursive_2016,
	title = {Recursive {Partitioning} for {Heterogeneous} {Causal} {Effects}},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1504.01132},
	doi = {10.1073/pnas.1510489113},
	abstract = {In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unit's attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unit's outcome. The challenge is that the "ground truth" for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.},
	number = {27},
	urldate = {2025-12-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Athey, Susan and Imbens, Guido},
	month = jul,
	year = {2016},
	note = {arXiv:1504.01132 [stat]},
	keywords = {Economics - Econometrics, Statistics - Machine Learning},
	pages = {7353--7360},
	file = {Preprint PDF:/Users/nb/Zotero/storage/4Y5CNYEH/Athey and Imbens - 2016 - Recursive Partitioning for Heterogeneous Causal Effects.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/53HXREY9/1504.html:text/html},
}

@article{fink_double_2023,
	title = {A {Double} machine learning trend model for citizen science data},
	volume = {14},
	copyright = {© 2023 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
	issn = {2041-210X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.14186},
	doi = {10.1111/2041-210X.14186},
	abstract = {Citizen and community science datasets are typically collected using flexible protocols. These protocols enable large volumes of data to be collected globally every year; however, the consequence is that these protocols typically lack the structure necessary to maintain consistent sampling across years. This can result in complex and pronounced interannual changes in the observation process, which can complicate the estimation of population trends because population changes over time are confounded with changes in the observation process. Here we describe a novel modelling approach designed to estimate spatially explicit species population trends while controlling for the interannual confounding common in citizen science data. The approach is based on Double machine learning, a statistical framework that uses machine learning (ML) methods to estimate population change and the propensity scores used to adjust for confounding discovered in the data. ML makes it possible to use large sets of features to control for confounding and to model spatial heterogeneity in trends. Additionally, we present a simulation method to identify and adjust for residual confounding missed by the propensity scores. To illustrate the approach, we estimated species trends using data from the citizen science project eBird. We used a simulation study to assess the ability of the method to estimate spatially varying trends when faced with realistic confounding and temporal correlation. Results demonstrated the ability to distinguish between spatially constant and spatially varying trends. There were low error rates on the estimated direction of population change (increasing/decreasing) at each location and high correlations on the estimated magnitude of population change. The ability to estimate spatially explicit trends while accounting for confounding inherent in citizen science data has the potential to fill important information gaps, helping to estimate population trends for species and/or regions lacking rigorous monitoring data.},
	language = {en},
	number = {9},
	urldate = {2025-12-16},
	journal = {Methods in Ecology and Evolution},
	author = {Fink, Daniel and Johnston, Alison and Strimas-Mackey, Matt and Auer, Tom and Hochachka, Wesley M. and Ligocki, Shawn and Oldham Jaromczyk, Lauren and Robinson, Orin and Wood, Chris and Kelling, Steve and Rodewald, Amanda D.},
	year = {2023},
	note = {\_eprint: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.14186},
	keywords = {Causal Forests, causal inference, citizen science, confounding, Double machine learning, machine learning, propensity score, trends, Core, Applied},
	pages = {2435--2448},
	file = {Full Text PDF:/Users/nb/Zotero/storage/6MSUDA8X/Fink et al. - 2023 - A Double machine learning trend model for citizen science data.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/L3ZIA4IS/2041-210X.html:text/html},
}

@article{conley_standard_2025,
	title = {The standard errors of persistence},
	volume = {153},
	issn = {0022-1996},
	url = {https://www.sciencedirect.com/science/article/pii/S0022199624001545},
	doi = {10.1016/j.jinteco.2024.104027},
	abstract = {Many studies of historical persistence find that modern outcomes strongly reflect characteristics of the same places in the distant past. However they rely on data that often exhibit extreme spatial trends and autocorrelation, suggesting that their unusually large t-statistics may be due to inadequately controlling for spurious correlation. To analyze this we introduce a new regression procedure and two diagnostic tests of no treatment effect: (a) a placebo test where the treatment is replaced with spatial noise and (b) a synthetic outcomes test of the hypothesis that the outcome is generated by a trend plus a spatial noise process independent of the treatment. We then show how reliable regression results can be obtained by adding a low dimensional spatial basis to the regression of interest, and applying a large cluster standard error correction. Examining 30 persistence studies in leading journals we find that few approach significance at conventional levels. Our procedure applies to regressions with spatial observations more generally and is implemented in an open source package.},
	urldate = {2025-12-16},
	journal = {Journal of International Economics},
	author = {Conley, Timothy G. and Kelly, Morgan},
	month = jan,
	year = {2025},
	keywords = {Spatial Data, Theory, Historical persistence, Spatial econometrics},
	pages = {104027},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/6MHQQBTR/Conley and Kelly - 2025 - The standard errors of persistence.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/VVS9KVD2/S0022199624001545.html:text/html},
}

@article{conley_gmm_1999,
	title = {{GMM} estimation with cross sectional dependence},
	volume = {92},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407698000840},
	doi = {10.1016/S0304-4076(98)00084-0},
	abstract = {This paper presents a spatial model of dependence among agents using a metric of economic distance. Measurements of this economic distance provide cross-sectional data with a structure similar to that provided by the time index in time-series data. Generalized method of moments estimators using such dependent data are shown to be consistent and asymptotically normal. This paper presents a class of non-parametric, positive semi-definite covariance matrix estimators that allow for general forms of dependence characterized by economic distance. These covariance matrix estimators are shown to remain consistent when economic distances are not precisely observed.},
	number = {1},
	urldate = {2025-12-16},
	journal = {Journal of Econometrics},
	author = {Conley, T. G.},
	month = sep,
	year = {1999},
	keywords = {Spatial Data, Theory, Cross-sectional dependence, Generalized method of moments, Non-parametric covariance matrix estimation, Random fields},
	pages = {1--45},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/K9C3X73J/Conley - 1999 - GMM estimation with cross sectional dependence.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/SSWKC9SK/S0304407698000840.html:text/html},
}

@article{belloni_high-dimensional_2014,
	title = {High-{Dimensional} {Methods} and {Inference} on {Structural} and {Treatment} {Effects}},
	volume = {28},
	issn = {0895-3309},
	url = {https://www.aeaweb.org/articles?id=10.1257/jep.28.2.29},
	doi = {10.1257/jep.28.2.29},
	abstract = {Data with a large number of variables relative to the sample size—"high-dimensional data"—are readily available and increasingly common in empirical economics. High-dimensional data arise through a combination of two phenomena. First, the data may be inherently high dimensional in that many different characteristics per observation are available. For example, the US Census collects information on hundreds of individual characteristics and scanner datasets record transaction-level data for households across a wide range of products. Second, even when the number of available variables is relatively small, researchers rarely know the exact functional form with which the small number of variables enter the model of interest. Researchers are thus faced with a large set of potential variables formed by different ways of interacting and transforming the underlying variables. This paper provides an overview of how innovations in "data mining" can be adapted and modified to provide high-quality inference about model parameters. Note that we use the term "data mining" in a modern sense which denotes a principled search for "true" predictive power that guards against false discovery and overfitting, does not erroneously equate in-sample fit to out-of-sample predictive ability, and accurately accounts for using the same data to examine many different hypotheses or models.},
	language = {en},
	number = {2},
	urldate = {2025-12-16},
	journal = {Journal of Economic Perspectives},
	author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
	month = may,
	year = {2014},
	keywords = {Modeling with Large Data Sets},
	pages = {29--50},
	file = {Full Text PDF:/Users/nb/Zotero/storage/ZNIIGJTR/Belloni et al. - 2014 - High-Dimensional Methods and Inference on Structural and Treatment Effects.pdf:application/pdf},
}

@article{leung_treatment_2020,
	title = {Treatment and {Spillover} {Effects} {Under} {Network} {Interference}},
	volume = {102},
	issn = {0034-6535},
	url = {https://doi.org/10.1162/rest_a_00818},
	doi = {10.1162/rest_a_00818},
	abstract = {We study nonparametric and regression estimators of treatment and spillover effects when interference is mediated by a network. Inference is nonstandard due to dependence induced by treatment spillovers and network-correlated effects. We derive restrictions on the network degree distribution under which the estimators are consistent and asymptotically normal and show they can be verified under a strategic model of network formation. We also construct consistent variance estimators robust to heteroskedasticity and network dependence. Our results allow for the estimation of spillover effects using data from only a single, possibly sampled, network.},
	number = {2},
	urldate = {2025-12-16},
	journal = {The Review of Economics and Statistics},
	author = {Leung, Michael P.},
	month = may,
	year = {2020},
	keywords = {Theory},
	pages = {368--380},
	file = {Full Text PDF:/Users/nb/Zotero/storage/9DXCS3FV/Leung - 2020 - Treatment and Spillover Effects Under Network Interference.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/ED6HYPUH/rest_a_00818.html:text/html},
}

@article{callaway_difference--differences_2021,
	series = {Themed {Issue}: {Treatment} {Effect} 1},
	title = {Difference-in-{Differences} with multiple time periods},
	volume = {225},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407620303948},
	doi = {10.1016/j.jeconom.2020.12.001},
	abstract = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.},
	number = {2},
	urldate = {2025-12-16},
	journal = {Journal of Econometrics},
	author = {Callaway, Brantly and Sant’Anna, Pedro H. C.},
	month = dec,
	year = {2021},
	keywords = {Theory, DiD, Dynamic treatment effects, Difference-in-Differences, Doubly robust, Event study, Semi-parametric, Treatment effect heterogeneity, Variation in treatment timing},
	pages = {200--230},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/XPRUVZPI/Callaway and Sant’Anna - 2021 - Difference-in-Differences with multiple time periods.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/5TK8AYEQ/S0304407620303948.html:text/html},
}

@article{debarsy_identification_2025,
	title = {Identification of {Spatial} {Spillovers}: {Do}'s and {Don}'ts},
	volume = {39},
	copyright = {© 2025 John Wiley \& Sons Ltd.},
	issn = {1467-6419},
	shorttitle = {Identification of {Spatial} {Spillovers}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12692},
	doi = {10.1111/joes.12692},
	abstract = {The notion of spatial spillovers has been widely used in applied spatial econometrics. In this paper, we consider how they can be identified in both structural and causal reduced-form models. First, discussing the various threats to identification in structural models, we point out that the typical estimation framework proposed in the applied spatial econometric literature boils down to considering spatial spillovers as a side-effect of a data-driven chosen specification. We also discuss the limits of blindly relying on interaction matrices purely based on geography to identify the source and content of spillovers. Then, we present reduced forms impact evaluation models for spatial data and show that the current spatial versions of usual impact evaluation models are not fully satisfactory when considering the identification issue. Finally, we propose a set of recommendations for applied articles aimed at identifying spatial spillovers.},
	language = {en},
	number = {5},
	urldate = {2025-12-16},
	journal = {Journal of Economic Surveys},
	author = {Debarsy, Nicolas and Le Gallo, Julie},
	year = {2025},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/joes.12692},
	keywords = {causal inference, Survey, interference, spatial interactions, spatial spillovers, structural and reduced-form identification},
	pages = {2152--2173},
	file = {Snapshot:/Users/nb/Zotero/storage/R4M3WTJA/joes.html:text/html;Submitted Version:/Users/nb/Zotero/storage/YVWQUJFU/Debarsy and Le Gallo - 2025 - Identification of Spatial Spillovers Do's and Don'ts.pdf:application/pdf},
}

@inproceedings{ali_tutorial_2024,
	address = {New York, NY, USA},
	series = {{STCausal} '24},
	title = {Tutorial on {Causal} {Inference} with {Spatiotemporal} {Data}},
	isbn = {979-8-4007-1154-1},
	url = {https://dl.acm.org/doi/10.1145/3681778.3698786},
	doi = {10.1145/3681778.3698786},
	abstract = {Spatiotemporal data, which captures how variables evolve across space and time, is ubiquitous in fields such as environmental science, epidemiology, and urban planning. However, identifying causal relationships in these datasets is challenging due to the presence of spatial dependencies, temporal autocorrelation, and confounding factors. This tutorial provides a comprehensive introduction to spatiotemporal causal inference, offering both theoretical foundations and practical guidance for researchers and practitioners. We explore key concepts such as causal inference frameworks, the impact of confounding in spatiotemporal settings, and the challenges posed by spatial and temporal dependencies. The paper covers synthetic spatiotemporal benchmark data generation, widely used spatiotemporal causal inference techniques, including regression-based, propensity score-based, and deep learning-based methods, and demonstrates their application using synthetic datasets. Through step-by-step examples, readers will gain a clear understanding of how to address common challenges and apply causal inference techniques to spatiotemporal data. This tutorial serves as a valuable resource for those looking to improve the rigor and reliability of their causal analyses in spatiotemporal contexts.},
	urldate = {2025-12-16},
	booktitle = {Proceedings of the 1st {ACM} {SIGSPATIAL} {International} {Workshop} on {Spatiotemporal} {Causal} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Ali, Sahara and Wang, Jianwu},
	month = nov,
	year = {2024},
	keywords = {Spatial Data, Spatio Temporal Causal Inference, Survey, Causal Inference},
	pages = {23--25},
	file = {Full Text PDF:/Users/nb/Zotero/storage/K5A759GG/Ali and Wang - 2024 - Tutorial on Causal Inference with Spatiotemporal Data.pdf:application/pdf},
}

@misc{liang_foundation_2025,
	title = {Foundation {Models} for {Spatio}-{Temporal} {Data} {Science}: {A} {Tutorial} and {Survey}},
	shorttitle = {Foundation {Models} for {Spatio}-{Temporal} {Data} {Science}},
	url = {http://arxiv.org/abs/2503.13502},
	doi = {10.48550/arXiv.2503.13502},
	abstract = {Spatio-Temporal (ST) data science, which includes sensing, managing, and mining large-scale data across space and time, is fundamental to understanding complex systems in domains such as urban computing, climate science, and intelligent transportation. Traditional deep learning approaches have significantly advanced this field, particularly in the stage of ST data mining. However, these models remain task-specific and often require extensive labeled data. Inspired by the success of Foundation Models (FM), especially large language models, researchers have begun exploring the concept of Spatio-Temporal Foundation Models (STFMs) to enhance adaptability and generalization across diverse ST tasks. Unlike prior architectures, STFMs empower the entire workflow of ST data science, ranging from data sensing, management, to mining, thereby offering a more holistic and scalable approach. Despite rapid progress, a systematic study of STFMs for ST data science remains lacking. This survey aims to provide a comprehensive review of STFMs, categorizing existing methodologies and identifying key research directions to advance ST general intelligence.},
	urldate = {2025-12-16},
	publisher = {arXiv},
	author = {Liang, Yuxuan and Wen, Haomin and Xia, Yutong and Jin, Ming and Yang, Bin and Salim, Flora and Wen, Qingsong and Pan, Shirui and Cong, Gao},
	month = mar,
	year = {2025},
	note = {arXiv:2503.13502 [cs]},
	keywords = {Computer Science - Machine Learning, Spatio Temporal Causal Inference, Survey, Computer Science - Databases},
	file = {Preprint PDF:/Users/nb/Zotero/storage/DALXPMVR/Liang et al. - 2025 - Foundation Models for Spatio-Temporal Data Science A Tutorial and Survey.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/KJPSSU7D/2503.html:text/html},
}

@article{roth_whats_2023,
	title = {What’s trending in difference-in-differences? {A} synthesis of the recent econometrics literature},
	volume = {235},
	issn = {0304-4076},
	shorttitle = {What’s trending in difference-in-differences?},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407623001318},
	doi = {10.1016/j.jeconom.2023.03.008},
	abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of “canonical” assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on (i) multiple periods and variation in treatment timing, (ii) potential violations of parallel trends, or (iii) alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.},
	number = {2},
	urldate = {2025-12-16},
	journal = {Journal of Econometrics},
	author = {Roth, Jonathan and Sant’Anna, Pedro H. C. and Bilinski, Alyssa and Poe, John},
	month = aug,
	year = {2023},
	keywords = {Survey, Causal Inference, DiD, Difference-in-differences, Clustering, Parallel trends, Sensitivity Analysis, Staggered Treatment timing, Treatment Effect Heterogeneity},
	pages = {2218--2244},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/IRSYQQJX/Roth et al. - 2023 - What’s trending in difference-in-differences A synthesis of the recent econometrics literature.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/U2L6BBLT/S0304407623001318.html:text/html},
}

@article{dechaisemartin_two-way_2023,
	title = {Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey},
	volume = {26},
	issn = {1368-4221},
	shorttitle = {Two-way fixed effects and differences-in-differences with heterogeneous treatment effects},
	url = {https://doi.org/10.1093/ectj/utac017},
	doi = {10.1093/ectj/utac017},
	abstract = {Linear regressions with period and group fixed effects are widely used to estimate policie’s effects: 26 of the 100 most cited papers published by the American Economic Review from 2015 to 2019 estimate such regressions. It has recently been shown that those regressions may produce misleading estimates if the policy’s effect is heterogeneous between groups or over time, as is often the case. This survey reviews a fast-growing literature that documents this issue and that proposes alternative estimators robust to heterogeneous effects. We use those alternative estimators to revisit Wolfers (2006a).},
	number = {3},
	urldate = {2025-12-16},
	journal = {The Econometrics Journal},
	author = {de Chaisemartin, Clément and D’Haultfœuille, Xavier},
	month = sep,
	year = {2023},
	keywords = {Survey, DiD},
	pages = {C1--C30},
	file = {Full Text PDF:/Users/nb/Zotero/storage/YV48V6UB/de Chaisemartin and D’Haultfœuille - 2023 - Two-way fixed effects and differences-in-differences with heterogeneous treatment effects a survey.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/B5VEN7WZ/utac017.html:text/html},
}

@article{chiang_multiway_2022,
	title = {Multiway {Cluster} {Robust} {Double}/{Debiased} {Machine} {Learning}},
	volume = {40},
	issn = {0735-0015},
	url = {https://doi.org/10.1080/07350015.2021.1895815},
	doi = {10.1080/07350015.2021.1895815},
	abstract = {This article investigates double/debiased machine learning (DML) under multiway clustered sampling environments. We propose a novel multiway cross-fitting algorithm and a multiway DML estimator based on this algorithm. We also develop a multiway cluster robust standard error formula. Simulations indicate that the proposed procedure has favorable finite sample performance. Applying the proposed method to market share data for demand analysis, we obtain larger two-way cluster robust standard errors for the price coefficient than nonrobust ones in the demand model.},
	number = {3},
	urldate = {2025-12-22},
	journal = {Journal of Business \& Economic Statistics},
	publisher = {Taylor \& Francis},
	author = {Chiang, Harold D. and Kato, Kengo and Ma, Yukun and Sasaki, Yuya},
	month = jun,
	year = {2022},
	note = {\_eprint: https://doi.org/10.1080/07350015.2021.1895815},
	keywords = {DML, Double/debiased machine learning, Multiway clustering, Multiway cross-fitting},
	pages = {1046--1056},
	file = {Full Text PDF:/Users/nb/Zotero/storage/EYGAPELW/Chiang et al. - 2022 - Multiway Cluster Robust DoubleDebiased Machine Learning.pdf:application/pdf},
}

@article{von_uexkull_civil_2016,
	title = {Civil conflict sensitivity to growing-season drought},
	volume = {113},
	issn = {1091-6490},
	doi = {10.1073/pnas.1607542113},
	abstract = {To date, the research community has failed to reach a consensus on the nature and significance of the relationship between climate variability and armed conflict. We argue that progress has been hampered by insufficient attention paid to the context in which droughts and other climatic extremes may increase the risk of violent mobilization. Addressing this shortcoming, this study presents an actor-oriented analysis of the drought-conflict relationship, focusing specifically on politically relevant ethnic groups and their sensitivity to growing-season drought under various political and socioeconomic contexts. To this end, we draw on new conflict event data that cover Asia and Africa, 1989-2014, updated spatial ethnic settlement data, and remote sensing data on agricultural land use. Our procedure allows quantifying, for each ethnic group, drought conditions during the growing season of the locally dominant crop. A comprehensive set of multilevel mixed effects models that account for the groups' livelihood, economic, and political vulnerabilities reveals that a drought under most conditions has little effect on the short-term risk that a group challenges the state by military means. However, for agriculturally dependent groups as well as politically excluded groups in very poor countries, a local drought is found to increase the likelihood of sustained violence. We interpret this as evidence of the reciprocal relationship between drought and conflict, whereby each phenomenon makes a group more vulnerable to the other.},
	language = {eng},
	number = {44},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {von Uexkull, Nina and Croicu, Mihai and Fjelde, Hanne and Buhaug, Halvard},
	month = nov,
	year = {2016},
	keywords = {Africa, Agriculture, Applied, armed conflict, Armed Conflicts, Asia, Climate, Climate Change, climate variability, Conflict forecasting, Crops, Agricultural, drought, Droughts, Economic Development, ethnicity, georeferenced event data, Humans, Politics, Risk Factors, Seasons, Time Factors, Violence},
	pages = {12391--12396},
	file = {Full Text:/Users/nb/Zotero/storage/VHQP7C7V/von Uexkull et al. - 2016 - Civil conflict sensitivity to growing-season drought.pdf:application/pdf},
}

@article{shmueli_explain_2010,
	title = {To {Explain} or to {Predict}?},
	volume = {25},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full},
	doi = {10.1214/10-STS330},
	abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
	number = {3},
	urldate = {2025-12-25},
	journal = {Statistical Science},
	publisher = {Institute of Mathematical Statistics},
	author = {Shmueli, Galit},
	month = aug,
	year = {2010},
	keywords = {causality, data mining, Explanatory modeling, predictive modeling, predictive power, scientific research, statistical strategy},
	pages = {289--310},
	file = {Full Text PDF:/Users/nb/Zotero/storage/KSZDLATK/Shmueli - 2010 - To Explain or to Predict.pdf:application/pdf},
}

@article{chadefaux_conflict_2017,
	title = {Conflict forecasting and its limits},
	volume = {1},
	issn = {2451-8484},
	url = {https://doi.org/10.3233/DS-170002},
	doi = {10.3233/DS-170002},
	abstract = {Research on international conflict has mostly focused on explaining events such as the onset or termination of wars, rather than on trying to predict them. Recently, however, forecasts of political phenomena have received growing attention. Predictions of violent events, in particular, have been increasingly accurate using various methods ranging from expert knowledge to quantitative methods and formal modeling. Yet, we know little about the limits of these approaches, even though information about these limits has critical implications for both future research and policy-making. In particular, are our predictive inaccuracies due to limitations of our models, data, or assumptions, in which case improvements should occur incrementally. Or are there aspects of conflicts that will always remain fundamentally unpredictable? After reviewing some of the current approaches to forecasting conflict, I suggest avenues of research that could disentangle the causes of our current predictive failures.},
	language = {EN},
	number = {1-2},
	urldate = {2025-12-25},
	journal = {Data Science},
	publisher = {SAGE Publications},
	author = {Chadefaux, Thomas},
	month = dec,
	year = {2017},
	pages = {7--17},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/VM8T98J2/Chadefaux - 2017 - Conflict forecasting and its limits.pdf:application/pdf},
}

@article{cederman_predicting_2017,
	title = {Predicting armed conflict: {Time} to adjust our expectations?},
	volume = {355},
	shorttitle = {Predicting armed conflict},
	url = {https://www.science.org/doi/10.1126/science.aal4483},
	doi = {10.1126/science.aal4483},
	abstract = {This Essay provides an introduction to the general challenges of predicting political violence, particularly compared with predicting other types of events (such as earthquakes). What is possible? What is less realistic? We aim to debunk myths about predicting violence, as well as to illustrate the substantial progress in this field.},
	number = {6324},
	urldate = {2025-12-25},
	journal = {Science},
	publisher = {American Association for the Advancement of Science},
	author = {Cederman, Lars-Erik and Weidmann, Nils B.},
	month = feb,
	year = {2017},
	pages = {474--476},
	file = {Full Text PDF:/Users/nb/Zotero/storage/92SUCRK2/Cederman and Weidmann - 2017 - Predicting armed conflict Time to adjust our expectations.pdf:application/pdf},
}

@article{athey_machine_2019,
	title = {Machine {Learning} {Methods} {That} {Economists} {Should} {Know} {About}},
	volume = {11},
	issn = {1941-1383, 1941-1391},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-economics-080217-053433},
	doi = {10.1146/annurev-economics-080217-053433},
	abstract = {We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.},
	language = {en},
	number = {Volume 11, 2019},
	urldate = {2025-12-26},
	journal = {Annual Review of Economics},
	publisher = {Annual Reviews},
	author = {Athey, Susan and Imbens, Guido W.},
	month = aug,
	year = {2019},
	pages = {685--725},
	file = {PDF:/Users/nb/Zotero/storage/2WHLYQZP/Athey and Imbens - 2019 - Machine Learning Methods That Economists Should Know About.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/UE7W6PS5/annurev-economics-080217-053433.html:text/html},
}

@article{robinson_root-n-consistent_1988,
	title = {Root-{N}-{Consistent} {Semiparametric} {Regression}},
	volume = {56},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/1912705},
	doi = {10.2307/1912705},
	abstract = {One type of semiparametric regression on an {\textless}tex-math{\textgreater}\${\textbackslash}scr\{R\}{\textasciicircum}\{p\}{\textbackslash}times {\textbackslash}scr\{R\}{\textasciicircum}\{q\}{\textbackslash}text\{-valued\}\${\textless}/tex-math{\textgreater} random variable (X, Z) is β′X + θ(Z), where β and θ(Z) are an unknown slope coefficient vector and function, and X is neither wholly dependent on Z nor necessarily independent of it. Estimators of β based on incorrect parameterization of θ are generally inconsistent, whereas consistent nonparametric estimators deviate from β by a larger probability order than N$^{\textrm{-1/2}}$, where N is sample size. An estimator generalizing the ordinary least squares estimator of β is constructed by inserting nonparametric regression estimators in the nonlinear orthogonal projection on Z. Under regularity conditions β̂ is shown to be {\textless}tex-math{\textgreater}\$N{\textasciicircum}\{1/2\}{\textbackslash}text\{-consistent\}\${\textless}/tex-math{\textgreater} for β and asymptotically normal, and a consistent estimator of its limiting covariance matrix is given, affording statistical inference that is not only asymptotically valid but has nonzero asymptotic first-order efficiency relative to estimators based on a correctly parameterized θ. We discuss the identification problem and β̂'s efficiency, and report results of a Monte Carlo study of finite-sample performance. While the paper focuses on the simplest interesting setting of multiple regression with independent observations, extensions to other econometric models are described, in particular seemingly unrelated and nonlinear regressions, simultaneous equations, distributed lags, and sample selectivity models.},
	number = {4},
	urldate = {2025-12-26},
	journal = {Econometrica},
	publisher = {[Wiley, Econometric Society]},
	author = {Robinson, P. M.},
	year = {1988},
	pages = {931--954},
	file = {JSTOR Full Text PDF:/Users/nb/Zotero/storage/XWZ836RQ/Robinson - 1988 - Root-N-Consistent Semiparametric Regression.pdf:application/pdf},
}

@article{hegre_views_2019,
	title = {{ViEWS}: {A} political violence early-warning system},
	volume = {56},
	issn = {0022-3433},
	shorttitle = {{ViEWS}},
	url = {https://doi.org/10.1177/0022343319823860},
	doi = {10.1177/0022343319823860},
	abstract = {This article presents ViEWS – a political violence early-warning system that seeks to be maximally transparent, publicly available, and have uniform coverage, and sketches the methodological innovations required to achieve these objectives. ViEWS produces monthly forecasts at the country and subnational level for 36 months into the future and all three UCDP types of organized violence: state-based conflict, non-state conflict, and one-sided violence in Africa. The article presents the methodology and data behind these forecasts, evaluates their predictive performance, provides selected forecasts for October 2018 through October 2021, and indicates future extensions. ViEWS is built as an ensemble of constituent models designed to optimize its predictions. Each of these represents a theme that the conflict research literature suggests is relevant, or implements a specific statistical/machine-learning approach. Current forecasts indicate a persistence of conflict in regions in Africa with a recent history of political violence but also alert to new conflicts such as in Southern Cameroon and Northern Mozambique. The subsequent evaluation additionally shows that ViEWS is able to accurately capture the long-term behavior of established political violence, as well as diffusion processes such as the spread of violence in Cameroon. The performance demonstrated here indicates that ViEWS can be a useful complement to non-public conflict-warning systems, and also serves as a reference against which future improvements can be evaluated.},
	language = {EN},
	number = {2},
	urldate = {2025-12-27},
	journal = {Journal of Peace Research},
	publisher = {SAGE Publications Ltd},
	author = {Hegre, Håvard and Allansson, Marie and Basedau, Matthias and Colaresi, Michael and Croicu, Mihai and Fjelde, Hanne and Hoyles, Frederick and Hultman, Lisa and Högbladh, Stina and Jansen, Remco and Mouhleb, Naima and Muhammad, Sayyed Auwn and Nilsson, Desirée and Nygård, Håvard Mokleiv and Olafsdottir, Gudlaug and Petrova, Kristina and Randahl, David and Rød, Espen Geelmuyden and Schneider, Gerald and von Uexkull, Nina and Vestby, Jonas},
	month = mar,
	year = {2019},
	pages = {155--174},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/9M4DDNPE/Hegre et al. - 2019 - ViEWS A political violence early-warning system.pdf:application/pdf},
}

@article{rod_review_2024,
	title = {A review and comparison of conflict early warning systems},
	volume = {40},
	issn = {0169-2070},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207023000018},
	doi = {10.1016/j.ijforecast.2023.01.001},
	abstract = {We review and compare conflict early warning systems on three dimensions: transparency and accessibility, key parameters, and forecasts. The review reveals a need for improved transparency and accessibility of data and code, considerable variation in key parameters across systems, and significant overlaps in countries with the highest risk. We propose that developing standards and platforms that promote transparency, accessibility, and inter-system cooperation can improve knowledge proliferation and system development to mitigate and prevent political violence.},
	number = {1},
	urldate = {2025-12-27},
	journal = {International Journal of Forecasting},
	author = {Rød, Espen Geelmuyden and Gåsste, Tim and Hegre, Håvard},
	month = jan,
	year = {2024},
	keywords = {Conflict forecasting, Armed conflict, Conflict early warning systems, Research transparency and data accessibility, Review article},
	pages = {96--112},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/U8TXAHEP/Rød et al. - 2024 - A review and comparison of conflict early warning systems.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/XNDZ2XSA/S0169207023000018.html:text/html},
}

@article{raleigh_introducing_2010,
	title = {Introducing {ACLED}: {An} {Armed} {Conflict} {Location} and {Event} {Dataset}},
	volume = {47},
	issn = {0022-3433},
	shorttitle = {Introducing {ACLED}},
	url = {https://www.jstor.org/stable/20798933},
	abstract = {This article presents ACLED, an Armed Conflict Location and Event Dataset. ACLED codes the actions of rebels, governments, and militias within unstable states, specifying the exact location and date of battle events, transfers of military control, headquarter establishment, civilian violence, and rioting. In the current version, the dataset covers 50 unstable countries from 1997 through 2010. ACLED's disaggregation of civil war and transnational violent events allow for research on local level factors and the dynamics of civil and communal conflict. Findings from subnational conflict research challenges conclusions from larger national-level studies. In a brief descriptive analysis, the authors find that, on average, conflict covers 15\% of a state's territory, but almost half of a state can be directly affected by internal wars.},
	number = {5},
	urldate = {2025-12-27},
	journal = {Journal of Peace Research},
	publisher = {Sage Publications, Ltd.},
	author = {Raleigh, Clionadh and Linke, Andrew and Hegre, Håvard and Karlsen, Joakim},
	year = {2010},
	pages = {651--660},
	file = {JSTOR Full Text PDF:/Users/nb/Zotero/storage/CBZ3EXJ6/Raleigh et al. - 2010 - Introducing ACLED An Armed Conflict Location and Event Dataset.pdf:application/pdf},
}

@article{sundberg_introducing_2013,
	title = {Introducing the {UCDP} {Georeferenced} {Event} {Dataset}},
	volume = {50},
	issn = {0022-3433},
	url = {https://doi.org/10.1177/0022343313484347},
	doi = {10.1177/0022343313484347},
	abstract = {This article presents the UCDP Georeferenced Event Dataset (UCDP GED). The UCDP GED is an event dataset that disaggregates three types of organized violence (state-based conflict, non-state conflict, and one-sided violence) both spatially and temporally. Each event – defined as an instance of organized violence with at least one fatality – comes with date, geographical location, and identifiers that allow the dataset to be linked to and merged with other UCDP datasets. The first version of the dataset covers events of fatal violence on the African continent between 1989 and 2010. This article, firstly, introduces the rationale for the new dataset, and explains the basic coding procedures as well as the quality controls. Secondly, we discuss some of the data’s potential weaknesses in representing the universe of organized violence, as well as some potential biases induced by the operationalizations. Thirdly, we provide an example of how the data can be used, by illustrating the association between cities and organized violence, taking population density into account. The UCDP GED is a useful resource for conflict analyses below the state and country-year levels, and can provide us with new insights into the geographical determinants and temporal sequencing of warfare and violence.},
	language = {EN},
	number = {4},
	urldate = {2025-12-27},
	journal = {Journal of Peace Research},
	publisher = {SAGE Publications Ltd},
	author = {Sundberg, Ralph and Melander, Erik},
	month = jul,
	year = {2013},
	pages = {523--532},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/Z35M5XTZ/Sundberg and Melander - 2013 - Introducing the UCDP Georeferenced Event Dataset.pdf:application/pdf},
}

@article{mueller_hard_2022,
	title = {The {Hard} {Problem} of {Prediction} for {Conflict} {Prevention}},
	volume = {20},
	issn = {1542-4766},
	url = {https://doi.org/10.1093/jeea/jvac025},
	doi = {10.1093/jeea/jvac025},
	abstract = {In this article, we propose a framework to tackle conflict prevention, an issue which has received interest in several policy areas. A key challenge of conflict forecasting for prevention is that outbreaks of conflict in previously peaceful countries are rare events and therefore hard to predict. To make progress in this hard problem, this project summarizes more than four million newspaper articles using a topic model. The topics are then fed into a random forest to predict conflict risk, which is then integrated into a simple static framework in which a decision maker decides on the optimal number of interventions to minimize the total cost of conflict and intervention. According to the stylized model, cost savings compared to not intervening pre-conflict are over US\$1 trillion even with relatively ineffective interventions and US\$13 trillion with effective interventions.},
	number = {6},
	urldate = {2025-12-27},
	journal = {Journal of the European Economic Association},
	author = {Mueller, Hannes and Rauh, Christopher},
	month = dec,
	year = {2022},
	pages = {2440--2467},
	file = {Full Text PDF:/Users/nb/Zotero/storage/RXBFCAJ4/Mueller and Rauh - 2022 - The Hard Problem of Prediction for Conflict Prevention.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/TRC3YB7L/jvac025.html:text/html},
}

@article{mueller_using_2022,
	title = {Using past violence and current news to predict changes in violence},
	volume = {48},
	issn = {0305-0629},
	url = {https://doi.org/10.1080/03050629.2022.2063853},
	doi = {10.1080/03050629.2022.2063853},
	abstract = {This article proposes a new method for predicting escalations and de-escalations of violence using a model which relies on conflict history and text features. The text features are generated from over 3.5 million newspaper articles using a so-called topic-model. We show that the combined model relies to a large extent on conflict dynamics, but that text is able to contribute meaningfully to the prediction of rare outbreaks of violence in previously peaceful countries. Given the very powerful dynamics of the conflict trap these cases are particularly important for prevention efforts. Este artículo propone un nuevo método para la predicción de escaladas y desescaladas de violencia a través de la aplicación de un modelo basado en los antecedentes del conflicto y las características propias del texto. Las características del texto se generan a partir de más de 3,5 millones de artículos de periódicos mediante el uso de lo que se denomina “modelo de tópicos”. Demostramos que, si bien este modelo combinado hace referencia a una extensa dinámica del conflicto, el texto es una contribución relevante que permite predecir los estallidos de violencia inesperados en países que antes eran pacíficos. Dada la dinámica de gran intensidad característica de la trampa del conflicto, estos casos son de especial importancia en lo que se refiere a las iniciativas de prevención. Dans cet article, nous proposons une nouvelle méthode destinée à anticiper les escalades et désescalades de violence grâce à un modèle reposant sur les antécédents conflictuels et sur des caractéristiques textuelles. Ces caractéristiques sont extraites à partir de plus de 3,5 millions d’articles de presse à l’aide d’un modèle thématique (topic model). Nous montrons que si ce modèle mixte s’appuie largement sur les dynamiques conflictuelles, les données textuelles peuvent être très utiles en vue d’anticiper les rares explosions de violence dans les pays habituellement pacifiques. Étant donné la puissante dynamique qui sous-tend les conflits récurrents, les exemples exposés revêtent une importance particulière dans une optique de prévention.},
	number = {4},
	urldate = {2025-12-27},
	journal = {International Interactions},
	publisher = {Routledge},
	author = {Mueller, Hannes and Rauh, Christopher},
	month = jul,
	year = {2022},
	note = {\_eprint: https://doi.org/10.1080/03050629.2022.2063853},
	keywords = {Armed conflict, civil war, forecasting, machine learning, text analysis},
	pages = {579--596},
	file = {Submitted Version:/Users/nb/Zotero/storage/JWLT53A2/Mueller and Rauh - 2022 - Using past violence and current news to predict changes in violence.pdf:application/pdf},
}

@article{vesco_united_2022,
	title = {United they stand: {Findings} from an escalation prediction competition},
	volume = {48},
	issn = {0305-0629},
	shorttitle = {United they stand},
	url = {https://doi.org/10.1080/03050629.2022.2029856},
	doi = {10.1080/03050629.2022.2029856},
	abstract = {This article presents results and lessons learned from a prediction competition organized by ViEWS to improve collective scientific knowledge on forecasting (de-)escalation in Africa. The competition call asked participants to forecast changes in state-based violence for the true future (October 2020–March 2021) as well as for a held-out test partition. An external scoring committee, independent from both the organizers and participants, was formed to evaluate the models based on both qualitative and quantitative criteria, including performance, novelty, uniqueness, and replicability. All models contributed to advance the research frontier by providing novel methodological or theoretical insight, including new data, or adopting innovative model specifications. While we discuss several facets of the competition that could be improved moving forward, the collection passes an important test. When we build a simple ensemble prediction model—which draws on the unique insights of each contribution to differing degrees—we can measure an improvement in the prediction from the group, over and above what the average individual model can achieve. This wisdom of the crowd effect suggests that future competitions that build on both the successes and failures of ours, can contribute to scientific knowledge by incentivizing diverse contributions as well as focusing a group’s attention on a common problem. Este artículo presenta los resultados y las enseñanzas extraídas en el marco de un certamen de predicción organizado por los responsables del proyecto Sistema de Alerta Temprana de Violencia (Violence Early-Warning System, ViEWS) con el propósito de mejorar los conocimientos científicos colectivos sobre la previsión de la (des)escalada en el continente africano. En el certamen se pidió a los participantes que desarrollaran una previsión con respecto a los cambios en la violencia estatal para el futuro real (de octubre de 2020 a marzo de 2021), así como para una muestra de prueba que se mantendría. Se formó un comité de calificación externo, independiente tanto de los organizadores como de los participantes, para evaluar los modelos en función de criterios cualitativos y cuantitativos, como el rendimiento, la novedad, la singularidad y la replicabilidad. Todos los modelos contribuyeron a avanzar en la frontera de la investigación mediante el aporte de nuevos conocimientos metodológicos o teóricos, la inclusión de nuevos datos o la adopción de especificaciones innovadoras del modelo. Aunque se debarió sobre varios aspectos del certamen que podrían mejorarse de cara al futuro, lo que se recopiló pasó una prueba importante. Cuando se construye un simple modelo de predicción de conjunto, que se basa en los conocimientos únicos de cada contribución en diferentes grados, se puede medir una mejora en la predicción del grupo, por encima de lo que el modelo individual promedio puede lograr. Este efecto de la sabiduría de la multitud sugiere que los futuros certámenes que se basen tanto en los éxitos como en los fracasos propios, pueden contribuir al conocimiento científico incentivando diversas contribuciones, así como centrando la atención de un grupo en un problema común. Cet article présente les résultats et les enseignements tirés d’un concours de prédiction organisé par ViEWS (Violence early-warning system, système d’alerte précoce sur la violence) pour améliorer nos connaissances scientifiques collectives en prévision de la (dés)escalade de la violence sur le continent africain. L’appel à concours demandait aux participants de prévoir les évolutions de la violence étatique pour le futur réel (octobre 2020-mars 2021) ainsi que pour une partition test retenue. Un comité de notation externe, indépendant à la fois des organisateurs et des participants, a été constitué pour évaluer les modèles à la fois sur des critères qualitatifs et quantitatifs, notamment sur leurs performances, leur nouveauté, leur unicité et leur reproductibilité. Tous les modèles ont contribué à faire avancer la frontière des recherches en apportant un éclairage méthodologique ou théorique inédit, en incluant de nouvelles données ou en adoptant des caractéristiques de modèle innovantes. Bien que nous abordions plusieurs facettes du concours qui pourraient être améliorées en allant de l’avant, l’ensemble de modèles a réussi un test important. Lorsque nous concevons un modèle de prédiction par ensemble simple - qui s’appuie sur les renseignements uniques de chaque contribution aux différents degrés -, nous pouvons mesurer une amélioration de la prédiction du groupe par rapport à ce que le modèle individuel moyen permet d’obtenir. Cet effet de sagesse de la foule suggère que les futurs concours, qui s’appuieront à la fois sur les réussites et les échecs du nôtre, pourront contribuer aux connaissances scientifiques en encourageant des contributions diverses et en concentrant l’attention d’un groupe sur un problème commun.},
	number = {4},
	urldate = {2025-12-27},
	journal = {International Interactions},
	publisher = {Routledge},
	author = {Vesco, Paola and Hegre, Håvard and Colaresi, Michael and Jansen, Remco Bastiaan and Lo, Adeline and Reisch, Gregor and Weidmann, Nils B.},
	month = jul,
	year = {2022},
	note = {\_eprint: https://doi.org/10.1080/03050629.2022.2029856},
	keywords = {Conflict, escalation, forecasting, political violence, prediction},
	pages = {860--896},
	file = {Full Text PDF:/Users/nb/Zotero/storage/TNXUBMSB/Vesco et al. - 2022 - United they stand Findings from an escalation prediction competition.pdf:application/pdf},
}

@article{tollefsen_prio-grid_2012,
	title = {{PRIO}-{GRID}: {A} unified spatial data structure},
	volume = {49},
	issn = {0022-3433},
	shorttitle = {{PRIO}-{GRID}},
	url = {https://doi.org/10.1177/0022343311431287},
	doi = {10.1177/0022343311431287},
	abstract = {Contributions to the quantitative civil war literature increasingly rely on geo-referenced data and disaggregated research designs. While this is a welcome trend, it necessitates geographic information systems (GIS) skills and imposes new challenges for data collection and analysis. So far, solutions to these challenges differ between studies, obstructing direct comparison of findings and hampering replication and extension of earlier work. This article presents a standardized structure for storing, manipulating, and analyzing high-resolution spatial data. PRIO-GRID is a vector grid network with a resolution of 0.5 x 0.5 decimal degrees, covering all terrestrial areas of the world. Gridded data comprise inherently apolitical entities; the grid cells are fixed in time and space, they are insensitive to political boundaries and developments, and they are completely exogenous to likely features of interest, such as civil war outbreak, ethnic settlement patterns, extreme weather events, or the spatial distribution of wealth. Moreover, unlike other disaggregated approaches, gridded data may be scaled up or down in a consistent manner by varying the resolution of the grid. The released dataset comes with cell-specific information on a large selection of political, economic, demographic, environmental, and conflict variables for all years, 1946–2008. A simple descriptive data assessment of population density and economic activity is offered to demonstrate how PRIO-GRID may be applied in quantitative social science research.},
	language = {EN},
	number = {2},
	urldate = {2025-12-27},
	journal = {Journal of Peace Research},
	publisher = {SAGE Publications Ltd},
	author = {Tollefsen, Andreas Forø and Strand, Håvard and Buhaug, Halvard},
	month = mar,
	year = {2012},
	pages = {363--374},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/U8SEJPQB/Tollefsen et al. - 2012 - PRIO-GRID A unified spatial data structure.pdf:application/pdf},
}

@article{hegre_introduction_2017,
	title = {Introduction: {Forecasting} in peace research},
	volume = {54},
	issn = {0022-3433},
	shorttitle = {Introduction},
	url = {https://doi.org/10.1177/0022343317691330},
	doi = {10.1177/0022343317691330},
	abstract = {Prediction and forecasting have now fully reached peace and conflict research. We define forecasting as predictions about unrealized outcomes given model estimates from realized data, and predictions more generally as the assignment of probability distributions to realized or unrealized outcomes. Increasingly, scholars present within- and out-of-sample prediction results in their publications and sometimes even forecasts for unrealized, future outcomes. The articles in this special issue demonstrate the ability of current approaches to forecast events of interest and contributes to the formulation of best practices for forecasting within peace research. We highlight the role of forecasting for theory evaluation and as a bridge between academics and policymakers, summarize the contributions in the special issue, and provide some thoughts on how research on forecasting in peace research should proceed. We suggest some best practices, noting the importance of theory development, interpretability of models, replicability of results, and data collection.},
	language = {EN},
	number = {2},
	urldate = {2025-12-27},
	journal = {Journal of Peace Research},
	publisher = {SAGE Publications Ltd},
	author = {Hegre, Håvard and Metternich, Nils W and Nygård, Håvard Mokleiv and Wucherpfennig, Julian},
	month = mar,
	year = {2017},
	pages = {113--124},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/P8NFSFTF/Hegre et al. - 2017 - Introduction Forecasting in peace research.pdf:application/pdf},
}

@article{hegre_views2020_2021,
	title = {{ViEWS2020}: {Revising} and evaluating the {ViEWS} political {Violence} {Early}-{Warning} {System}},
	volume = {58},
	issn = {0022-3433},
	shorttitle = {{ViEWS2020}},
	url = {https://doi.org/10.1177/0022343320962157},
	doi = {10.1177/0022343320962157},
	abstract = {This article presents an update to the ViEWS political Violence Early-Warning System. This update introduces (1) a new infrastructure for training, evaluating, and weighting models that allows us to more optimally combine constituent models into ensembles, and (2) a number of new forecasting models that contribute to improve overall performance, in particular with respect to effectively classifying high- and low-risk cases. Our improved evaluation procedures allow us to develop models that specialize in either the immediate or the more distant future. We also present a formal, ‘retrospective’ evaluation of how well ViEWS has done since we started publishing our forecasts from July 2018 up to December 2019. Our metrics show that ViEWS is performing well when compared to previous out-of-sample forecasts for the 2015–17 period. Finally, we present our new forecasts for the January 2020–December 2022 period. We continue to predict a near-constant situation of conflict in Nigeria, Somalia, and DRC, but see some signs of decreased risk in Cameroon and Mozambique.},
	language = {EN},
	number = {3},
	urldate = {2025-12-27},
	journal = {Journal of Peace Research},
	publisher = {SAGE Publications Ltd},
	author = {Hegre, Håvard and Bell, Curtis and Colaresi, Michael and Croicu, Mihai and Hoyles, Frederick and Jansen, Remco and Leis, Maxine Ria and Lindqvist-McGowan, Angelica and Randahl, David and Rød, Espen Geelmuyden and Vesco, Paola},
	month = may,
	year = {2021},
	pages = {599--611},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/GFWYW3AY/Hegre et al. - 2021 - ViEWS2020 Revising and evaluating the ViEWS political Violence Early-Warning System.pdf:application/pdf},
}

@misc{nemkova_large_2025,
	title = {Do {Large} {Language} {Models} {Know} {Conflict}? {Investigating} {Parametric} vs. {Non}-{Parametric} {Knowledge} of {LLMs} for {Conflict} {Forecasting}},
	shorttitle = {Do {Large} {Language} {Models} {Know} {Conflict}?},
	url = {http://arxiv.org/abs/2505.09852},
	doi = {10.48550/arXiv.2505.09852},
	abstract = {Large Language Models (LLMs) have shown impressive performance across natural language tasks, but their ability to forecast violent conflict remains underexplored. We investigate whether LLMs possess meaningful parametric knowledge-encoded in their pretrained weights-to predict conflict escalation and fatalities without external data. This is critical for early warning systems, humanitarian planning, and policy-making. We compare this parametric knowledge with non-parametric capabilities, where LLMs access structured and unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent news reports via Retrieval-Augmented Generation (RAG). Incorporating external information could enhance model performance by providing up-to-date context otherwise missing from pretrained weights. Our two-part evaluation framework spans 2020-2024 across conflict-prone regions in the Horn of Africa and the Middle East. In the parametric setting, LLMs predict conflict trends and fatalities relying only on pretrained knowledge. In the non-parametric setting, models receive summaries of recent conflict events, indicators, and geopolitical developments. We compare predicted conflict trend labels (e.g., Escalate, Stable Conflict, De-escalate, Peace) and fatalities against historical data. Our findings highlight the strengths and limitations of LLMs for conflict forecasting and the benefits of augmenting them with structured external knowledge.},
	urldate = {2025-12-28},
	publisher = {arXiv},
	author = {Nemkova, Apollinaire Poli and Lingareddy, Sarath Chandra and Choudhury, Sagnik Ray and Albert, Mark V.},
	month = may,
	year = {2025},
	note = {arXiv:2505.09852 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/nb/Zotero/storage/KN6TRTAH/Nemkova et al. - 2025 - Do Large Language Models Know Conflict Investigating Parametric vs. Non-Parametric Knowledge of LLM.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/KNMJN2RE/2505.html:text/html},
}

@misc{croicu_newswire_2025,
	title = {From {Newswire} to {Nexus}: {Using} text-based actor embeddings and transformer networks to forecast conflict dynamics},
	shorttitle = {From {Newswire} to {Nexus}},
	url = {http://arxiv.org/abs/2501.03928},
	doi = {10.48550/arXiv.2501.03928},
	abstract = {This study advances the field of conflict forecasting by using text-based actor embeddings with transformer models to predict dynamic changes in violent conflict patterns at the actor level. More specifically, we combine newswire texts with structured conflict event data and leverage recent advances in Natural Language Processing (NLP) techniques to forecast escalations and de-escalations among conflicting actors, such as governments, militias, separatist movements, and terrorists. This new approach accurately and promptly captures the inherently volatile patterns of violent conflicts, which existing methods have not been able to achieve. To create this framework, we began by curating and annotating a vast international newswire corpus, leveraging hand-labeled event data from the Uppsala Conflict Data Program. By using this hybrid dataset, our models can incorporate the textual context of news sources along with the precision and detail of structured event data. This combination enables us to make both dynamic and granular predictions about conflict developments. We validate our approach through rigorous back-testing against historical events, demonstrating superior out-of-sample predictive power. We find that our approach is quite effective in identifying and predicting phases of conflict escalation and de-escalation, surpassing the capabilities of traditional models. By focusing on actor interactions, our explicit goal is to provide actionable insights to policymakers, humanitarian organizations, and peacekeeping operations in order to enable targeted and effective intervention strategies.},
	urldate = {2025-12-28},
	publisher = {arXiv},
	author = {Croicu, Mihai and Maase, Simon Polichinel von der},
	month = jan,
	year = {2025},
	note = {arXiv:2501.03928 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/nb/Zotero/storage/BY76F8T4/Croicu and Maase - 2025 - From Newswire to Nexus Using text-based actor embeddings and transformer networks to forecast confl.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/262AGHVX/2501.html:text/html},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Statistical {Modeling}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full},
	doi = {10.1214/ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2026-01-09},
	journal = {Statistical Science},
	publisher = {Institute of Mathematical Statistics},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	pages = {199--231},
	file = {Full Text PDF:/Users/nb/Zotero/storage/P7CLJMUU/Breiman - 2001 - Statistical Modeling The Two Cultures (with comments and a rejoinder by the author).pdf:application/pdf},
}

@article{rudin_stop_2019,
	title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	volume = {1},
	copyright = {2019 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-019-0048-x},
	doi = {10.1038/s42256-019-0048-x},
	abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
	language = {en},
	number = {5},
	urldate = {2026-01-09},
	journal = {Nature Machine Intelligence},
	publisher = {Nature Publishing Group},
	author = {Rudin, Cynthia},
	month = may,
	year = {2019},
	keywords = {Computer science, Criminology, Science, Statistics, technology and society},
	pages = {206--215},
	file = {Full Text PDF:/Users/nb/Zotero/storage/AWBYU92Q/Rudin - 2019 - Stop explaining black box machine learning models for high stakes decisions and use interpretable mo.pdf:application/pdf},
}

@article{sunstein_use_2024,
	title = {The use of algorithms in society},
	volume = {37},
	issn = {1573-7128},
	url = {https://doi.org/10.1007/s11138-023-00625-z},
	doi = {10.1007/s11138-023-00625-z},
	abstract = {The judgments of human beings can be biased; they can also be noisy. Across a wide range of settings, use of algorithms is likely to improve accuracy, because algorithms will reduce both bias and noise. Indeed, algorithms can help identify the role of human biases; they might even identify biases that have not been named before. As compared to algorithms, for example, human judges, deciding whether to give bail to criminal defendants, show Current Offense Bias and Mugshot Bias; as compared to algorithms, human doctors, deciding whether to test people for heart attacks, show Current Symptom Bias and Demographic Bias. These are cases in which large data sets are able to associate certain inputs with specific outcomes. But in important cases, algorithms struggle to make accurate predictions, not because they are algorithms but because they do not have enough data to answer the question at hand. Those cases often, though not always, involve complex systems. (1) Algorithms might not be able to foresee the effects of social interactions, which can depend on a large number of random or serendipitous factors, and which can lead in unanticipated and unpredictable directions. (2) Algorithms might not be able to foresee the effects of context, timing, or mood. (3) Algorithms might not be able to identify people’s preferences, which might be concealed or falsified, and which might be revealed at an unexpected time. (4) Algorithms might not be able to anticipate sudden or unprecedented leaps or shocks (a technological breakthrough, a successful terrorist attack, a pandemic, a black swan). (5) Algorithms might not have “local knowledge,” or private information, which human beings might have. Predictions about romantic attraction, about the success of cultural products, and about coming revolutions are cases in point. The limitations of algorithms are analogous to the limitations of planners, emphasized by Hayek in his famous critique of central planning. It is an unresolved question whether and to what extent some of the limitations of algorithms might be reduced or overcome over time, with more data or various improvements; calculations are improving in extraordinary ways, but some of the relevant challenges cannot be solved with ex ante calculations.},
	language = {en},
	number = {4},
	urldate = {2026-01-09},
	journal = {The Review of Austrian Economics},
	author = {Sunstein, Cass R.},
	month = dec,
	year = {2024},
	keywords = {Algorithms, B31, Cognitive bias, Complexity, D80, D81, D83, D90, D91, Hayek, Local knowledge},
	pages = {399--420},
	file = {Full Text PDF:/Users/nb/Zotero/storage/K886PFVE/Sunstein - 2024 - The use of algorithms in society.pdf:application/pdf},
}

@article{rubin_estimating_1974,
	title = {Estimating causal effects of treatments in randomized and nonrandomized studies.},
	volume = {66},
	issn = {1939-2176, 0022-0663},
	url = {https://doi.apa.org/doi/10.1037/h0037350},
	doi = {10.1037/h0037350},
	language = {en},
	number = {5},
	urldate = {2026-01-10},
	journal = {Journal of Educational Psychology},
	author = {Rubin, Donald B.},
	month = oct,
	year = {1974},
	pages = {688--701},
	file = {PDF:/Users/nb/Zotero/storage/2PV9MI8G/Rubin - 1974 - Estimating causal effects of treatments in randomized and nonrandomized studies..pdf:application/pdf},
}

@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	language = {en},
	author = {Holland, Paul W},
	year = {1986},
	file = {PDF:/Users/nb/Zotero/storage/CPES64SS/Holland - 1986 - Statistics and Causal Inference.pdf:application/pdf},
}

@misc{cloudwards_i_2026,
	title = {I {Tested} {The} 4 {Best} {Google} {Workspace} {Alternatives} in 2025},
	url = {https://www.youtube.com/watch?v=yFzwP2jRr4w},
	abstract = {☁️ Try any Zoho Workplace plan FREE for 14 days 🔗➡️ https://www.cloudwards.net/go/zoho-wo...

📰 Want to stay up to date on all the latest news about the cloud, online security and digital privacy? Sign up for our newsletter and stay ahead of the curve! 🔗➡️ https://www.cloudwards.net/join-thous...},
	urldate = {2026-01-10},
	collaborator = {{Cloudwards}},
	month = jan,
	year = {2026},
}

@article{hill_bayesian_2011,
	title = {Bayesian {Nonparametric} {Modeling} for {Causal} {Inference}},
	volume = {20},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/jcgs.2010.08162},
	doi = {10.1198/jcgs.2010.08162},
	abstract = {Researchers have long struggled to identify causal effects in nonexperimental settings. Many recently proposed strategies assume ignorability of the treatment assignment mechanism and require fitting two models—one for the assignment mechanism and one for the response surface. This article proposes a strategy that instead focuses on very flexibly modeling just the response surface using a Bayesian nonparametric modeling procedure, Bayesian Additive Regression Trees (BART). BART has several advantages: it is far simpler to use than many recent competitors, requires less guesswork in model fitting, handles a large number of predictors, yields coherent uncertainty intervals, and fluidly handles continuous treatment variables and missing data for the outcome variable. BART also naturally identifies heterogeneous treatment effects. BART produces more accurate estimates of average treatment effects compared to propensity score matching, propensity-weighted estimators, and regression adjustment in the nonlinear simulation situations examined. Further, it is highly competitive in linear settings with the “correct” model, linear regression. Supplemental materials including code and data to replicate simulations and examples from the article as well as methods for population inference are available online.},
	number = {1},
	urldate = {2026-01-10},
	journal = {Journal of Computational and Graphical Statistics},
	publisher = {Taylor \& Francis},
	author = {Hill, Jennifer L.},
	month = jan,
	year = {2011},
	note = {\_eprint: https://doi.org/10.1198/jcgs.2010.08162},
	keywords = {Bayesian, Causal inference, Nonparametrics},
	pages = {217--240},
	file = {Full Text PDF:/Users/nb/Zotero/storage/6874EFG9/Hill - 2011 - Bayesian Nonparametric Modeling for Causal Inference.pdf:application/pdf},
}

@misc{lan_meta-learner_2025,
	title = {A {Meta}-learner for {Heterogeneous} {Effects} in {Difference}-in-{Differences}},
	url = {http://arxiv.org/abs/2502.04699},
	doi = {10.48550/arXiv.2502.04699},
	abstract = {We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines.},
	urldate = {2026-01-10},
	publisher = {arXiv},
	author = {Lan, Hui and Chang, Haoge and Dillon, Eleanor and Syrgkanis, Vasilis},
	month = apr,
	year = {2025},
	note = {arXiv:2502.04699 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/nb/Zotero/storage/2KBGPWQN/Lan et al. - 2025 - A Meta-learner for Heterogeneous Effects in Difference-in-Differences.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/BK282PH3/2502.html:text/html},
}

@article{colangelo_double_2025,
	title = {Double {Debiased} {Machine} {Learning} {Nonparametric} {Inference} with {Continuous} {Treatments}},
	volume = {0},
	issn = {0735-0015},
	url = {https://doi.org/10.1080/07350015.2025.2505487},
	doi = {10.1080/07350015.2025.2505487},
	abstract = {We propose a doubly robust inference method for causal effects of continuous treatment variables, under unconfoundedness and with nonparametric or high-dimensional nuisance functions. Our double debiased machine learning (DML) estimators for the average dose–response function (or the average structural function) and the partial effects are asymptotically normal with nonparametric convergence rates. The first-step estimators for the nuisance conditional expectation function and the conditional density can be nonparametric or ML methods. Using a kernel-based doubly robust moment function and cross-fitting, we give high-level conditions under which the nuisance function estimators do not affect the first-order large sample distribution of the DML estimators. We provide sufficient low-level conditions for kernel, series, and deep neural networks. We justify the use of kernel to localize the continuous treatment at a given value by the Gateaux derivative. We implement various ML methods in Monte Carlo simulations and an empirical application on a job training program evaluation.},
	number = {0},
	urldate = {2026-01-12},
	journal = {Journal of Business \& Economic Statistics},
	publisher = {Taylor \& Francis},
	author = {Colangelo, Kyle and Lee, Ying-Ying},
	month = jul,
	year = {2025},
	note = {\_eprint: https://doi.org/10.1080/07350015.2025.2505487},
	keywords = {Average structural function, Cross-fitting, Dose–response function, Doubly robust},
	pages = {1--13},
	file = {Full Text PDF:/Users/nb/Zotero/storage/PBJE3DHV/Colangelo and Lee - 2025 - Double Debiased Machine Learning Nonparametric Inference with Continuous Treatments.pdf:application/pdf},
}

@misc{hays_double_2025,
	title = {Double {Machine} {Learning} for {Causal} {Inference} under {Shared}-{State} {Interference}},
	url = {http://arxiv.org/abs/2504.08836},
	doi = {10.48550/arXiv.2504.08836},
	abstract = {Researchers and practitioners often wish to measure treatment effects in settings where units interact via markets and recommendation systems. In these settings, units are affected by certain shared states, like prices, algorithmic recommendations or social signals. We formalize this structure, calling it shared-state interference, and argue that our formulation captures many relevant applied settings. Our key modeling assumption is that individuals' potential outcomes are independent conditional on the shared state. We then prove an extension of a double machine learning (DML) theorem providing conditions for achieving efficient inference under shared-state interference. We also instantiate our general theorem in several models of interest where it is possible to efficiently estimate the average direct effect (ADE) or global average treatment effect (GATE).},
	urldate = {2026-01-12},
	publisher = {arXiv},
	author = {Hays, Chris and Raghavan, Manish},
	month = apr,
	year = {2025},
	note = {arXiv:2504.08836 [stat]},
	keywords = {Computer Science - Machine Learning, Economics - Econometrics, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/nb/Zotero/storage/IE9VYGBJ/Hays and Raghavan - 2025 - Double Machine Learning for Causal Inference under Shared-State Interference.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/CXXP6SHH/2504.html:text/html},
}

@misc{gilbert_causal_2024,
	title = {A causal inference framework for spatial confounding},
	url = {http://arxiv.org/abs/2112.14946},
	doi = {10.48550/arXiv.2112.14946},
	abstract = {Over the past few decades, addressing "spatial confounding" has become a major topic in spatial statistics. However, the literature has provided conflicting definitions, and many proposed solutions are tied to specific analysis models and do not address the issue of confounding as it is understood in causal inference. We offer an analysis-model-agnostic definition of spatial confounding as the existence of an unmeasured causal confounder variable with a spatial structure. We present a causal inference framework for nonparametric identification of the causal effect of a continuous exposure on an outcome in the presence of spatial confounding. In particular, we identify two critical additional assumptions that allow the use of the spatial coordinates as a proxy for the unmeasured spatial confounder: the measurability of the confounder as a function of space, which is required for conditional ignorability to hold, and the presence of a non-spatial component in the exposure, required for positivity to hold. We also propose studying a causal estimand based on a "shift intervention" that requires less stringent identifying assumptions than traditional estimands. We then turn to estimation and focus on "double machine learning" (DML), a procedure in which flexible models are used to regress both the exposure and outcome variables on confounders to arrive at a causal estimator with favorable robustness properties and convergence rates. This procedure avoids restrictive assumptions, such as linearity and effect homogeneity, which are typically made in spatial models and which can lead to bias when violated. We demonstrate the advantages of the DML approach analytically and via extensive simulation studies. We apply our methods and reasoning to a study of the effect of fine particulate matter exposure during pregnancy on birthweight in California.},
	urldate = {2026-01-13},
	publisher = {arXiv},
	author = {Gilbert, Brian and Datta, Abhirup and Casey, Joan A. and Ogburn, Elizabeth L.},
	month = oct,
	year = {2024},
	note = {arXiv:2112.14946 [stat]},
	keywords = {Statistics - Methodology},
	file = {Preprint PDF:/Users/nb/Zotero/storage/FCPLA7XM/Gilbert et al. - 2024 - A causal inference framework for spatial confounding.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/R42CHVPV/2112.html:text/html},
}

@article{schutte_diffusion_2011,
	title = {Diffusion patterns of violence in civil wars},
	volume = {30},
	issn = {0962-6298},
	url = {https://www.sciencedirect.com/science/article/pii/S0962629811000424},
	doi = {10.1016/j.polgeo.2011.03.005},
	abstract = {Much of the current conflict literature attempts to explain the occurrence of violence as the result of determinants exogenous to the conflict process. This paper takes a different approach and analyzes how violence in civil wars spreads in space and time, drawing on earlier work on micro-diffusion of violence in criminology as well as high resolution conflict data. Two general scenarios are distinguished in our analysis: the relocation and the escalation of conflict. Relocation diffusion corresponds to a shift in the location of violence, whereas escalation diffusion refers to the spatial expansion of the conflict site. We argue that unconventional warfare in civil wars without demarcated front lines should primarily lead to the second type of pattern. We describe an extension to a joint count statistic to measure both diffusion types in conflict event data. Monte Carlo simulation allows for the establishment of a baseline for the frequency of contiguous conflict events under the assumption of independence, and thus provides a significance test for the observed patterns. Our results suggest that violence in civil wars exhibits patterns of diffusion, and in particular, that these patterns are primarily of the escalation type, driven by the dynamic expansion of the scope of the conflict.},
	number = {3},
	urldate = {2026-01-15},
	journal = {Political Geography},
	author = {Schutte, Sebastian and Weidmann, Nils B.},
	month = mar,
	year = {2011},
	keywords = {Civil war, Conflict diffusion, Conflict event data, Monte Carlo simulation},
	pages = {143--152},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/QIWT85VW/Schutte and Weidmann - 2011 - Diffusion patterns of violence in civil wars.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/YHZT2GL3/S0962629811000424.html:text/html},
}

@article{jun_flexible_2024,
	title = {Flexible multivariate spatiotemporal {Hawkes} process models of terrorism},
	volume = {18},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-18/issue-2/Flexible-multivariate-spatiotemporal-Hawkes-process-models-of-terrorism/10.1214/23-AOAS1839.full},
	doi = {10.1214/23-AOAS1839},
	abstract = {We develop flexible multivariate spatiotemporal Hawkes process models to analyze patterns of terrorism. Previous applications of point process methods to political violence data mainly utilize temporal Hawkes process models, neglecting spatial variation in these attack patterns. This limits what can be learned from these models, as any effective counter-terrorism strategy requires knowledge on both when and where attacks are likely to occur. Even the existing work on spatiotemporal Hawkes processes imposes restrictions on the triggering function that are not well-suited for terrorism data. Therefore, we generalize the structure of the spatiotemporal triggering function considerably, allowing for nonseparability, nonstationarity, and cross-triggering (across multiple terror groups). To demonstrate the utility of our models, we analyze two samples of real-world terrorism data: Afghanistan (2002–2013) as a univariate analysis and Nigeria (2009–2017) as a bivariate analysis. Jointly, these two studies demonstrate that our generalized models outperform standard Hawkes process models, besting widely-used alternatives in overall model fit and revealing spatiotemporal patterns that are, by construction, masked in these models (e.g., increasing dispersion in cross-triggering over time).},
	number = {2},
	urldate = {2026-01-15},
	journal = {The Annals of Applied Statistics},
	publisher = {Institute of Mathematical Statistics},
	author = {Jun, Mikyoung and Cook, Scott},
	month = jun,
	year = {2024},
	keywords = {GTD, Hawkes processes, multivariate point process, spatiotemporal point patterns, terrorism},
	pages = {1378--1403},
	file = {Submitted Version:/Users/nb/Zotero/storage/NFBEXDNZ/Jun and Cook - 2024 - Flexible multivariate spatiotemporal Hawkes process models of terrorism.pdf:application/pdf},
}

@article{wiecha_two-stage_2025,
	title = {Two-stage estimators for spatial confounding with point-referenced data},
	volume = {81},
	issn = {0006-341X},
	url = {https://doi.org/10.1093/biomtc/ujaf093},
	doi = {10.1093/biomtc/ujaf093},
	abstract = {Public health data are often spatially dependent, but standard spatial regression methods can suffer from bias and invalid inference when the independent variable is associated with spatially correlated residuals. This could occur if, for example, there is an unmeasured environmental contaminant associated with the independent and outcome variables in a spatial regression analysis. Geoadditive structural equation modeling (gSEM), in which an estimated spatial trend is removed from both the explanatory and response variables before estimating the parameters of interest, has previously been proposed as a solution but there has been little investigation of gSEM’s properties with point-referenced data. We link gSEM to results on double machine learning and semiparametric regression based on two-stage procedures. We propose using these semiparametric estimators for spatial regression using Gaussian processes with Matèrn covariance to estimate the spatial trends and term this class of estimators double spatial regression (DSR). We derive regularity conditions for root-n asymptotic normality and consistency and closed-form variance estimation, and show that in simulations where standard spatial regression estimators are highly biased and have poor coverage, DSR can mitigate bias more effectively than competitors and obtain nominal coverage.},
	number = {3},
	urldate = {2026-01-15},
	journal = {Biometrics},
	author = {Wiecha, Nate and Hoppin, Jane A and Reich, Brian J},
	month = sep,
	year = {2025},
	pages = {ujaf093},
	file = {Full Text PDF:/Users/nb/Zotero/storage/WNNVZ34Z/Wiecha et al. - 2025 - Two-stage estimators for spatial confounding with point-referenced data.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/NA3MXZWV/ujaf093.html:text/html},
}

@article{rubin_bayesian_1978,
	title = {Bayesian {Inference} for {Causal} {Effects}: {The} {Role} of {Randomization}},
	volume = {6},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Bayesian {Inference} for {Causal} {Effects}},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-1/Bayesian-Inference-for-Causal-Effects-The-Role-of-Randomization/10.1214/aos/1176344064.full},
	doi = {10.1214/aos/1176344064},
	abstract = {Causal effects are comparisons among values that would have been observed under all possible assignments of treatments to experimental units. In an experiment, one assignment of treatments is chosen and only the values under that assignment can be observed. Bayesian inference for causal effects follows from finding the predictive distribution of the values under the other assignments of treatments. This perspective makes clear the role of mechanisms that sample experimental units, assign treatments and record data. Unless these mechanisms are ignorable (known probabilistic functions of recorded values), the Bayesian must model them in the data analysis and, consequently, confront inferences for causal effects that are sensitive to the specification of the prior distribution of the data. Moreover, not all ignorable mechanisms can yield data from which inferences for causal effects are insensitive to prior specifications. Classical randomized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis.},
	number = {1},
	urldate = {2026-01-16},
	journal = {The Annals of Statistics},
	publisher = {Institute of Mathematical Statistics},
	author = {Rubin, Donald B.},
	month = jan,
	year = {1978},
	keywords = {62A15, 62B15, 62C10, 62F15, 62K99, Bayesian, causality, experimentation, inference, missing data, Randomization},
	pages = {34--58},
	file = {Full Text PDF:/Users/nb/Zotero/storage/43V4MD2Q/Rubin - 1978 - Bayesian Inference for Causal Effects The Role of Randomization.pdf:application/pdf},
}

@article{hudgens_toward_2008,
	title = {Toward {Causal} {Inference} {With} {Interference}},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214508000000292},
	doi = {10.1198/016214508000000292},
	abstract = {A fundamental assumption usually made in causal inference is that of no interference between individuals (or units); that is, the potential outcomes of one individual are assumed to be unaffected by the treatment assignment of other individuals. However, in many settings, this assumption obviously does not hold. For example, in the dependent happenings of infectious diseases, whether one person becomes infected depends on who else in the population is vaccinated. In this article, we consider a population of groups of individuals where interference is possible between individuals within the same group. We propose estimands for direct, indirect, total, and overall causal effects of treatment strategies in this setting. Relations among the estimands are established; for example, the total causal effect is shown to equal the sum of direct and indirect causal effects. Using an experimental design with a two-stage randomization procedure (first at the group level, then at the individual level within groups), unbiased estimators of the proposed estimands are presented. Variances of the estimators are also developed. The methodology is illustrated in two different settings where interference is likely: assessing causal effects of housing vouchers and of vaccines.},
	number = {482},
	urldate = {2026-01-17},
	journal = {Journal of the American Statistical Association},
	publisher = {Taylor \& Francis},
	author = {Hudgens, Michael G and Halloran, M. Elizabeth},
	month = jun,
	year = {2008},
	note = {\_eprint: https://doi.org/10.1198/016214508000000292},
	keywords = {Group-randomized trials, Potential outcomes, Stable unit treatment value assumption, SUTVA, Vaccine},
	pages = {832--842},
	file = {Full Text PDF:/Users/nb/Zotero/storage/65ZKFYDE/Hudgens and Halloran - 2008 - Toward Causal Inference With Interference.pdf:application/pdf},
}

@article{sobel_what_2006,
	title = {What {Do} {Randomized} {Studies} of {Housing} {Mobility} {Demonstrate}?: {Causal} {Inference} in the {Face} of {Interference}},
	volume = {101},
	issn = {0162-1459},
	shorttitle = {What {Do} {Randomized} {Studies} of {Housing} {Mobility} {Demonstrate}?},
	url = {https://doi.org/10.1198/016214506000000636},
	doi = {10.1198/016214506000000636},
	abstract = {During the past 20 years, social scientists using observational studies have generated a large and inconclusive literature on neighborhood effects. Recent workers have argued that estimates of neighborhood effects based on randomized studies of housing mobility, such as the “Moving to Opportunity” (MTO) demonstration, are more credible. These estimates are based on the implicit assumption of no interference between units; that is, a subject's value on the response depends only on the treatment to which that subject is assigned, not on the treatment assignments of other subjects. For the MTO studies, this assumption is not reasonable. Although little work has been done on the definition and estimation of treatment effects when interference is present, interference is common in studies of neighborhood effects and in many other social settings (e.g., schools and networks), and when data from such studies are analyzed under the “no-interference assumption,” very misleading inferences can result. Furthermore, the consequences of interference (e.g., spillovers) should often be of great substantive interest, even though little attention has been paid to this. Using the MTO demonstration as a concrete context, this article develops a frame-work for causal inference when interference is present and defines a number of causal estimands of interest. The properties of the usual estimators of treatment effects, which are unbiased and/or consistent in randomized studies without interference, are also characterized. When interference is present, the difference between a treatment group mean and a control group mean (unadjusted or adjusted for covariates) estimates not an average treatment effect, but rather the difference between two effects defined on two distinct subpopulations. This result is of great importance, for a researcher who fails to recognize this could easily infer that a treatment is beneficial when in fact it is universally harmful.},
	number = {476},
	urldate = {2026-01-17},
	journal = {Journal of the American Statistical Association},
	publisher = {Taylor \& Francis},
	author = {Sobel, Michael E},
	month = dec,
	year = {2006},
	note = {\_eprint: https://doi.org/10.1198/016214506000000636},
	keywords = {Causal inference, Interference, Neighborhood effects, Stable unit treatment value assumption},
	pages = {1398--1407},
	file = {Full Text PDF:/Users/nb/Zotero/storage/TKIZS66P/Sobel - 2006 - What Do Randomized Studies of Housing Mobility Demonstrate Causal Inference in the Face of Interfe.pdf:application/pdf},
}

@article{robins_new_1986,
	title = {A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
	volume = {7},
	issn = {0270-0255},
	url = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
	doi = {10.1016/0270-0255(86)90088-6},
	abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
	number = {9},
	urldate = {2026-01-17},
	journal = {Mathematical Modelling},
	author = {Robins, James},
	month = jan,
	year = {1986},
	pages = {1393--1512},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/3EPALFS8/Robins - 1986 - A new approach to causal inference in mortality studies with a sustained exposure period—application.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/HRK7W7ZV/0270025586900886.html:text/html},
}

@article{gohdes_first_2013,
	title = {First {Things} {First}: {Assessing} {Data} {Quality} before {Model} {Quality}},
	volume = {57},
	issn = {0022-0027},
	shorttitle = {First {Things} {First}},
	url = {https://doi.org/10.1177/0022002712459708},
	doi = {10.1177/0022002712459708},
	abstract = {We address weaknesses in the Peace Research Insitute Oslo (PRIO) Battle Deaths Dataset, and as a result draw contradicting conclusions to those presented by Lacina and Gleditsch. Our analysis focuses on the availability of data on battle deaths within specific conflict-years and problems encountered when data from multiple types of sources are combined. We repeat Lacina, Gleditsch, and Russett’s analysis of battle deaths over time, with an attempt to provide a more robust model and incorporate an estimate of the uncertainty present in the PRIO Battle Deaths Dataset. This reanalysis reveals that the data used to establish the PRIO Battle Deaths Dataset does not offer a clear answer as to whether battle deaths have decreased or increased since the end of the Second World War. We contend that while the PRIO Battle Deaths Dataset offers the most comprehensive assembly of battle deaths data available to date, it is not suitable for analysis across countries or over time.},
	language = {EN},
	number = {6},
	urldate = {2026-01-17},
	journal = {Journal of Conflict Resolution},
	publisher = {SAGE Publications Inc},
	author = {Gohdes, Anita and Price, Megan},
	month = dec,
	year = {2013},
	pages = {1090--1108},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/Z4ZKCXEI/Gohdes and Price - 2013 - First Things First Assessing Data Quality before Model Quality.pdf:application/pdf},
}

@article{weidmann_closer_2016,
	title = {A {Closer} {Look} at {Reporting} {Bias} in {Conflict} {Event} {Data}},
	volume = {60},
	copyright = {© 2015 by the Midwest Political Science Association},
	issn = {1540-5907},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12196},
	doi = {10.1111/ajps.12196},
	abstract = {Recent data collections about political violence are frequently based on media reports, which can lead to reporting bias. This is an issue in particular for the emergent literature on communication technology and conflict, since this technology may not only affect violence, but also the reporting about it. Using the effect of cellphones on violence as an example, this article presents a quantitative assessment of reporting bias in a micro-level analysis. Comparing media-based event reports and those from military sources, the results show that the purported violence-increasing effect of cellphone coverage is partly due to higher reporting rates of violence in cellphone-covered areas. A simple diagnostic procedure for this problem is implemented. Applied to the analysis of cellphones and violence in Africa, it produces a pattern that is consistent with reporting bias driving much of the effect found in the Pierskalla and Hollenbach (2013) study about this topic.},
	language = {en},
	number = {1},
	urldate = {2026-01-17},
	journal = {American Journal of Political Science},
	author = {Weidmann, Nils B.},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ajps.12196},
	pages = {206--218},
	file = {Full Text PDF:/Users/nb/Zotero/storage/6SQWBTQE/Weidmann - 2016 - A Closer Look at Reporting Bias in Conflict Event Data.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/CYQH9II3/ajps.html:text/html},
}

@article{weidmann_accuracy_2015,
	title = {On the {Accuracy} of {Media}-based {Conflict} {Event} {Data}},
	volume = {59},
	issn = {0022-0027},
	url = {https://doi.org/10.1177/0022002714530431},
	doi = {10.1177/0022002714530431},
	abstract = {Empirical researchers of civil war rarely collect data on violence themselves and instead rely on other sources of information. One frequently used source is media reports, which serve as the basis for many ongoing data projects in the discipline. However, news reports rarely cover a conflict comprehensively and objectively and may therefore be prone to various reporting issues. This article provides an analysis of the accuracy of information given in news reports. In particular, if focuses on two types of “hard facts” that event data sets require: the location of an event and its severity. By linking media reports to firsthand accounts from a military database, the article does two things: (1) it analyzes the determinants of inaccuracy and confirms the expectation that events with a low number of observers tend to have higher reporting inaccuracies and (2) it assesses the magnitude of these inaccuracies and the implications for conducting empirical analyses with media-based event data.},
	language = {EN},
	number = {6},
	urldate = {2026-01-17},
	journal = {Journal of Conflict Resolution},
	publisher = {SAGE Publications Inc},
	author = {Weidmann, Nils B.},
	month = sep,
	year = {2015},
	pages = {1129--1149},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/K4SEY6MW/Weidmann - 2015 - On the Accuracy of Media-based Conflict Event Data.pdf:application/pdf},
}

@article{robins_association_1999,
	title = {Association, {Causation}, {And} {Marginal} {Structural} {Models}},
	volume = {121},
	issn = {1573-0964},
	url = {https://doi.org/10.1023/A:1005285815569},
	doi = {10.1023/A:1005285815569},
	language = {en},
	number = {1},
	urldate = {2026-01-17},
	journal = {Synthese},
	author = {Robins, James M.},
	month = nov,
	year = {1999},
	keywords = {Marginal Structural Model},
	pages = {151--179},
	file = {Full Text PDF:/Users/nb/Zotero/storage/YG6AKXE6/Robins - 1999 - Association, Causation, And Marginal Structural Models.pdf:application/pdf},
}

@article{rosenbaum_central_1983,
	title = {The central role of the propensity score in observational studies for causal effects},
	volume = {70},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/70.1.41},
	doi = {10.1093/biomet/70.1.41},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.},
	number = {1},
	urldate = {2026-01-18},
	journal = {Biometrika},
	author = {ROSENBAUM, PAUL R. and RUBIN, DONALD B.},
	month = apr,
	year = {1983},
	pages = {41--55},
	file = {Full Text PDF:/Users/nb/Zotero/storage/AI5L3434/ROSENBAUM and RUBIN - 1983 - The central role of the propensity score in observational studies for causal effects.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/D9TBE5UM/70.1.html:text/html},
}

@article{crump_dealing_2009,
	title = {Dealing with limited overlap in estimation of average treatment effects},
	volume = {96},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asn055},
	doi = {10.1093/biomet/asn055},
	abstract = {Estimation of average treatment effects under unconfounded or ignorable treatment assignment is often hampered by lack of overlap in the covariate distributions between treatment groups. This lack of overlap can lead to imprecise estimates, and can make commonly used estimators sensitive to the choice of specification. In such cases researchers have often used ad hoc methods for trimming the sample. We develop a systematic approach to addressing lack of overlap. We characterize optimal subsamples for which the average treatment effect can be estimated most precisely. Under some conditions, the optimal selection rules depend solely on the propensity score. For a wide range of distributions, a good approximation to the optimal rule is provided by the simple rule of thumb to discard all units with estimated propensity scores outside the range [0.1,0.9].},
	number = {1},
	urldate = {2026-01-18},
	journal = {Biometrika},
	author = {Crump, Richard K. and Hotz, V. Joseph and Imbens, Guido W. and Mitnik, Oscar A.},
	month = mar,
	year = {2009},
	pages = {187--199},
	file = {Full Text PDF:/Users/nb/Zotero/storage/YM4UUUNL/Crump et al. - 2009 - Dealing with limited overlap in estimation of average treatment effects.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/MNMW629D/asn055.html:text/html},
}

@article{damour_overlap_2021,
	title = {Overlap in observational studies with high-dimensional covariates},
	volume = {221},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407620302694},
	doi = {10.1016/j.jeconom.2019.10.014},
	abstract = {Estimating causal effects under exogeneity hinges on two key assumptions: unconfoundedness and overlap. Researchers often argue that unconfoundedness is more plausible when more covariates are included in the analysis. Less discussed is the fact that covariate overlap is more difficult to satisfy in this setting. In this paper, we explore the implications of overlap in observational studies with high-dimensional covariates and formalize curse-of-dimensionality argument, suggesting that these assumptions are stronger than investigators likely realize. Our key innovation is to explore how strict overlap restricts global discrepancies between the covariate distributions in the treated and control populations. Exploiting results from information theory, we derive explicit bounds on the average imbalance in covariate means under strict overlap and show that these bounds become more restrictive as the dimension grows large. We discuss how these implications interact with assumptions and procedures commonly deployed in observational causal inference, including sparsity and trimming.},
	number = {2},
	urldate = {2026-01-18},
	journal = {Journal of Econometrics},
	author = {D’Amour, Alexander and Ding, Peng and Feller, Avi and Lei, Lihua and Sekhon, Jasjeet},
	month = apr,
	year = {2021},
	keywords = {Causal inference, Curse of dimensionality, Information theory, Overlap},
	pages = {644--654},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/594KVRSP/D’Amour et al. - 2021 - Overlap in observational studies with high-dimensional covariates.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/HY347ABQ/S0304407620302694.html:text/html},
}

@article{petersen_diagnosing_2012,
	title = {Diagnosing and responding to violations in the positivity assumption},
	volume = {21},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/0962280210386207},
	doi = {10.1177/0962280210386207},
	abstract = {The assumption of positivity or experimental treatment assignment requires that observed treatment levels vary within confounder strata. This article discusses the positivity assumption in the context of assessing model and parameter-specific identifiability of causal effects. Positivity violations occur when certain subgroups in a sample rarely or never receive some treatments of interest. The resulting sparsity in the data may increase bias with or without an increase in variance and can threaten valid inference. The parametric bootstrap is presented as a tool to assess the severity of such threats and its utility as a diagnostic is explored using simulated and real data. Several approaches for improving the identifiability of parameters in the presence of positivity violations are reviewed. Potential responses to data sparsity include restriction of the covariate adjustment set, use of an alternative projection function to define the target parameter within a marginal structural working model, restriction of the sample, and modification of the target intervention. All of these approaches can be understood as trading off proximity to the initial target of inference for identifiability; we advocate approaching this tradeoff systematically.},
	language = {EN},
	number = {1},
	urldate = {2026-01-18},
	journal = {Statistical Methods in Medical Research},
	publisher = {SAGE Publications Ltd STM},
	author = {Petersen, Maya L and Porter, Kristin E and Gruber, Susan and Wang, Yue and van der Laan, Mark J},
	month = feb,
	year = {2012},
	pages = {31--54},
	file = {SAGE PDF Full Text:/Users/nb/Zotero/storage/WZ5EVBRI/Petersen et al. - 2012 - Diagnosing and responding to violations in the positivity assumption.pdf:application/pdf},
}

@article{li_addressing_2019,
	title = {Addressing {Extreme} {Propensity} {Scores} via the {Overlap} {Weights}},
	volume = {188},
	issn = {0002-9262},
	url = {https://doi.org/10.1093/aje/kwy201},
	doi = {10.1093/aje/kwy201},
	abstract = {The popular inverse probability weighting method in causal inference is often hampered by extreme propensity scores, resulting in biased estimates and excessive variance. A common remedy is to trim patients with extreme scores (i.e., remove them from the weighted analysis). However, such methods are often sensitive to the choice of cutoff points and discard a large proportion of the sample. The implications for bias and the precision of the treatment effect estimate are unclear. These problems are mitigated by a newly developed method, the overlap weighting method. Overlap weights emphasize the target population with the most overlap in observed characteristics between treatments, by continuously down-weighting the units in the tails of the propensity score distribution. Here we use simulations to compare overlap weights to standard inverse probability weighting with trimming, in terms of bias, variance, and 95\% confidence interval coverage. A range of propensity score distributions are considered, including settings with substantial nonoverlap and extreme values. To facilitate practical implementation, we further provide a consistent estimator for the standard error of the treatment effect estimated using overlap weighting.},
	number = {1},
	urldate = {2026-01-18},
	journal = {American Journal of Epidemiology},
	author = {Li, Fan and Thomas, Laine E and Li, Fan},
	month = jan,
	year = {2019},
	pages = {250--257},
	file = {Full Text PDF:/Users/nb/Zotero/storage/JJRA7S8F/Li et al. - 2019 - Addressing Extreme Propensity Scores via the Overlap Weights.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/DL35HBYY/kwy201.html:text/html},
}

@article{buhaug_contagion_2008,
	title = {Contagion or {Confusion}? {Why} {Conflicts} {Cluster} in {Space1}},
	volume = {52},
	issn = {0020-8833},
	shorttitle = {Contagion or {Confusion}?},
	url = {https://doi.org/10.1111/j.1468-2478.2008.00499.x},
	doi = {10.1111/j.1468-2478.2008.00499.x},
	abstract = {Civil wars cluster in space as well as time. In this study, we develop and evaluate empirically alternative explanations for this observed clustering. We consider whether the spatial pattern of intrastate conflict simply stems from a similar distribution of relevant country attributes or whether conflicts indeed constitute a threat to other proximate states. Our results strongly suggest that there is a genuine neighborhood effect of armed conflict, over and beyond what individual country characteristics can account for. We then examine whether the risk of contagion depends on the degree of exposure to proximate conflicts. Contrary to common expectations, this appears not to be the case. Rather, we find that conflict is more likely when there are ethnic ties to groups in a neighboring conflict and that contagion is primarily a feature of separatist conflicts. This suggests that transnational ethnic linkages constitute a central mechanism of conflict contagion.},
	number = {2},
	urldate = {2026-01-18},
	journal = {International Studies Quarterly},
	author = {Buhaug, Halvard and Gleditsch, Kristian Skrede},
	month = jun,
	year = {2008},
	pages = {215--233},
	file = {Full Text PDF:/Users/nb/Zotero/storage/9JQT77FG/Buhaug and Gleditsch - 2008 - Contagion or Confusion Why Conflicts Cluster in Space1.pdf:application/pdf;Snapshot:/Users/nb/Zotero/storage/YD9LGEI3/j.1468-2478.2008.00499.html:text/html},
}

@article{kelejian_hac_2007,
	series = {Analysis of spatially dependent data},
	title = {{HAC} estimation in a spatial framework},
	volume = {140},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407606002260},
	doi = {10.1016/j.jeconom.2006.09.005},
	abstract = {We suggest a non-parametric heteroscedasticity and autocorrelation consistent (HAC) estimator of the variance–covariance (VC) matrix for a vector of sample moments within a spatial context. We demonstrate consistency under a set of assumptions that should be satisfied by a wide class of spatial models. We allow for more than one measure of distance, each of which may be measured with error. Monte Carlo results suggest that our estimator is reasonable in finite samples. We then consider a spatial model containing various complexities and demonstrate that our HAC estimator can be applied in the context of that model.},
	number = {1},
	urldate = {2026-01-18},
	journal = {Journal of Econometrics},
	author = {Kelejian, Harry H. and Prucha, Ingmar R.},
	month = sep,
	year = {2007},
	keywords = {Heteroscedasticity and autocorrelation consistent (HAC) estimator, Instrumental variable estimator, Spatial models},
	pages = {131--154},
	file = {ScienceDirect Full Text PDF:/Users/nb/Zotero/storage/JYKFPJNW/Kelejian and Prucha - 2007 - HAC estimation in a spatial framework.pdf:application/pdf;ScienceDirect Snapshot:/Users/nb/Zotero/storage/2WPSMTJY/S0304407606002260.html:text/html},
}

@article{cameron_robust_2011,
	title = {Robust {Inference} {With} {Multiway} {Clustering}},
	volume = {29},
	issn = {0735-0015},
	url = {https://doi.org/10.1198/jbes.2010.07136},
	doi = {10.1198/jbes.2010.07136},
	abstract = {In this article we propose a variance estimator for the OLS estimator as well as for nonlinear estimators such as logit, probit, and GMM. This variance estimator enables cluster-robust inference when there is two-way or multiway clustering that is nonnested. The variance estimator extends the standard cluster-robust variance estimator or sandwich estimator for one-way clustering (e.g., Liang and Zeger 1986; Arellano 1987) and relies on similar relatively weak distributional assumptions. Our method is easily implemented in statistical packages, such as Stata and SAS, that already offer cluster-robust standard errors when there is one-way clustering. The method is demonstrated by a Monte Carlo analysis for a two-way random effects model; a Monte Carlo analysis of a placebo law that extends the state–year effects example of Bertrand, Duflo, and Mullainathan (2004) to two dimensions; and by application to studies in the empirical literature where two-way clustering is present.},
	number = {2},
	urldate = {2026-01-18},
	journal = {Journal of Business \& Economic Statistics},
	publisher = {Taylor \& Francis},
	author = {Cameron, A. Colin and Gelbach, Jonah B. and Miller, Douglas L.},
	month = apr,
	year = {2011},
	note = {\_eprint: https://doi.org/10.1198/jbes.2010.07136},
	keywords = {Cluster-robust standard errors, Two-way clustering},
	pages = {238--249},
	file = {Full Text PDF:/Users/nb/Zotero/storage/RHU5I2SQ/Cameron et al. - 2011 - Robust Inference With Multiway Clustering.pdf:application/pdf},
}
