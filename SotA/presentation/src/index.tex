%% index.tex
%% Based on source

%% 1. TITLE PAGE
% Using the standard template option 'title green horizontal'
% If you have a background image, add: picture=path/to/image
\begin{frame}[title green horizontal, kitlogo=white]
	\titlepage
\end{frame}

%% 2. TABLE OF CONTENTS
% Using the template option to color the TOC
\begin{frame}[tableofcontents=green]{Contents}
	\tableofcontents
\end{frame}

%% ----------------------------------------------------------------
%% CONTENT SECTIONS
%% ----------------------------------------------------------------

\section{Motivation \& Example Model}

\begin{frame}{Causal Machine Learning for Conflict Research}
	\textbf{Context: Quantitative Conflict Research}
	\begin{itemize}
		\item Spatio-temporal data (geolocations, daily events) to monitor conflict.
		\item \textbf{Forecasting conflict} using Machine Learning (e.g., \textit{Racek et al., 2025}; \textit{Fritz et al., 2022}).
	\end{itemize}

	\vspace{1em}

	\textbf{Project Goal}
	\begin{itemize}
		\item Pure \textit{prediction} to \textbf{causal explanation}.
		\item Identify the causal impact of interventions (e.g., foreign aid, economic shocks, climate events).
		\item Utilize high-dimensional data (e.g., remote sensing covariates (\textit{Racek et al., 2024})).
	\end{itemize}

	\vspace{1em}

	\begin{lightgraybox} % {This Presentations Focus}
		Theoretical foundation of \textbf{Double/Debiased Machine Learning (DML)} based on \textit{Chernozhukov et al. (2018)}
		% To handle these high-dimensional confounders, we need robust methods.
		This presentation covers the theoretical foundation: \textbf{Double Machine Learning (DML)} based on \textit{Chernozhukov et al. (2018)}.
	\end{lightgraybox}
\end{frame}


\begin{frame}{Partial Linear Regression (PLR)}

	\begin{greenblock}{Partial Linear Regression (Robinson, 1988)}
		\vspace{-1.0em} % Removes extra top space
		\begin{alignat*}{2}
			Y = D\theta_0 + g_0(X) + U & ,\qquad & E[U|X,D] & = 0 \\[0.5em]
			D = m_0(X) + V             & ,\qquad & E[V|X]   & = 0
		\end{alignat*}
	\end{greenblock}

	\vspace{1em}
	\begin{itemize}
		\item $Y$: Outcome
		\item $D$: Policy/Treatment
		\item $X$: High-dimensional confounding factors
		\item $\theta_0$: Low-dimensional \textbf{target parameter}
		\item $g_0(X), m_0(X)$: Unknown nuisance functions
		\item $U, V$: Disturbances
	\end{itemize}\end{frame}

\section{Naive Approach}
\begin{frame}{The Naive ML Approach}
	\begin{lightgraybox}
		Construct a ML estimator $D\hat{\theta}_0 + \hat{g}_0(X)$ to learn regression function $D\theta_0 + g_0(X)$.
	\end{lightgraybox}

	\vspace{1.5em}

	Estimate of $\theta_0$ with learned component $\hat{g}_0$:
	$$ \hat{\theta}_0 = \left( \frac{1}{n} \sum_{i \in I} D_i^2 \right)^{-1} \frac{1}{n} \sum_{i \in I} D_i (Y_i - \hat{g}_0(X_i)) $$

	\textbf{Problem:} Convergence slower than $\frac{1}{\sqrt{n}}$:
	$$ \sqrt{n}(\check{\theta}_0 - \theta_0) = a + b + o_P(1) \xrightarrow{p} \infty $$
	with
	$$ a = (E[D_i^2])^{-1} \frac{1}{\sqrt{n}} \sum_{i \in I} D_i U_i \qquad b = (E[D_i^2])^{-1} \frac{1}{\sqrt{n}} \sum_{i \in I} m_0(X_i)(g_0(X_i) - \hat{g}_0(X_i)). $$

\end{frame}

\begin{frame}{The Naive ML Approach}
	\begin{itemize}
		\item High-dimensional data requiered ML (Random Forest, Lasso)
		\item Necessity of regularization
		\item Induces substantive biases and slows down convergence rate
	\end{itemize}

	\vspace{1em}

	\textbf{$\Rightarrow$ Error in $\hat{g}$ decays slower than $\frac{1}{\sqrt{n}}$}

	\vspace{1em}

	\begin{redblock}{The Bias Problem}
		Bias in $\hat{\theta}$ depends mostly on the interaction:

		$$ \sum \underbrace{m_0(X)}_{\text{'Constant' Term}} \times \underbrace{(\hat{g}(X) - g_0(X))}_{\text{Slow Estimation Error}} $$

		\begin{itemize}
			\item $m_0(X)$ acts like a \textbf{non-zero weight}
			\item Because $m_0 \neq 0$, the slow error in $\hat{g}$ is not averaged out
			\item Ultimately biases our estimate
		\end{itemize}
	\end{redblock}
\end{frame}

\section{DML Approach}

\begin{frame}{The DML Solution: Orthogonalization}

	\begin{lightgraybox}
		Partiall out effect of X from D to obtain orthogonalized regressor $V = D - m_0(X)$
	\end{lightgraybox}
	\vspace{1em}
	\begin{enumerate}
		\setlength\itemsep{1em} % Adds nice spacing between the numbered steps

		% \makebox aligns the math by forcing the text to be exactly 4.5cm wide
		\item Predict outcome (Predict $Y$ from $X$)
		      $$\hat{g}(X) = E[Y|X]$$

		\item Predict treatment (Predict $D$ from $X$)
		      $$\hat{m}(X) = E[D|X]$$

		\item Obtain Residuals:
		      $$ \hat{Y}_i = Y_i - \hat{g}(X_i) \quad \text{and} \quad \hat{V}_i = D_i - \hat{m}(X_i) $$

		\item Regress $\hat{Y}$ on $\hat{V}$ to get $\check{\theta}$
	\end{enumerate}

	% \vspace{1em}
	% \begin{greenbox}
	%     \textbf{Result:} Estimator becomes insensitive ("orthogonal") to errors in the ML steps.
	% \end{greenbox}
\end{frame}

\begin{frame}{The DML Solution: Orthogonalization}
	We get the following DML estimator for $\theta_0$:
	% FIX: Removed '&' here. Use standard '=' inside $$ ... $$
	$$ \check{\theta}_0 = \left( \frac{1}{n} \sum_{i \in I} \hat{V}_i D_i \right)^{-1} \frac{1}{n} \sum_{i \in I} \hat{V}_i (Y_i - \hat{g}_0(X_i)) $$

	Decomposition of the scaled estimation error of $\check{\theta}_0$:
	% FIX: Removed '&' here as well.
	$$ \sqrt{n}(\check{\theta}_0 - \theta_0) = a^* + b^* + o_P(1)$$

	with
	% This part was already correct because it uses align*
	\begin{align*}
		a^* & = (E[V^2])^{-1} \frac{1}{\sqrt{n}} \sum_{i \in I} V_i U_i \rightsquigarrow N(0, \Sigma)                  \\
		b^* & = (E[V^2])^{-1} \frac{1}{\sqrt{n}} \sum_{i \in I} (\hat{m}_0(X_i) - m_0(X_i))(\hat{g}_0(X_i) - g_0(X_i))
	\end{align*}
\end{frame}
\begin{frame}{The DML Solution: Orthonogonalization}
	Regularization bias no longer depends on a single estimation error.
	\vspace{0.5em}

	\begin{greenblock}{The Solution to the Bias Problem}
		Bias in $\check{\theta}$ is mostly determined by the interaction:
		$$ \sum \underbrace{(\hat{m}(X) - m_0(X))}_{\text{Error in } D} \times \underbrace{(\hat{g}(X) - g_0(X))}_{\text{Error in } Y} $$

		\begin{itemize}
			\item With $\hat{m}$ and $\hat{g}$ converging slowly (e.g., $n^{-1/4}$), their \textbf{product converges fast} ($n^{-1/2}$)
			\item Bias vanishes faster than $\frac{1}{\sqrt{n}}$

			\item $\Rightarrow \sqrt{n}(\check{\theta}_0 - \theta_0) \to N(0, \Sigma)$
		\end{itemize}
	\end{greenblock}

\end{frame}

\begin{frame}{Experiment Results: Naive vs. DML}
	\begin{columns}
		\column{0.65\textwidth}
		\centering
		% \includegraphics[width=\linewidth]{fig1.pdf}

		\column{0.35\textwidth}
		\begin{itemize}
			\item 500 Simulation runs
			\item Random Forest
			\item $N = 500$, $p = 20$ vars
			\item $\theta_{0} = 0.5$

			\item $m_0(X) = X_1 + 0.25 \cdot \sigma(X_3)$
			\item $g_0(X) = \sigma(X_1) + 0.25 \cdot X_3$
		\end{itemize}
	\end{columns}
\end{frame}

\section{Sample Splitting}

\begin{frame}{Why Sample Splitting?}
	Recall the error decomposition for the DML estimator:
	$$ \sqrt{n}(\check{\theta}_0 - \theta_0) = \underbrace{a^*}_{N(0, \Sigma)} + \underbrace{b^*}_{\text{Regularization Bias}} + \underbrace{c^*}_{\text{Remainder}} $$

	\textbf{The Critical Remainder Term:}

	$$ c^* = \frac{1}{\sqrt{n}} \sum_{i \in I} V_i (\hat{g}(X_i) - g_0(X_i)) $$


	\vspace{1em}

	\begin{redblock}{The Overfitting Problem}
		Estimate $\hat{g}$ and $\theta$ on the \textbf{same data}:
		\begin{itemize}
			\item $\hat{g}$ is trained on $Y_i$, which contains $V_i \Rightarrow \hat{g}(X_i)$ is correlated with $V_i$
			\item Even if $\hat{g}$ is a great predictor, correlation prevents $c^*$ from vanishing

		\end{itemize}
	\end{redblock}
\end{frame}

\begin{frame}{Removing Bias via Sample Splitting}
	\textbf{The Solution:} Train nuisance models ($\hat{g}, \hat{m}$) on auxiliary ($I^c$), estimate $\check{\theta}$ on main ($I$).
	\vspace{1em}
	\begin{greenblock}{Mathematical Consequence}
		Conditioning on $I^c$ and with $E[V_i|X_i] = 0$, the remainder term $c^*$ has \textbf{mean zero} and variance of order:

		$$ \frac{1}{n} \sum_{i \in I} (\hat{g}_0(X_i) - g_0(X_i))^2 \xrightarrow{p} 0 $$

		$\Rightarrow$ Term vanishes in probability by \textbf{Chebyshevâ€™s inequality}.
	\end{greenblock}
	\vspace{1em}
	\begin{lightgraybox}
		To avoid data loss, Cross-Fitting should be used.
	\end{lightgraybox}
\end{frame}

\begin{frame}{Experiment Results: Sample Splitting}
	\vspace{-2em}
	\centering
	% \includegraphics[width=0.7\linewidth]{fig2.pdf}
\end{frame}

\section{Generalization}

\begin{frame}{Generalization: Neyman Orthogonality}

	The DML estimator $\check{\theta}_0$ solves the empirical analog:
	$$ \frac{1}{n} \sum_{i \in I} \psi(W_i; \check{\theta}_0, \hat{\eta}_0) = 0 $$

	\vspace{0.5em}

	\begin{greenblock}{Neyman Orthogonality}
		Score $\psi$ is Neyman orthogonal if the moment condition is insensitive to local perturbations in the nuisance parameter $\eta$:
		$$ \partial_{\eta} E[\psi(W; \theta_0, \eta_0)][\eta - \eta_0] = 0 $$

		\textbf{Result:} We can plug in noisy estimates $\hat{\eta}$ without creating first-order bias.
	\end{greenblock}

	\vspace{0.5em}

	Previous examples Neyman orthogonal score function:
	$$ \psi(W; \theta, \eta) = (Y - D\theta - g(X)) (D - m(X)) $$
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion}
	\begin{itemize}
		\setlength\itemsep{1em} % Adds nice spacing between the numbered steps
		\item \textbf{The Challenge:} For high-dimensional conflict data, standard ML leads to \textbf{biased estimates} due to Regularization and Overfitting.
		\item \textbf{The DML Solution:}
		      \begin{enumerate}
			      \item \textbf{Neyman Orthogonality:} Makes estimator insensitive to errors in nuisance parameters ($\hat{g}, \hat{m}$)
			      \item \textbf{Sample Splitting:} Ensures remainder term vanishes by decoupling model estimation from the noise
		      \end{enumerate}
	\end{itemize}

	\vspace{1em}

	\begin{greenblock}{Key Takeaway}
		DML allows us to utilize powerful ML methods (e.g., Random Forests)
		to control for complex confounders, while recovering a \textbf{$\sqrt{n}$-consistent, asymptotically normal} estimator for the causal effect $\theta_0$.
	\end{greenblock}
\end{frame}
